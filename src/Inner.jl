#  Copyright (c) 2017-25, Oscar Dowson and SDDP.jl contributors
#  This Source Code Form is subject to the terms of the Mozilla Public
#  License, v. 2.0. If a copy of the MPL was not distributed with this
#  file, You can obtain one at http://mozilla.org/MPL/2.0/.

module Inner
# This code is heavily based on bellman_functions.jl

import JuMP
import JSON
import ..SDDP

import JuMP: MOI

mutable struct Vertex
    value::Float64
    state::Dict{Symbol,Float64}
    obj_y::Union{Nothing,NTuple{N,Float64} where {N}}
    belief_y::Union{Nothing,Dict{T,Float64} where {T}}
    non_dominated_count::Int
    variable_ref::Union{Nothing,JuMP.VariableRef}
end

mutable struct InnerConvexApproximation
    theta::JuMP.VariableRef
    states::Dict{Symbol,JuMP.VariableRef}
    objective_states::Union{Nothing,NTuple{N,JuMP.VariableRef} where {N}}
    belief_states::Union{Nothing,Dict{T,JuMP.VariableRef} where {T}}
    # Storage for cut selection
    vertices::Vector{Vertex}
    sampled_states::Vector{SDDP.SampledState}
    vertices_to_be_deleted::Vector{Vertex}
    deletion_minimum::Int
    Lipschitz_constant::Float64

    function InnerConvexApproximation(
        theta::JuMP.VariableRef,
        states::Dict{Symbol,JuMP.VariableRef},
        objective_states,
        belief_states,
        deletion_minimum::Int,
        Lipschitz_constant::Float64,
    )
        return new(
            theta,
            states,
            objective_states,
            belief_states,
            Vertex[],
            SDDP.SampledState[],
            Vertex[],
            deletion_minimum,
            Lipschitz_constant,
        )
    end
end

function _add_vertex(
    V::InnerConvexApproximation,
    θᵏ::Float64,
    xᵏ::Dict{Symbol,Float64},
    obj_y::Union{Nothing,NTuple{N,Float64}},
    belief_y::Union{Nothing,Dict{T,Float64}};
    optimizer = nothing,
    cut_selection::Bool = true,
) where {N,T}
    vertex = Vertex(θᵏ, xᵏ, obj_y, belief_y, 1, nothing)
    _add_vertex_var_to_model(V, vertex)
    push!(V.vertices, vertex)
    if cut_selection
        # _vertex_selection(V, optimizer)
        nothing
    end
    return
end

# TODO(bfpc): implement Objective and Belief states
function _add_vertex_var_to_model(V::InnerConvexApproximation, vertex::Vertex)
    model = JuMP.owner_model(V.theta)
    if V.objective_states !== nothing
        error("Objective states not yet implemented.")
    end
    # TODO(rjmalves): restore belief state check when supporting markovian graphs
    # if V.belief_states !== nothing
    #     error("Belief states not yet implemented.")
    # end

    # Add a new variable to the convex combination constraints
    σk = JuMP.@variable(model, lower_bound = 0.0, upper_bound = 1.0)
    JuMP.set_normalized_coefficient(model[:σ_cc], σk, 1)

    xk = vertex.state
    for key in keys(xk)
        JuMP.set_normalized_coefficient(model[:x_cc][key], σk, xk[key])
    end

    vk = vertex.value
    JuMP.set_normalized_coefficient(model[:theta_cc], σk, vk)

    vertex.variable_ref = σk
    return
end

""" Exact vertex selection

Test if (x_j, v_j) is covered by {convex combinations of (x_i, v_i)} + (0, t)

We could add the Lipschitz cone, generated by [+ (±e_j, Lip)],
but in practice this almost never happens.
"""
function _vertex_selection(V::InnerConvexApproximation, solver; tol = 1e-6)
    nvertices = length(V.vertices)
    state_keys = keys(V.states)

    m = JuMP.Model(solver)
    JuMP.set_silent(m)
    JuMP.@variable(m, 0 <= σ[1:nvertices] <= 1)
    JuMP.@variable(m, t)
    JuMP.@objective(m, Max, t)
    JuMP.@constraint(
        m,
        cc_x[k in state_keys],
        sum(σ[i] * V.vertices[i].state[k] for i in 1:nvertices) .== 0
    )
    JuMP.@constraint(
        m,
        cc_t,
        sum(σ[i] * V.vertices[i].value for i in 1:nvertices) + t == 0
    )
    JuMP.@constraint(m, sum(σ[i] for i in 1:nvertices) == 1)

    remove_vars = JuMP.VariableRef[]
    for j in 1:nvertices
        v = V.vertices[j]
        for k in state_keys
            JuMP.set_normalized_rhs(cc_x[k], v.state[k])
        end
        JuMP.set_normalized_rhs(cc_t, v.value)

        JuMP.optimize!(m)
        margin = JuMP.objective_value(m)
        if margin > tol
            push!(remove_vars, v.variable_ref)
            # Don't use this vertex anymore for vertex selection
            JuMP.delete(m, σ[j])
        end
    end
    # Remove all vertices from subproblem
    if length(remove_vars) > 0
        sp = JuMP.owner_model(V.theta)
        JuMP.delete(sp, remove_vars)
        println("Selection removed $(length(remove_vars)) vertices")
    end

    return m
end

"""
    InnerBellmanFunction

An inner representation of the value function, currently only using

 1) x - convex "resource" states

This function penalizes the convex combination constraint to define a valid
approximation in the entire state space.

In addition, we have three types of vertices (x, v) (in "cut" terminology):

 1) Single-cuts (also called "average" cuts in the literature), which involve
    the risk-adjusted expectation of the cost-to-go.
 2) Multi-cuts, which use a different cost-to-go term for each realization w.
 3) Risk-cuts, which correspond to the facets of the dual interpretation of a
    convex risk measure.

Therefore, InnerBellmanFunction returns a JuMP model of the following form (when minimizing):

```
V(x, b, y) =
    min: θ
    s.t. # "Single" / "Average" cuts
         θ ≥ Σ α(j) * v(j) + Lip |delta|
         x = Σ α(j) * x(j) + delta
         1 = Σ α(j)
         0 ≤ α(j) ≤ 1,                                ∀ j ∈ J
         # "Multi" cuts
         φ(w) ≥ Σ α(k, w) * v(k, w) + Lip |delta(w)|, ∀ w ∈ Ω
         x = Σ α(k, w) * x(k, w) + delta(w),          ∀ w ∈ Ω
         1 = Σ α(k, w),                               ∀ w ∈ Ω
         0 ≤ α(k, w) ≤ 1,                             ∀ k ∈ K, w ∈ Ω
         # "Risk-set" cuts
         θ ≥ Σ{p(k, w) * φ(w)}_w - μᵀb(k) - νᵀy(k),   ∀ k ∈ K
```
"""
mutable struct InnerBellmanFunction
    cut_type::SDDP.CutType
    global_theta::InnerConvexApproximation
    local_thetas::Vector{InnerConvexApproximation}
    # Cuts defining the dual representation of the risk measure.
    risk_set_cuts::Set{Vector{Float64}}
    Lipschitz_constant::Float64
end

"""
    InnerBellmanFunction(Lipschitz_constant;
        lower_bound = -Inf,
        upper_bound = Inf,
        deletion_minimum::Int = 1,
        vertex_type::SDDP.CutType = SDDP.MULTI_CUT,
    )
"""
function InnerBellmanFunction(
    Lipschitz_constant;
    lower_bound = -Inf,
    upper_bound = Inf,
    deletion_minimum::Int = 1,
    vertex_type::SDDP.CutType = SDDP.MULTI_CUT,
)
    return SDDP.InstanceFactory{InnerBellmanFunction}(;
        Lipschitz_constant = Lipschitz_constant,
        lower_bound = lower_bound,
        upper_bound = upper_bound,
        deletion_minimum = deletion_minimum,
        vertex_type = vertex_type,
    )
end

function SDDP.bellman_term(bellman_function::InnerBellmanFunction)
    return bellman_function.global_theta.theta
end

# to_node is an internal helper function so users can pass arguments like:
#   Lipschitz_constant = 2000.0,
#   Lipschitz_constant = Dict(1=>2000.0, 2=>1000.0)
#   Lipschitz_constant = (node_index) -> 1000.0 * (nStages - node_index)
# See [`to_nodal_form`] in SDDP.jl
function to_node(node::SDDP.Node{T}, element) where {T}
    return element
end

function to_node(node::SDDP.Node{T}, builder::Function) where {T}
    return builder(node.index)
end

function to_node(node::SDDP.Node{T}, dict::Dict{T,V}) where {T,V}
    return dict[node.index]
end

function SDDP.initialize_bellman_function(
    factory::SDDP.InstanceFactory{InnerBellmanFunction},
    model::SDDP.PolicyGraph{T},
    node::SDDP.Node{T},
) where {T}
    lower_bound,
    upper_bound,
    deletion_minimum,
    vertex_type,
    Lipschitz_constant = -Inf, Inf, 0, SDDP.SINGLE_CUT, 0.0
    for (kw, value) in factory.kwargs
        value = to_node(node, value)
        if kw == :lower_bound
            lower_bound = value
        elseif kw == :upper_bound
            upper_bound = value
        elseif kw == :deletion_minimum
            deletion_minimum = value
        elseif kw == :vertex_type
            vertex_type = value
        elseif kw == :Lipschitz_constant
            Lipschitz_constant = value
        end
    end
    if length(node.children) == 0
        lower_bound = upper_bound = 0.0
    end

    # Add epigraph variable for inner approximation
    sp = node.subproblem
    Θᴳ = JuMP.@variable(sp)
    # Initialize bounds for the objective states. If objective_state==nothing,
    # this check will be skipped by dispatch.
    SDDP._add_initial_bounds(node.objective_state, Θᴳ)
    x′ = Dict(key => var.out for (key, var) in node.states)
    obj_μ = node.objective_state !== nothing ? node.objective_state.μ : nothing
    belief_μ = node.belief_state !== nothing ? node.belief_state.μ : nothing

    # model convex combination + Lipschitz penalty
    if length(node.children) != 0
        JuMP.@variable(sp, δ[keys(x′)])
        JuMP.@variable(sp, δ_abs[keys(x′)] >= 0)
        JuMP.@constraint(sp, [k in keys(x′)], δ_abs[k] >= δ[k])
        JuMP.@constraint(sp, [k in keys(x′)], δ_abs[k] >= -δ[k])

        JuMP.@variable(sp, 0 <= σ0 <= 1)
        if JuMP.objective_sense(sp) == MOI.MIN_SENSE
            JuMP.@constraint(
                sp,
                theta_cc,
                Lipschitz_constant * sum(δ_abs) + σ0 * upper_bound <= Θᴳ
            )
        else
            JuMP.@constraint(
                sp,
                theta_cc,
                Lipschitz_constant * sum(δ_abs) + σ0 * lower_bound >= Θᴳ
            )
        end
        JuMP.@constraint(sp, x_cc[k in keys(x′)], δ[k] == x′[k])
        JuMP.@constraint(sp, σ_cc, σ0 == 1)

        sp[:vertex_coverage_distance] = JuMP.@expression(sp, sum(δ_abs))
        # No children => no cost
    else
        JuMP.fix(Θᴳ, 0.0)
        sp[:vertex_coverage_distance] = JuMP.@expression(sp, 0.0)
    end

    return InnerBellmanFunction(
        vertex_type,
        InnerConvexApproximation(
            Θᴳ,
            x′,
            obj_μ,
            belief_μ,
            deletion_minimum,
            Lipschitz_constant,
        ),
        InnerConvexApproximation[],
        Set{Vector{Float64}}(),
        Lipschitz_constant,
    )
end

function SDDP.refine_bellman_function(
    model::SDDP.PolicyGraph{T},
    node::SDDP.Node{T},
    bellman_function::InnerBellmanFunction,
    risk_measure::SDDP.AbstractRiskMeasure,
    outgoing_state::Dict{Symbol,Float64},
    dual_variables::Vector{Dict{Symbol,Float64}},
    noise_supports::Vector,
    nominal_probability::Vector{Float64},
    objective_realizations::Vector{Float64},
) where {T}
    lock(node.lock)
    try
        return _refine_inner_bellman_function_no_lock(
            model,
            node,
            bellman_function,
            risk_measure,
            outgoing_state,
            dual_variables,
            noise_supports,
            nominal_probability,
            objective_realizations,
        )
    finally
        unlock(node.lock)
    end
end

function _refine_inner_bellman_function_no_lock(
    model::SDDP.PolicyGraph{T},
    node::SDDP.Node{T},
    bellman_function::InnerBellmanFunction,
    risk_measure::SDDP.AbstractRiskMeasure,
    outgoing_state::Dict{Symbol,Float64},
    dual_variables::Vector{Dict{Symbol,Float64}},
    noise_supports::Vector,
    nominal_probability::Vector{Float64},
    objective_realizations::Vector{Float64},
) where {T}
    # Sanity checks.
    @assert length(dual_variables) ==
            length(noise_supports) ==
            length(nominal_probability) ==
            length(objective_realizations)
    # Preliminaries that are common to all cut types.
    risk_adjusted_probability = similar(nominal_probability)
    offset = SDDP.adjust_probability(
        risk_measure,
        risk_adjusted_probability,
        nominal_probability,
        noise_supports,
        objective_realizations,
        model.objective_sense == MOI.MIN_SENSE,
    )
    # The meat of the function.
    if bellman_function.cut_type == SDDP.SINGLE_CUT
        return _add_average_vertex(
            node,
            outgoing_state,
            risk_adjusted_probability,
            objective_realizations,
            dual_variables,
            offset,
        )
    else  # TODO(bfpc): Add a multi-cut
        @assert bellman_function.cut_type == SDDP.MULTI_CUT
        error("Multi-vertices not yet implemented.")
        # SDDP._add_locals_if_necessary(
        #     node,
        #     bellman_function,
        #     length(dual_variables),
        # )
        # return _add_multi_vertex(
        #     node,
        #     outgoing_state,
        #     risk_adjusted_probability,
        #     objective_realizations,
        #     dual_variables,
        #     offset,
        # )
    end
end

function _add_average_vertex(
    node::SDDP.Node,
    outgoing_state::Dict{Symbol,Float64},
    risk_adjusted_probability::Vector{Float64},
    objective_realizations::Vector{Float64},
    dual_variables::Vector{Dict{Symbol,Float64}},
    offset::Float64,
)
    N = length(risk_adjusted_probability)
    @assert N == length(objective_realizations) == length(dual_variables)
    # Calculate the expected intercept with respect to the
    # risk-adjusted probability distribution.
    θᵏ = offset
    for i in 1:length(objective_realizations)
        p = risk_adjusted_probability[i]
        θᵏ += p * objective_realizations[i]
    end
    # Now add the average-vertex to the subproblem. We include the objective-state
    # component μᵀy and the belief state (if it exists).
    obj_y =
        node.objective_state === nothing ? nothing : node.objective_state.state
    belief_y =
        node.belief_state === nothing ? nothing : node.belief_state.belief
    _add_vertex(
        node.bellman_function.global_theta,
        θᵏ,
        outgoing_state,
        obj_y,
        belief_y;
        optimizer = node.optimizer,
    )
    return (theta = θᵏ, x = outgoing_state, obj_y = obj_y, belief_y = belief_y)
end

function model_type(model::SDDP.PolicyGraph{T}) where {T}
    return T
end

"""
    _validate_linear_graph(graph::SDDP.Graph)

Validates a given graph to be linear by counting the number of children
at each node. This function does not replace the graph validation step from
the main SDDP module and is meant to be used in conjunction with it while
the Inner module does not support other graphs.

"""
function _validate_linear_graph(graph::SDDP.Graph)
    node_count = length(graph.nodes)
    has_leaf = false
    for (node, children) in graph.nodes
        children_count = length(children)
        if children_count == 0
            has_leaf = true
        elseif children_count != 1
            error(
                "The graph is not linear since node $(node) has " *
                "$(children_count) children and should have 1.",
            )
        end
    end
    if !has_leaf
        error(
            "The graph is not linear since no leaf node was " *
            "found among $(node_count) nodes.",
        )
    end
    return
end

"""
    InnerPolicyGraph(
        builder::Function,
        graph::SDDP.Graph{T};
        sense::Symbol = :Min,
        lower_bound = -Inf,
        upper_bound = Inf,
        optimizer = nothing,
        lipschitz_constant = nothing,
    ) where {T}

Construct a policy graph for inner policy approximation based on the graph
structure of `graph`, which use an InnerBellmanFunction in each node.
(See [`SDDP.Graph`](@ref) for details.)

## Keyword arguments

 - `sense`: whether we are minimizing (`:Min`) or maximizing (`:Max`).

 - `lower_bound`: if mimimizing, a valid lower bound for the cost to go in all
   subproblems.

 - `upper_bound`: if maximizing, a valid upper bound for the value to go in all
   subproblems.

 - `optimizer`: the optimizer to use for each of the subproblems

 - `lipschitz_constant`: an estimate for the Lipschitz constant of each subproblem

## Examples

```julia
function builder(subproblem::JuMP.Model, index)
    # ... subproblem definition ...
end

model = InnerPolicyGraph(
    builder,
    graph;
    lower_bound = 0.0,
    optimizer = HiGHS.Optimizer,
)
```

Or, using the Julia `do ... end` syntax:

```julia
model = InnerPolicyGraph(
    graph;
    lower_bound = 0.0,
    optimizer = HiGHS.Optimizer,
) do subproblem, index
    # ... subproblem definitions ...
end
```
"""
function InnerPolicyGraph(
    builder::Function,
    graph::SDDP.Graph{T};
    sense::Symbol = :Min,
    lower_bound = -Inf,
    upper_bound = Inf,
    optimizer = nothing,
    # This argument is the only hard difference from the "Cut-Based" PolicyGraph
    lipschitz_constant = nothing,
    # These arguments are deprecated
    bellman_function = nothing,
    direct_mode::Bool = false,
) where {T}
    # This function conveniently reuses code from the general PolicyGraph builder
    # instead of simply calling LinearPolicyGraph, what should simplify things,
    # but will need to change when support to cyclic graphs is add.

    # While only LinearGraphs are supported
    _validate_linear_graph(graph)

    # Cannot call PolicyGraph from the SDDP default interface since the
    # bellman_function named arg is marked as deprecated

    # Spend a one-off cost validating the graph.
    SDDP._validate_graph(graph)
    # Construct a basic policy graph. We will add to it in the remainder of this
    # function.
    policy_graph = SDDP.PolicyGraph(sense, graph.root_node)
    if bellman_function === nothing
        if sense == :Min && lower_bound === -Inf
            error(
                "You must specify a finite lower bound on the objective value" *
                " using the `lower_bound = value` keyword argument.",
            )
        elseif sense == :Max && upper_bound === Inf
            error(
                "You must specify a finite upper bound on the objective value" *
                " using the `upper_bound = value` keyword argument.",
            )
        else
            if lipschitz_constant === nothing
                error(
                    "With inner approximations, you must specify an estimate for " *
                    "the Lipschitz constant to be used in the InnerBellmanFunction",
                )
            end
            if sense == :Min && upper_bound === Inf
                error(
                    "With inner approximations, you must specify a finite upper bound" *
                    "on the objective value using the `upper_bound = value`" *
                    " keyword argument even when sense = :Min.",
                )
            end
            if sense == :Max && lower_bound === -Inf
                error(
                    "With inner approximations, you must specify a finite lower bound" *
                    "on the objective value using the `lower_bound = value`" *
                    " keyword argument even when sense = :Max.",
                )
            end
            bellman_function = InnerBellmanFunction(
                lipschitz_constant;
                lower_bound = lower_bound,
                upper_bound = upper_bound,
                vertex_type = SDDP.SINGLE_CUT,
            )
        end
    end
    # Initialize nodes.
    for (node_index, children) in graph.nodes
        if node_index == graph.root_node
            continue
        end
        subproblem = SDDP.construct_subproblem(optimizer, direct_mode)
        node = SDDP.Node(
            node_index,
            subproblem,
            SDDP.Noise{T}[],
            SDDP.Noise[],
            (ω) -> nothing,
            Dict{Symbol,SDDP.State{JuMP.VariableRef}}(),
            0.0,
            false,
            # Delay initializing the bellman function until later so that it can
            # use information about the children and number of
            # stagewise-independent noise realizations.
            nothing,
            # Likewise for the objective states.
            nothing,
            # And for belief states.
            nothing,
            # The optimize hook defaults to nothing.
            nothing,
            nothing,
            false,
            direct_mode ? nothing : optimizer,
            # The extension dictionary.
            Dict{Symbol,Any}(),
            ReentrantLock(),
            Dict{Symbol,Tuple{Float64,Float64,Bool}}(),
        )
        subproblem.ext[:sddp_policy_graph] = policy_graph
        policy_graph.nodes[node_index] = subproblem.ext[:sddp_node] = node
        JuMP.set_objective_sense(subproblem, policy_graph.objective_sense)
        builder(subproblem, node_index)
        # Add a dummy noise here so that all nodes have at least one noise term.
        if length(node.noise_terms) == 0
            push!(node.noise_terms, SDDP.Noise(nothing, 1.0))
        end
        ctypes = JuMP.list_of_constraint_types(subproblem)
        node.has_integrality =
            (JuMP.VariableRef, MOI.Integer) in ctypes ||
            (JuMP.VariableRef, MOI.ZeroOne) in ctypes
    end
    # Loop back through and add the arcs/children.
    for (node_index, children) in graph.nodes
        if node_index == graph.root_node
            continue
        end
        node = policy_graph.nodes[node_index]
        for (child, probability) in children
            push!(node.children, SDDP.Noise(child, probability))
        end
        # Intialize the bellman function. (See note in creation of Node above.)
        node.bellman_function = SDDP.initialize_bellman_function(
            bellman_function,
            policy_graph,
            node,
        )
    end
    # Add root nodes
    for (child, probability) in graph.nodes[graph.root_node]
        push!(policy_graph.root_children, SDDP.Noise(child, probability))
        # We check the feasibility of the initial point here. It is a really
        # tricky feasibility bug to diagnose otherwise. See #387 for details.
        for (k, v) in policy_graph.initial_root_state
            x_out = policy_graph[child].states[k].out
            if JuMP.has_lower_bound(x_out) && JuMP.lower_bound(x_out) > v
                error("Initial point $(v) violates lower bound on state $k")
            elseif JuMP.has_upper_bound(x_out) && JuMP.upper_bound(x_out) < v
                error("Initial point $(v) violates upper bound on state $k")
            end
        end
    end
    # Skip belief states since they are not supported

    domain = SDDP._get_incoming_domain(policy_graph)
    for (node_name, node) in policy_graph.nodes
        for (k, v) in domain[node_name]
            node.incoming_state_bounds[k] = something(v, (-Inf, Inf, false))
        end
    end
    return policy_graph
end

"""
    dp_vertices_from_visited_states(
        inner_model::SDDP.PolicyGraph
        outer_model::SDDP.PolicyGraph;
        optimizer = nothing,
        risk_measures::SDDP.AbstractRiskMeasure = SDDP.Expectation(),
        print_level::Int = 0,
    )

Seeds vertices to `inner_model`, a `PolicyGraph` with an `InnerBellmanFunction``
from the states sampled by `outer_model`, another `PolicyGraph` with an
"Outer" `BellmanFunction`.

In this process, performs a single dynamic programming backwards iteration,
building inner approximations of the value functions.

Currently, only supports LinearPolicyGraphs.

This function is mean to be an alternative to the `inner_dp` function
that does not require passing the builder or building externally the
`InnerBellmanFunction`, and may be used together with the
`InnerPolicyGraph` constructor.

## Keyword arguments

 - `optimizer` optimizer to use in the vertex selection.

 - `risk_measures::SDDP.AbstractRiskMeasure` risk measures
    for the problem.

 - `print_level::Int` level of verbosity.
"""
function dp_vertices_from_visited_states(
    inner_model::SDDP.PolicyGraph,
    outer_model::SDDP.PolicyGraph;
    optimizer = nothing,
    risk_measures::SDDP.AbstractRiskMeasure = SDDP.Expectation(),
    print_level::Int = 0,
    vertex_selection_tol::Float64 = 1e-6,
)
    opts =
        SDDP.Options(inner_model, outer_model.initial_root_state; risk_measures)
    T = model_type(inner_model)
    for node_index in sort(collect(keys(outer_model.nodes)); rev = true)[2:end]
        dt = @elapsed begin
            node = inner_model[node_index]
            fw_samples =
                outer_model[node_index].bellman_function.global_theta.sampled_states
            for sampled_state in fw_samples
                outgoing_state = sampled_state.state
                items = SDDP.BackwardPassItems(T, SDDP.Noise)
                SDDP.solve_all_children(
                    inner_model,
                    node,
                    items,
                    1.0,
                    nothing, # belief_state
                    nothing, # objective_state
                    outgoing_state,
                    opts.backward_sampling_scheme,
                    Tuple{T,Any}[], # scenario_path
                    opts.duality_handler,
                    opts,
                )
                SDDP.refine_bellman_function(
                    inner_model,
                    node,
                    node.bellman_function,
                    opts.risk_measures[node_index],
                    outgoing_state,
                    items.duals,
                    items.supports,
                    items.probability,
                    items.objectives,
                )
            end
        end
        if optimizer !== nothing
            dt_vs = @elapsed _vertex_selection(
                node.bellman_function.global_theta,
                optimizer;
                tol = vertex_selection_tol,
            )
        else
            dt_vs = 0.0
        end
        if print_level > 0
            println(
                "Node: $(node_index) - ",
                "elapsed time: $(round(dt; digits = 2)) ",
                "plus $(round(dt_vs; digits = 2)) for vertex selection.",
            )
        end
    end
    return
end

"""
    inner_dp(
        build::Function,
        pb::SDDP.PolicyGraph;
        stages::Int,
        sense::Symbol = :Min,
        optimizer,
        lower_bound::Float64 = -Inf,
        upper_bound::Float64 = Inf,
        bellman_function,
        risk_measures::SDDP.AbstractRiskMeasure,
        print_level::Int = 1,
    )

Perform a single dynamic programming backwards iteration on the policy
graph `pb`, building inner approximations of the value functions at the
states sampled through a previous series of forward iterations.
The `build` function is needed because it constructs a new policy graph to
store the (inner) value function approximations.

Currently, only supports LinearPolicyGraphs.

## Keyword arguments

 - `stages::Int` number of stages in the policy graph.

 - `sense::Symbol` optimization sense of the problem.

 - `optimizer` optimizer to use.

 - `lower_bound::Float64` lower bound for minimization problems.

 - `upper_bound::Float64` upper bound for maximization problems.

 - `bellman_function` an InnerBellmanFunction object (including bounds
    and Lipschitz constant information).

 - `risk_measure::SDDP.AbstractRiskMeasure` risk measures for the problem.

 - `print_level::Int` level of verbosity.
"""
function inner_dp(
    build::Function,
    pb::SDDP.PolicyGraph;
    stages::Int,
    sense::Symbol = :Min,
    optimizer,
    lower_bound::Float64 = -Inf,
    upper_bound::Float64 = Inf,
    bellman_function,
    risk_measure::SDDP.AbstractRiskMeasure,
    print_level::Int = 1,
    vertex_selection_tol::Float64 = 1e-6,
)
    pb_inner = SDDP.LinearPolicyGraph(
        build;
        stages,
        sense,
        optimizer,
        lower_bound,
        upper_bound,
        bellman_function,
    )

    for (k, node) in pb_inner.nodes
        SDDP.set_objective(node)
    end

    opts = SDDP.Options(
        pb_inner,
        pb.initial_root_state;
        risk_measures = risk_measure,
    )
    T = model_type(pb_inner)
    total_dt = 0.0
    for node_index in sort(collect(keys(pb.nodes)); rev = true)[2:end]
        dt = @elapsed begin
            node = pb_inner[node_index]
            fw_samples =
                pb[node_index].bellman_function.global_theta.sampled_states
            for sampled_state in fw_samples
                outgoing_state = sampled_state.state
                items = SDDP.BackwardPassItems(T, SDDP.Noise)
                SDDP.solve_all_children(
                    pb_inner,
                    node,
                    items,
                    1.0,
                    nothing, # belief_state
                    nothing, # objective_state
                    outgoing_state,
                    opts.backward_sampling_scheme,
                    Tuple{T,Any}[], # scenario_path
                    opts.duality_handler,
                    opts,
                )
                SDDP.refine_bellman_function(
                    pb_inner,
                    node,
                    node.bellman_function,
                    opts.risk_measures[node_index],
                    outgoing_state,
                    items.duals,
                    items.supports,
                    items.probability,
                    items.objectives,
                )
            end
        end
        dt_vs = @elapsed _vertex_selection(
            node.bellman_function.global_theta,
            optimizer;
            tol = vertex_selection_tol,
        )
        if print_level > 0
            println(
                "Node: $(node_index) - ",
                "elapsed time: $(round(dt; digits = 2)) ",
                "plus $(round(dt_vs; digits = 2)) for vertex selection.",
            )
        end
        total_dt += dt + dt_vs
    end

    ub = SDDP.calculate_bound(pb_inner; risk_measure)
    if print_level > 0
        println("First-stage upper bound: ", ub)
        println("Total time for upper bound: ", total_dt)
    end
    return pb_inner, ub, total_dt
end

"""
    write_vertices_to_file(
        model::PolicyGraph{T},
        filename::String;
        kwargs...,
    ) where {T}

Write the vertices that form the policy in `model` to `filename` in JSON format.

## Keyword arguments

 - `node_name_parser` is a function which converts the name of each node into a
    string representation. It has the signature: `node_name_parser(::T)::String`.

 - `write_only_selected_vertices` write only the selected vertices to the json file.
    Defaults to false.

See also [`SDDP.read_vertices_from_file`](@ref) and [`SDDP.write_cuts_to_file`](@ref).
"""
function write_vertices_to_file(
    model::SDDP.PolicyGraph{T},
    filename::String;
    node_name_parser::Function = string,
    write_only_selected_vertices::Bool = false,
) where {T}
    vertices = Dict{String,Any}[]
    for (node_name, node) in model.nodes
        node_vertices = Dict(
            "node" => node_name_parser(node_name),
            "vertices" => Dict{String,Any}[],
            "multi_vertices" => Dict{String,Any}[],
            "risk_set_cuts" => Vector{Float64}[],
        )
        oracle = node.bellman_function.global_theta
        for vertex in oracle.vertices
            push!(
                node_vertices["vertices"],
                Dict("value" => vertex.value, "state" => copy(vertex.state)),
            )
        end
        # TODO(rjmalves): restore multi vertices and risk set when they are supported
        # for (i, theta) in enumerate(node.bellman_function.local_thetas)
        #     for vertex in theta.vertices
        #         if write_only_selected_vertices &&
        #            vertex.variable_ref === nothing
        #             continue
        #         end
        #         push!(
        #             node_vertices["multi_vertices"],
        #             Dict(
        #                 "realization" => i,
        #                 "value" => vertex.value,
        #                 "state" => copy(state.state),
        #             ),
        #         )
        #     end
        # end
        # for p in node.bellman_function.risk_set_cuts
        #     push!(node_vertices["risk_set_cuts"], p)
        # end
        push!(vertices, node_vertices)
    end
    open(filename, "w") do io
        return write(io, JSON.json(vertices))
    end
    return
end

function _vertex_name_parser(vertex_name::String)::String
    return vertex_name
end

function _get_vertex_info(
    json_vertex;
    vertex_name_parser::Function = _vertex_name_parser,
)
    value = json_vertex["value"]
    state = Dict(Symbol(k) => v for (k, v) in json_vertex["state"])
    obj_y = nothing
    belief_y = nothing
    return value, state, obj_y, belief_y
end

"""
    read_vertices_from_file(
        model::PolicyGraph{T},
        filename::String;
        kwargs...,
    ) where {T}

Read vertices from `filename` into `model`.

Since `T` can be an arbitrary Julia type, the conversion to JSON is lossy. When
reading, `read_vertices_from_file` only supports `T=Int`, `T=NTuple{N, Int}`,
and `T=Symbol` (as [`read_cuts_from_file`](@ref)). If you have manually created
a policy graph with a different node type `T`, provide a function
`node_name_parser` with the signature `node_name_parser(T, name::String)::T`.

## Keyword arguments

 - `node_name_parser(T, name::String)::T where {T}` that returns the name of each
    node given the string name `name`.
    If `node_name_parser` returns `nothing`, those cuts are skipped.

 - `vertex_name_parser::Function` that converts the name of each vertex in the
   file to the model, cf [`_get_vertex_info`](@ref)

 - `vertex_selection::Bool` run or not the exact vertex selection algorithm when
    adding vertices to the model; needs `optimizer` to be set.
"""
function read_vertices_from_file(
    model::SDDP.PolicyGraph{T},
    filename::String;
    node_name_parser::Function = SDDP._node_name_parser,
    vertex_name_parser::Function = _vertex_name_parser,
    vertex_selection::Bool = false,
    vertex_selection_tol::Float64 = 1e-6,
    optimizer = nothing,
) where {T}
    if vertex_selection && isnothing(optimizer)
        @warn "You must select an optimizer for performing vertex selection."
        throw(JuMP.NoOptimizer())
    end
    vertices = JSON.parsefile(filename; use_mmap = false)
    for node_info in vertices
        node_name = node_name_parser(T, node_info["node"])::Union{Nothing,T}
        node = model[node_name]
        bf = node.bellman_function
        # Loop through and add the vertices.
        for json_vertex in node_info["vertices"]
            value, state, obj_y, belief_y =
                _get_vertex_info(json_vertex; vertex_name_parser)
            _add_vertex(
                bf.global_theta,
                value,
                state,
                obj_y,
                belief_y;
                optimizer = optimizer,
            )
        end
        if vertex_selection
            _vertex_selection(
                bf.global_theta,
                optimizer;
                tol = vertex_selection_tol,
            )
        end

        # TODO(rjmalves): restore when multi vertices and risk set are supported
        # Loop through and add the multi-cuts. There are two parts:
        #  (i) the cuts w.r.t. the state variable x
        # (ii) the cuts that define the risk set
        # There is one additional complication: if these cuts are being read
        # into a new model, the local theta variables may not exist yet.
        # if length(node_info["risk_set_cuts"]) > 0
        #     _add_locals_if_necessary(
        #         node,
        #         bf,
        #         length(first(node_info["risk_set_cuts"])),
        #     )
        # end
        # for json_vertex in node_info["multi_vertices"]
        #     @error "Multi-vertices not yet implemented."
        #     value, state, obj_y, belief_y =
        #         _get_vertex_info(json_vertex; vertex_name_parser)
        #     _add_vertex(
        #         bf.local_thetas[json_vertex["realization"]],
        #         value,
        #         state,
        #         obj_y,
        #         belief_y,
        #     )
        # end
        # if vertex_selection && length(list) > 0
        #     for local_vf in bf.local_thetas
        #         _vertex_selection(
        #             local_vf,
        #             optimizer;
        #             tol = vertex_selection_tol,
        #         )
        #     end
        # end
        # # Here is part (ii): adding the constraints that define the risk-set
        # # representation of the risk measure.
        # for json_cut in node_info["risk_set_cuts"]
        #     expr = JuMP.@expression(
        #         node.subproblem,
        #         bf.global_theta.theta - sum(
        #             p * V.theta for (p, V) in zip(json_cut, bf.local_thetas)
        #         )
        #     )
        #     if JuMP.objective_sense(node.subproblem) == MOI.MIN_SENSE
        #         JuMP.@constraint(node.subproblem, expr >= 0)
        #     else
        #         JuMP.@constraint(node.subproblem, expr <= 0)
        #     end
        # end
    end
    return
end

end
