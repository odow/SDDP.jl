<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>An introduction to SDDP.jl · SDDP.jl</title><script data-outdated-warner src="../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../../"><img src="../../../assets/logo.png" alt="SDDP.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../../">SDDP.jl</a></span></div><form class="docs-search" action="../../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../../">Home</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox" checked/><label class="tocitem" for="menuitem-2"><span class="docs-label">Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><input class="collapse-toggle" id="menuitem-2-1" type="checkbox" checked/><label class="tocitem" for="menuitem-2-1"><span class="docs-label">Basic</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>An introduction to SDDP.jl</a><ul class="internal"><li><a class="tocitem" href="#What-is-a-node?"><span>What is a node?</span></a></li><li><a class="tocitem" href="#Policy-graphs"><span>Policy graphs</span></a></li><li><a class="tocitem" href="#More-notation"><span>More notation</span></a></li><li><a class="tocitem" href="#Assumptions"><span>Assumptions</span></a></li><li><a class="tocitem" href="#Dynamic-programming-and-subproblems"><span>Dynamic programming and subproblems</span></a></li><li><a class="tocitem" href="#Example:-hydro-thermal-scheduling"><span>Example: hydro-thermal scheduling</span></a></li><li><a class="tocitem" href="#Training-a-policy"><span>Training a policy</span></a></li><li><a class="tocitem" href="#Obtaining-the-decision-rule"><span>Obtaining the decision rule</span></a></li><li><a class="tocitem" href="#Simulating-the-policy"><span>Simulating the policy</span></a></li><li><a class="tocitem" href="#Obtaining-bounds"><span>Obtaining bounds</span></a></li><li><a class="tocitem" href="#Custom-recorders"><span>Custom recorders</span></a></li><li><a class="tocitem" href="#Extracting-the-marginal-water-values"><span>Extracting the marginal water values</span></a></li></ul></li><li><a class="tocitem" href="../03_objective_uncertainty/">Uncertainty in the objective function</a></li><li><a class="tocitem" href="../04_markov_uncertainty/">Markovian policy graphs</a></li><li><a class="tocitem" href="../05_plotting/">Plotting tools</a></li><li><a class="tocitem" href="../06_warnings/">Words of warning</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Advanced</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../advanced/11_objective_states/">Objective states</a></li><li><a class="tocitem" href="../../advanced/12_belief_states/">Belief states</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Theory</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../theory/21_theory_intro/">Introductory theory</a></li><li><a class="tocitem" href="../../theory/22_risk/">Risk aversion</a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">How-to guides</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../../guides/acess_previous_variables/">Access variables from a previous stage</a></li><li><a class="tocitem" href="../../../guides/add_a_multidimensional_state_variable/">Add a multi-dimensional state variable</a></li><li><a class="tocitem" href="../../../guides/add_a_risk_measure/">Add a risk measure</a></li><li><a class="tocitem" href="../../../guides/add_integrality/">Integrality</a></li><li><a class="tocitem" href="../../../guides/add_multidimensional_noise_Terms/">Add multi-dimensional noise terms</a></li><li><a class="tocitem" href="../../../guides/add_noise_in_the_constraint_matrix/">Add noise in the constraint matrix</a></li><li><a class="tocitem" href="../../../guides/choose_a_stopping_rule/">Choose a stopping rule</a></li><li><a class="tocitem" href="../../../guides/create_a_general_policy_graph/">Create a general policy graph</a></li><li><a class="tocitem" href="../../../guides/debug_a_model/">Debug a model</a></li><li><a class="tocitem" href="../../../guides/improve_computational_performance/">Improve computational performance</a></li><li><a class="tocitem" href="../../../guides/simulate_using_a_different_sampling_scheme/">Simulate using a different sampling scheme</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../../examples/FAST_hydro_thermal/">FAST: the hydro-thermal problem</a></li><li><a class="tocitem" href="../../../examples/FAST_production_management/">FAST: the production management problem</a></li><li><a class="tocitem" href="../../../examples/FAST_quickstart/">FAST: the quickstart problem</a></li><li><a class="tocitem" href="../../../examples/Hydro_thermal/">Hydro-thermal scheduling</a></li><li><a class="tocitem" href="../../../examples/StochDynamicProgramming.jl_multistock/">StochDynamicProgramming: the multistock problem</a></li><li><a class="tocitem" href="../../../examples/StochDynamicProgramming.jl_stock/">StochDynamicProgramming: the stock problem</a></li><li><a class="tocitem" href="../../../examples/StructDualDynProg.jl_prob5.2_2stages/">StructDualDynProg: Problem 5.2, 2 stages</a></li><li><a class="tocitem" href="../../../examples/StructDualDynProg.jl_prob5.2_3stages/">StructDualDynProg: Problem 5.2, 3 stages</a></li><li><a class="tocitem" href="../../../examples/agriculture_mccardle_farm/">The farm planning problem</a></li><li><a class="tocitem" href="../../../examples/air_conditioning/">Air conditioning</a></li><li><a class="tocitem" href="../../../examples/all_blacks/">Deterministic All Blacks</a></li><li><a class="tocitem" href="../../../examples/asset_management_simple/">Asset management</a></li><li><a class="tocitem" href="../../../examples/asset_management_stagewise/">Asset management with modifications</a></li><li><a class="tocitem" href="../../../examples/belief/">Partially observable inventory management</a></li><li><a class="tocitem" href="../../../examples/biobjective_hydro/">Biobjective hydro-thermal</a></li><li><a class="tocitem" href="../../../examples/booking_management/">Booking management</a></li><li><a class="tocitem" href="../../../examples/generation_expansion/">Generation expansion</a></li><li><a class="tocitem" href="../../../examples/hydro_valley/">Hydro valleys</a></li><li><a class="tocitem" href="../../../examples/infinite_horizon_hydro_thermal/">Infinite horizon hydro-thermal</a></li><li><a class="tocitem" href="../../../examples/infinite_horizon_trivial/">Infinite horizon trivial</a></li><li><a class="tocitem" href="../../../examples/no_strong_duality/">No strong duality</a></li><li><a class="tocitem" href="../../../examples/objective_state_newsvendor/">Newsvendor</a></li><li><a class="tocitem" href="../../../examples/sldp_example_one/">SLDP: example 1</a></li><li><a class="tocitem" href="../../../examples/sldp_example_two/">SLDP: example 2</a></li><li><a class="tocitem" href="../../../examples/stochastic_all_blacks/">Stochastic All Blacks</a></li><li><a class="tocitem" href="../../../examples/the_farmers_problem/">The farmer&#39;s problem</a></li><li><a class="tocitem" href="../../../examples/vehicle_location/">Vehicle location</a></li></ul></li><li><a class="tocitem" href="../../../apireference/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li><a class="is-disabled">Basic</a></li><li class="is-active"><a href>An introduction to SDDP.jl</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>An introduction to SDDP.jl</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/odow/SDDP.jl/blob/master/docs/src/tutorial/basic/01_first_steps.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="An-introduction-to-SDDP.jl"><a class="docs-heading-anchor" href="#An-introduction-to-SDDP.jl">An introduction to SDDP.jl</a><a id="An-introduction-to-SDDP.jl-1"></a><a class="docs-heading-anchor-permalink" href="#An-introduction-to-SDDP.jl" title="Permalink"></a></h1><p>SDDP.jl is a solver for multistage stochastic optimization problems. By <strong>multistage</strong>, we mean problems in which an agent makes a sequence of decisions over time. By <strong>stochastic</strong>, we mean that the agent is making decisions in the presence of uncertainty that is gradually revealed over the multiple stages.</p><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>Multistage stochastic programming has a lot in common with fields like stochastic optimal control, approximate dynamic programming, Markov decision processes, and reinforcement learning. If it helps, you can think of SDDP as Q-learning in which we approximate the value function using linear programming duality.</p></div></div><p>This tutorial is in two parts. First, it is an introduction to the background notation and theory we need, and second, it solves a simple multistage stochastic programming problem.</p><h2 id="What-is-a-node?"><a class="docs-heading-anchor" href="#What-is-a-node?">What is a node?</a><a id="What-is-a-node?-1"></a><a class="docs-heading-anchor-permalink" href="#What-is-a-node?" title="Permalink"></a></h2><p>A common feature of multistage stochastic optimization problems is that they model an agent controlling a system over time. To simplify things initially, we&#39;re going to start by describing what happens at an instant in time at which the agent makes a decision. Only after this will we extend our problem to multiple stages and the notion of time.</p><p>A <strong>node</strong> is a place at which the agent makes a decision.</p><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>For readers with a stochastic programming background, &quot;node&quot; is synonymous with &quot;stage&quot; in this section. However, for reasons that will become clear shortly, there can be more than one &quot;node&quot; per instant in time, which is why we prefer the term &quot;node&quot; over &quot;stage.&quot;</p></div></div><h3 id="States,-controls,-and-random-variables"><a class="docs-heading-anchor" href="#States,-controls,-and-random-variables">States, controls, and random variables</a><a id="States,-controls,-and-random-variables-1"></a><a class="docs-heading-anchor-permalink" href="#States,-controls,-and-random-variables" title="Permalink"></a></h3><p>The system that we are modeling can be described by three types of variables.</p><ol><li><p><strong>State</strong> variables track a property of the system over time.</p><p>Each node has an associated <em>incoming</em> state variable (the value of the state at the start of the node), and an <em>outgoing</em> state variable (the value of the state at the end of the node).</p><p>Examples of state variables include the volume of water in a reservoir, the number of units of inventory in a warehouse, or the spatial position of a moving vehicle.</p><p>Because state variables track the system over time, each node must have the same set of state variables.</p><p>We denote state variables by the letter <span>$x$</span> for the incoming state variable and <span>$x^\prime$</span> for the outgoing state variable.</p></li><li><p><strong>Control</strong> variables are actions taken (implicitly or explicitly) by the agent within a node which modify the state variables.</p><p>Examples of control variables include releases of water from the reservoir, sales or purchasing decisions, and acceleration or braking of the vehicle.</p><p>Control variables are local to a node <span>$i$</span>, and they can differ between nodes. For example, some control variables may be available within certain nodes.</p><p>We denote control variables by the letter <span>$u$</span>.</p></li><li><p><strong>Random</strong> variables are finite, discrete, exogenous random variables that the agent observes at the start of a node, before the control variables are decided.</p><p>Examples of random variables include rainfall inflow into a reservoir, probabilistic perishing of inventory, and steering errors in a vehicle.</p><p>Random variables are local to a node <span>$i$</span>, and they can differ between nodes. For example, some nodes may have random variables, and some nodes may not.</p><p>We denote random variables by the Greek letter <span>$\omega$</span> and the sample space from which they are drawn by <span>$\Omega_i$</span>. The probability of sampling <span>$\omega$</span> is denoted <span>$p_{\omega}$</span> for simplicity.</p><p>Importantly, the random variable associated with node <span>$i$</span> is independent of the random variables in all other nodes.</p></li></ol><h3 id="Dynamics"><a class="docs-heading-anchor" href="#Dynamics">Dynamics</a><a id="Dynamics-1"></a><a class="docs-heading-anchor-permalink" href="#Dynamics" title="Permalink"></a></h3><p>In a node <span>$i$</span>, the three variables are related by a <strong>transition function</strong>, which maps the incoming state, the controls, and the random variables to the outgoing state as follows: <span>$x^\prime = T_i(x, u, \omega)$</span>.</p><p>As a result of entering a node <span>$i$</span> with the incoming state <span>$x$</span>, observing random variable <span>$\omega$</span>, and choosing control <span>$u$</span>, the agent incurs a cost <span>$C_i(x, u, \omega)$</span>. (If the agent is a maximizer, this can be a profit, or a negative cost.) We call <span>$C_i$</span> the <strong>stage objective</strong>.</p><p>To choose their control variables in node <span>$i$</span>, the agent uses a <strong>decision</strong> <strong>rule</strong> <span>$u = \pi_i(x, \omega)$</span>, which is a function that maps the incoming state variable and observation of the random variable to a control <span>$u$</span>. This control must satisfy some feasibilty requirements <span>$u \in U_i(x, \omega)$</span>.</p><p>Here is a schematic which we can use to visualize a single node:</p><p><img src="../../../assets/hazard_decision.png" alt="Hazard-decision node"/></p><h2 id="Policy-graphs"><a class="docs-heading-anchor" href="#Policy-graphs">Policy graphs</a><a id="Policy-graphs-1"></a><a class="docs-heading-anchor-permalink" href="#Policy-graphs" title="Permalink"></a></h2><p>Now that we have a node, we need to connect multiple nodes together to form a multistage stochastic program. We call the graph created by connecting nodes together a <strong>policy graph</strong>.</p><p>The simplest type of policy graph is a <strong>linear policy graph</strong>. Here&#39;s a linear policy graph with three nodes:</p><p><img src="../../../assets/stochastic_linear_policy_graph.png" alt="Linear policy graph"/></p><p>Here we have dropped the notations inside each node and replaced them by a label (1, 2, and 3) to represent nodes <code>i=1</code>, <code>i=2</code>, and <code>i=3</code>.</p><p>In addition to nodes 1, 2, and 3, there is also a root node (the circle), and three arcs. Each arc has an origin node and a destination node, like <code>1 =&gt; 2</code>, and a corresponding probability of transitioning from the origin to the destination. Unless specified, we assume that the arc probabilities are uniform over the number of outgoing arcs. Thus, in this picture the arc probabilities are all 1.0.</p><p>State variables flow long the arcs of the graph. Thus, the outgoing state variable <span>$x^\prime$</span> from node 1 becomes the incoming state variable <span>$x$</span> to node 2, and so on.</p><p>We denote the set of nodes by <span>$\mathcal{N}$</span>, the root node by <span>$R$</span>, and the probability of transitioning from node <span>$i$</span> to node <span>$j$</span> by <span>$p_{ij}$</span>. (If no arc exists, then <span>$p_{ij} = 0$</span>.) We define the set of successors of node <span>$i$</span> as <span>$i^+ = \{j \in \mathcal{N} | p_{ij} &gt; 0\}$</span>.</p><p>Each node in the graph corresponds to a place at which the agent makes a decision, and we call moments in time at which the agent makes a decision <strong>stages</strong>. By convention, we try to draw policy graphs from left-to-right, with the stages as columns. There can be more than one node in a stage! Here&#39;s an example of a structure we call <strong>Markovian policy graphs</strong>:</p><p><img src="../../../assets/enso_markovian.png" alt="Markovian policy graph"/></p><p>Here each column represents a moment in time, the squiggly lines represent stochastic rainfall, and the rows represent the world in two discrete states: El Niño and La Niña. In the El Niño states, the distribution of the rainfall random variable is different to the distribution of the rainfall random variable in the La Niña states, and there is some switching probability between the two states that can be modelled by a Markov chain.</p><p>Moreover, policy graphs can have cycles! This allows them to model infinite horizon problems. Here&#39;s another example, taken from the paper <a href="https://doi.org/10.1002/net.21932">Dowson (2020)</a>:</p><p><img src="../../../assets/powder_policy_graph.png" alt="POWDer policy graph"/></p><p>The columns represent time, and the rows represent different states of the world. In this case, the rows represent different prices that milk can be sold for at the end of each year. The squiggly lines denote a multivariate random variable that models the weekly amount of rainfall that occurs.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The sum of probabilities on the outgoing arcs of node <span>$i$</span> can be less than 1, i.e., <span>$\sum\limits_{j\in i^+} p_{ij} \le 1$</span>. What does this mean? One interpretation is that the probability is a <a href="https://en.wikipedia.org/wiki/Discounting">discount factor</a>. Another interpretation is that there is an implicit &quot;zero&quot; node that we have not modeled, with <span>$p_{i0} = 1 - \sum\limits_{j\in i^+} p_{ij}$</span>. This zero node has <span>$C_0(x, u, \omega) = 0$</span>, and <span>$0^+ = \varnothing$</span>.</p></div></div><h2 id="More-notation"><a class="docs-heading-anchor" href="#More-notation">More notation</a><a id="More-notation-1"></a><a class="docs-heading-anchor-permalink" href="#More-notation" title="Permalink"></a></h2><p>Recall that each node <span>$i$</span> has a <strong>decision rule</strong> <span>$u = \pi_i(x, \omega)$</span>, which is a function that maps the incoming state variable and observation of the random variable to a control <span>$u$</span>.</p><p>The set of decision rules, with one element for each node in the policy graph, is called a <strong>policy</strong>.</p><p>The goal of the agent is to find a policy that minimizes the expected cost of starting at the root node with some initial condition <span>$x_R$</span>, and proceeding from node to node along the probabilistic arcs until they reach a node with no outgoing arcs (or it reaches an implicit &quot;zero&quot; node).</p><p class="math-container">\[\min_{\pi} \mathbb{E}_{i \in R^+, \omega \in \Omega_i}[V_i^\pi(x_R, \omega)],\]</p><p>where</p><p class="math-container">\[V_i^\pi(x, \omega) = C_i(x, u, \omega) + \mathbb{E}_{j \in i^+, \varphi \in \Omega_j}[V_j(x^\prime, \varphi)],\]</p><p>where <span>$u = \pi_i(x, \omega) \in U_i(x, \omega)$</span>, and <span>$x^\prime = T_i(x, u, \omega)$</span>.</p><p>The expectations are a bit complicated, but they are equivalent to:</p><p class="math-container">\[\mathbb{E}_{j \in i^+, \varphi \in \Omega_j}[V_j(x^\prime, \varphi)] = \sum\limits_{j \in i^+} p_{ij} \sum\limits_{\varphi \in \Omega_j} p_{\varphi}V_j(x^\prime, \varphi).\]</p><p>An optimal policy is the set of decision rules that the agent can use to make decisions and achieve the smallest expected cost.</p><h2 id="Assumptions"><a class="docs-heading-anchor" href="#Assumptions">Assumptions</a><a id="Assumptions-1"></a><a class="docs-heading-anchor-permalink" href="#Assumptions" title="Permalink"></a></h2><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>This section is important!</p></div></div><p>The space of problems you can model with this framework is very large. Too large, in fact, for us to form tractable solution algorithms for! Stochastic dual dynamic programming requires the following assumptions in order to work:</p><p><strong>Assumption 1: finite nodes</strong></p><p>There is a finite number of nodes in <span>$\mathcal{N}$</span>.</p><p><strong>Assumption 2: finite random variables</strong></p><p>The sample space <span>$\Omega_i$</span> is finite and discrete for each node <span>$i\in\mathcal{N}$</span>.</p><p><strong>Assumption 3: convex problems</strong></p><p>Given fixed <span>$\omega$</span>, <span>$C_i(x, u, \omega)$</span> is a convex function, <span>$T_i(x, u, \omega)$</span> is linear, and  <span>$U_i(x, u, \omega)$</span> is a non-empty, bounded convex set with respect to <span>$x$</span> and <span>$u$</span>.</p><p><strong>Assumption 4: no infinite loops</strong></p><p>For all loops in the policy graph, the product of the arc transition probabilities around the loop is strictly less than 1.</p><p><strong>Assumption 5: relatively complete recourse</strong></p><p>This is a technical but important assumption. See <a href="../06_warnings/#Relatively-complete-recourse">Relatively complete recourse</a> for more details.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>SDDP.jl relaxes assumption (3) to allow for integer state and control variables, but we won&#39;t go into the details here. Assumption (4) essentially means that we obtain a discounted-cost solution for infinite-horizon problems, instead of an average-cost solution; see <a href="https://doi.org/10.1002/net.21932">Dowson (2020)</a> for details.</p></div></div><h2 id="Dynamic-programming-and-subproblems"><a class="docs-heading-anchor" href="#Dynamic-programming-and-subproblems">Dynamic programming and subproblems</a><a id="Dynamic-programming-and-subproblems-1"></a><a class="docs-heading-anchor-permalink" href="#Dynamic-programming-and-subproblems" title="Permalink"></a></h2><p>Now that we have formulated our problem, we need some ways of computing optimal decision rules. One way is to just use a heuristic like &quot;choose a control randomally from the set of feasible controls.&quot; However, such a policy is unlikely to be optimal.</p><p>A better way of obtaining an optimal policy is to use <a href="https://en.wikipedia.org/wiki/Bellman_equation#Bellman&#39;s_principle_of_optimality">Bellman&#39;s principle of optimality</a>, a.k.a Dynamic Programming, and define a recursive <strong>subproblem</strong> as follows:</p><p class="math-container">\[\begin{aligned}
V_i(x, \omega) = \min\limits_{\bar{x}, x^\prime, u} \;\; &amp; C_i(\bar{x}, u, \omega) + \mathbb{E}_{j \in i^+, \varphi \in \Omega_j}[V_j(x^\prime, \varphi)]\\
&amp; x^\prime = T_i(\bar{x}, u, \omega) \\
&amp; u \in U_i(\bar{x}, \omega) \\
&amp; \bar{x} = x.
\end{aligned}\]</p><p>Our decision rule, <span>$\pi_i(x, \omega)$</span>, solves this optimization problem and returns a <span>$u^*$</span> corresponding to an optimal solution.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>We add <span>$\bar{x}$</span> as a decision variable, along with the fishing constraint <span>$\bar{x} = x$</span> for two reasons: it makes it obvious that formulating a problem with <span>$x \times u$</span> results in a bilinear program instead of a linear program (see Assumption 3), and it simplifies the implementation of the SDDP algorithm.</p></div></div><p>These subproblems are very difficult to solve exactly, because they involve recursive optimization problems with lots of nested expectations.</p><p>Therefore, instead of solving them exactly, SDDP.jl works by iteratively approximating the expectation term of each subproblem, which is also called the cost-to-go term. For now, you don&#39;t need to understand the details, other than that there is a nasty cost-to-go term that we deal with behind-the-scenes.</p><p>The subproblem view of a multistage stochastic program is also important, because it provides a convienient way of communicating the different parts of the broader problem, and it is how we will communicate the problem to SDDP.jl. All we need to do is drop the cost-to-go term and fishing constraint, and define a new subproblem <code>SP</code> as:</p><p class="math-container">\[\begin{aligned}
\texttt{SP}_i(x, \omega) : \min\limits_{\bar{x}, x^\prime, u} \;\; &amp; C_i(\bar{x}, u, \omega) \\
&amp; x^\prime = T_i(\bar{x}, u, \omega) \\
&amp; u \in U_i(\bar{x}, \omega).
\end{aligned}\]</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>When we talk about formulating a <strong>subproblem</strong> with SDDP.jl, this is the formulation we mean.</p></div></div><p>We&#39;ve retained the transition function and uncertainty set because they help to motivate the different components of the subproblem. However, in general, the subproblem can be more general. A better (less restrictive) representation might be:</p><p class="math-container">\[\begin{aligned}
\texttt{SP}_i(x, \omega) : \min\limits_{\bar{x}, x^\prime, u} \;\; &amp; C_i(\bar{x}, x^\prime, u, \omega) \\
&amp; (\bar{x}, x^\prime, u) \in \mathcal{X}_i(\omega).
\end{aligned}\]</p><p>Note that the outgoing state variable can appear in the objective, and we can add constraints involving the incoming and outgoing state variables. It should be obvious how to map between the two representations.</p><h2 id="Example:-hydro-thermal-scheduling"><a class="docs-heading-anchor" href="#Example:-hydro-thermal-scheduling">Example: hydro-thermal scheduling</a><a id="Example:-hydro-thermal-scheduling-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-hydro-thermal-scheduling" title="Permalink"></a></h2><p>Hydrothermal scheduling is the most common application of stochastic dual dynamic programming. To illustrate some of the basic functionality of <code>SDDP.jl</code>, we implement a very simple model of the hydrothermal scheduling problem.</p><h3 id="Problem-statement"><a class="docs-heading-anchor" href="#Problem-statement">Problem statement</a><a id="Problem-statement-1"></a><a class="docs-heading-anchor-permalink" href="#Problem-statement" title="Permalink"></a></h3><p>We consider the problem of scheduling electrical generation over three weeks in order to meet a known demand of 150 MWh in each week.</p><p>There are two generators: a thermal generator, and a hydro generator. In each week, the agent needs to decide how much energy to generate from thermal, and how much energy to generate from hydro.</p><p>The thermal generator has a short-run marginal cost of <span>$</span>50/MWh in the first stage, <span>$</span>100/MWh in the second stage, and <span>$</span>150/MWh in the third stage.</p><p>The hydro generator has a short-run marginal cost of <span>$</span>0/MWh.</p><p>The hydro generator draws water from a reservoir which has a maximum capacity of 200 MWh. (Although water is usually measured in m³, we measure it in the energy-equivalent MWh to simplify things. In practice, there is a conversion function between m³ flowing throw the turbine and MWh.) At the start of the first time period, the reservoir is full.</p><p>In addition to the ability to generate electricity by passing water through the hydroelectric turbine, the hydro generator can also spill water down a spillway (bypassing the turbine) in order to prevent the water from over-topping the dam. We assume that there is no cost of spillage.</p><p>In addtion to water leaving the reservoir, water that flows into the reservoir through rainfall or rivers are referred to as inflows. These inflows are uncertain, and are the cause of the main trade-off in hydro-thermal scheduling: the desire to use water now to generate cheap electricity, against the risk that future inflows will be low, leading to blackouts or expensive thermal generation.</p><p>For our simple model, we assume that the inflows can be modelled by a discrete distribution with the three outcomes given in the following table:</p><table><tr><th style="text-align: right">ω</th><th style="text-align: right">0</th><th style="text-align: right">50</th><th style="text-align: right">100</th></tr><tr><td style="text-align: right">P(ω)</td><td style="text-align: right">1/3</td><td style="text-align: right">1/3</td><td style="text-align: right">1/3</td></tr></table><p>The value of the noise (the random variable) is observed by the agent at the start of each stage. This makes the problem a <em>wait-and-see</em> or <em>hazard-decision</em> formulation.</p><p>The goal of the agent is to minimize the expected cost of generation over the three weeks.</p><h3 id="Formulating-the-problem"><a class="docs-heading-anchor" href="#Formulating-the-problem">Formulating the problem</a><a id="Formulating-the-problem-1"></a><a class="docs-heading-anchor-permalink" href="#Formulating-the-problem" title="Permalink"></a></h3><p>Before going further, we need to load SDDP.jl:</p><pre><code class="language-julia hljs">using SDDP</code></pre><h4 id="Graph-structure"><a class="docs-heading-anchor" href="#Graph-structure">Graph structure</a><a id="Graph-structure-1"></a><a class="docs-heading-anchor-permalink" href="#Graph-structure" title="Permalink"></a></h4><p>First, we need to identify the structre of the policy graph. From the problem statement, we want to model the problem over three weeks in weekly stages. Therefore, the policy graph is a linear graph with three stages:</p><pre><code class="language-julia hljs">graph = SDDP.LinearGraph(3)</code></pre><pre><code class="nohighlight hljs">Root
 0
Nodes
 1
 2
 3
Arcs
 0 =&gt; 1 w.p. 1.0
 1 =&gt; 2 w.p. 1.0
 2 =&gt; 3 w.p. 1.0
</code></pre><h4 id="Building-the-subproblem"><a class="docs-heading-anchor" href="#Building-the-subproblem">Building the subproblem</a><a id="Building-the-subproblem-1"></a><a class="docs-heading-anchor-permalink" href="#Building-the-subproblem" title="Permalink"></a></h4><p>Next, we need to construct the associated subproblem for each node in <code>graph</code>. To do so, we need to provide SDDP.jl a function which takes two arguments. The first is <code>subproblem::Model</code>, which is an empty JuMP model. The second is <code>node</code>, which is the name of each node in the policy graph. If the graph is linear, SDDP defaults to naming the nodes using the integers in <code>1:T</code>. Here&#39;s an example that we are going to flesh out over the next few paragraphs:</p><pre><code class="language-julia hljs">function subproblem_builder(subproblem::Model, node::Int)
    # ... stuff to go here ...
    return subproblem
end</code></pre><pre><code class="nohighlight hljs">subproblem_builder (generic function with 1 method)</code></pre><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>If you use a different type of graph, <code>node</code> may be a type different to <code>Int</code>. For example, in <a href="../../../apireference/#SDDP.MarkovianGraph"><code>SDDP.MarkovianGraph</code></a>, <code>node</code> is a <code>Tuple{Int,Int}</code>.</p></div></div><h4 id="State-variables"><a class="docs-heading-anchor" href="#State-variables">State variables</a><a id="State-variables-1"></a><a class="docs-heading-anchor-permalink" href="#State-variables" title="Permalink"></a></h4><p>The first part of the subproblem we need to identify are the state variables. Since we only have one reservoir, there is only one state variable, <code>volume</code>, the volume of water in the reservoir [MWh].</p><p>The volume had bounds of <code>[0, 200]</code>, and the reservoir was full at the start of time, so <span>$x_R = 200$</span>.</p><p>We add state variables to our <code>subproblem</code> using JuMP&#39;s <code>@variable</code> macro. However, in addition to the usual syntax, we also pass <code>SDDP.State</code>, and we need to provide the initial value (<span>$x_R$</span>) using the <code>initial_value</code> keyword.</p><pre><code class="language-julia hljs">function subproblem_builder(subproblem::Model, node::Int)
    # State variables
    @variable(subproblem, 0 &lt;= volume &lt;= 200, SDDP.State, initial_value = 200)
    return subproblem
end</code></pre><pre><code class="nohighlight hljs">subproblem_builder (generic function with 1 method)</code></pre><p>The syntax for adding a state variable is a little obtuse, because <code>volume</code> is not single JuMP variable. Instead, <code>volume</code> is a struct with two fields, <code>.in</code> and <code>.out</code>, corresponding to the incoming and outgoing state variables respectively.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>We don&#39;t need to add the fishing constraint <span>$\bar{x} = x$</span>; SDDP.jl does this automatically.</p></div></div><h4 id="Control-variables"><a class="docs-heading-anchor" href="#Control-variables">Control variables</a><a id="Control-variables-1"></a><a class="docs-heading-anchor-permalink" href="#Control-variables" title="Permalink"></a></h4><p>The next part of the subproblem we need to identiy are the control variables. The control variables for our problem are:</p><ul><li><code>thermal_generation</code>: the quantity of energy generated from thermal [MWh/week]</li><li><code>hydro_generation</code>: the quantity of energy generated from hydro [MWh/week]</li><li><code>hydro_spill</code>: the volume of water spilled from the reservoir in each week [MWh/week]</li></ul><p>Each of these variables is non-negative.</p><p>We add control variables to our <code>subproblem</code> as normal JuMP variables, using <code>@variable</code> or <code>@variables</code>:</p><pre><code class="language-julia hljs">function subproblem_builder(subproblem::Model, node::Int)
    # State variables
    @variable(subproblem, 0 &lt;= volume &lt;= 200, SDDP.State, initial_value = 200)
    # Control variables
    @variables(subproblem, begin
        thermal_generation &gt;= 0
        hydro_generation &gt;= 0
        hydro_spill &gt;= 0
    end)
    return subproblem
end</code></pre><pre><code class="nohighlight hljs">subproblem_builder (generic function with 1 method)</code></pre><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>Modeling is an art, and a tricky part of that art is figuring out which variables are state variables, and which are control variables. A good rule is: if you need a value of a control variable in some future node to make a decision, it is a state variable instead.</p></div></div><h4 id="Random-variables"><a class="docs-heading-anchor" href="#Random-variables">Random variables</a><a id="Random-variables-1"></a><a class="docs-heading-anchor-permalink" href="#Random-variables" title="Permalink"></a></h4><p>The next step is to identify any random variables. In our example, we had</p><ul><li><code>inflow</code>: the quantity of water that flows into the reservoir each week [MWh/week]</li></ul><p>To add an uncertain variable to the model, we create a new JuMP variable <code>inflow</code>, and then call the function <a href="../../../apireference/#SDDP.parameterize"><code>SDDP.parameterize</code></a>. The <a href="../../../apireference/#SDDP.parameterize"><code>SDDP.parameterize</code></a> function takes three arguments: the subproblem, a vector of realizations, and a corresponding vector of probabilities.</p><pre><code class="language-julia hljs">function subproblem_builder(subproblem::Model, node::Int)
    # State variables
    @variable(subproblem, 0 &lt;= volume &lt;= 200, SDDP.State, initial_value = 200)
    # Control variables
    @variables(subproblem, begin
        thermal_generation &gt;= 0
        hydro_generation &gt;= 0
        hydro_spill &gt;= 0
    end)
    # Random variables
    @variable(subproblem, inflow)
    Ω = [0.0, 50.0, 100.0]
    P = [1 / 3, 1 / 3, 1 / 3]
    SDDP.parameterize(subproblem, Ω, P) do ω
        return JuMP.fix(inflow, ω)
    end
    return subproblem
end</code></pre><pre><code class="nohighlight hljs">subproblem_builder (generic function with 1 method)</code></pre><p>Note how we use the JuMP function <a href="http://jump.dev/JuMP.jl/v0.21/variables/#JuMP.fix"><code>JuMP.fix</code></a> to set the value of the <code>inflow</code> variable to <code>ω</code>.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><a href="../../../apireference/#SDDP.parameterize"><code>SDDP.parameterize</code></a> can only be called once in each subproblem definition! If your random variable is multi-variate, read <a href="../../../guides/add_multidimensional_noise_Terms/#Add-multi-dimensional-noise-terms">Add multi-dimensional noise terms</a>.</p></div></div><h4 id="Transition-function-and-contraints"><a class="docs-heading-anchor" href="#Transition-function-and-contraints">Transition function and contraints</a><a id="Transition-function-and-contraints-1"></a><a class="docs-heading-anchor-permalink" href="#Transition-function-and-contraints" title="Permalink"></a></h4><p>Now that we&#39;ve identified our variables, we can define the transition function and the constraints.</p><p>For our problem, the state variable is the volume of water in the reservoir. The volume of water decreases in response to water being used for hydro generation and spillage. So the transition function is: <code>volume.out = volume.in - hydro_generation - hydro_spill + inflow</code>. (Note how we use <code>volume.in</code> and <code>volume.out</code> to refer to the incoming and outgoing state variables.)</p><p>There is also a constraint that the total generation must sum to 150 MWh.</p><p>Both the transition function and any additional constraint are added using JuMP&#39;s <code>@constraint</code> and <code>@constraints</code> macro.</p><pre><code class="language-julia hljs">function subproblem_builder(subproblem::Model, node::Int)
    # State variables
    @variable(subproblem, 0 &lt;= volume &lt;= 200, SDDP.State, initial_value = 200)
    # Control variables
    @variables(subproblem, begin
        thermal_generation &gt;= 0
        hydro_generation &gt;= 0
        hydro_spill &gt;= 0
    end)
    # Random variables
    @variable(subproblem, inflow)
    Ω = [0.0, 50.0, 100.0]
    P = [1 / 3, 1 / 3, 1 / 3]
    SDDP.parameterize(subproblem, Ω, P) do ω
        return JuMP.fix(inflow, ω)
    end
    # Transition function and constraints
    @constraints(
        subproblem,
        begin
            volume.out == volume.in - hydro_generation - hydro_spill + inflow
            demand_constraint, hydro_generation + thermal_generation == 150
        end
    )
    return subproblem
end</code></pre><pre><code class="nohighlight hljs">subproblem_builder (generic function with 1 method)</code></pre><h4 id="Objective-function"><a class="docs-heading-anchor" href="#Objective-function">Objective function</a><a id="Objective-function-1"></a><a class="docs-heading-anchor-permalink" href="#Objective-function" title="Permalink"></a></h4><p>Finally, we need to add an objective function using <code>@stageobjective</code>. The objective of the agent is to minimize the cost of thermal generation. This is complicated by a fuel cost that depends on the <code>node</code>.</p><p>One possibility is to use an <code>if</code> statement on <code>node</code> to define the correct objective:</p><pre><code class="language-julia hljs">function subproblem_builder(subproblem::Model, node::Int)
    # State variables
    @variable(subproblem, 0 &lt;= volume &lt;= 200, SDDP.State, initial_value = 200)
    # Control variables
    @variables(subproblem, begin
        thermal_generation &gt;= 0
        hydro_generation &gt;= 0
        hydro_spill &gt;= 0
    end)
    # Random variables
    @variable(subproblem, inflow)
    Ω = [0.0, 50.0, 100.0]
    P = [1 / 3, 1 / 3, 1 / 3]
    SDDP.parameterize(subproblem, Ω, P) do ω
        return JuMP.fix(inflow, ω)
    end
    # Transition function and constraints
    @constraints(
        subproblem,
        begin
            volume.out == volume.in - hydro_generation - hydro_spill + inflow
            demand_constraint, hydro_generation + thermal_generation == 150
        end
    )
    # Stage-objective
    if node == 1
        @stageobjective(subproblem, 50 * thermal_generation)
    elseif node == 2
        @stageobjective(subproblem, 100 * thermal_generation)
    else
        @assert node == 3
        @stageobjective(subproblem, 150 * thermal_generation)
    end
    return subproblem
end</code></pre><pre><code class="nohighlight hljs">subproblem_builder (generic function with 1 method)</code></pre><p>A second possibility is to use an array of fuel costs, and use <code>node</code> to index the correct value:</p><pre><code class="language-julia hljs">function subproblem_builder(subproblem::Model, node::Int)
    # State variables
    @variable(subproblem, 0 &lt;= volume &lt;= 200, SDDP.State, initial_value = 200)
    # Control variables
    @variables(subproblem, begin
        thermal_generation &gt;= 0
        hydro_generation &gt;= 0
        hydro_spill &gt;= 0
    end)
    # Random variables
    @variable(subproblem, inflow)
    Ω = [0.0, 50.0, 100.0]
    P = [1 / 3, 1 / 3, 1 / 3]
    SDDP.parameterize(subproblem, Ω, P) do ω
        return JuMP.fix(inflow, ω)
    end
    # Transition function and constraints
    @constraints(
        subproblem,
        begin
            volume.out == volume.in - hydro_generation - hydro_spill + inflow
            demand_constraint, hydro_generation + thermal_generation == 150
        end
    )
    # Stage-objective
    fuel_cost = [50, 100, 150]
    @stageobjective(subproblem, fuel_cost[node] * thermal_generation)
    return subproblem
end</code></pre><pre><code class="nohighlight hljs">subproblem_builder (generic function with 1 method)</code></pre><h3 id="Contructing-the-model"><a class="docs-heading-anchor" href="#Contructing-the-model">Contructing the model</a><a id="Contructing-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#Contructing-the-model" title="Permalink"></a></h3><p>Now that we&#39;ve written our subproblem, we need to contruct the full model. For that, we&#39;re going to need a linear solver. Let&#39;s choose GLPK:</p><pre><code class="language-julia hljs">using GLPK</code></pre><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>In larger problems, you should use a more robust commercial LP solver like Gurobi. Read <a href="../06_warnings/#Words-of-warning">Words of warning</a> for more details.</p></div></div><p>Then, we can create a full model using <a href="../../../apireference/#SDDP.PolicyGraph"><code>SDDP.PolicyGraph</code></a>, passing our <code>subproblem_builder</code> function as the first argument, and our <code>graph</code> as the second:</p><pre><code class="language-julia hljs">model = SDDP.PolicyGraph(
    subproblem_builder,
    graph;
    sense = :Min,
    lower_bound = 0.0,
    optimizer = GLPK.Optimizer,
)</code></pre><pre><code class="nohighlight hljs">A policy graph with 3 nodes.
 Node indices: 1, 2, 3
</code></pre><ul><li><code>sense</code>: the optimization sense. Must be <code>:Min</code> or <code>:Max</code>.</li><li><code>lower_bound</code>: you <em>must</em> supply a valid bound on the objective. For our problem, we know that we cannot incur a negative cost so <span>$</span>0 is a valid lower bound.</li><li><code>optimizer</code>: This is borrowed directly from JuMP&#39;s <code>Model</code> constructor: <code>Model(GLPK.Optimizer)</code></li></ul><p>Because linear policy graphs are the most commonly used structure, we can use <a href="../../../apireference/#SDDP.LinearPolicyGraph"><code>SDDP.LinearPolicyGraph</code></a> instead of passing <code>SDDP.LinearGraph(3)</code> to <a href="../../../apireference/#SDDP.PolicyGraph"><code>SDDP.PolicyGraph</code></a>.</p><pre><code class="language-julia hljs">model = SDDP.LinearPolicyGraph(
    subproblem_builder;
    stages = 3,
    sense = :Min,
    lower_bound = 0.0,
    optimizer = GLPK.Optimizer,
)</code></pre><pre><code class="nohighlight hljs">A policy graph with 3 nodes.
 Node indices: 1, 2, 3
</code></pre><p>There is also the option is to use Julia&#39;s <code>do</code> syntax to avoid needing to define a <code>subproblem_builder</code> function separately:</p><pre><code class="language-julia hljs">model = SDDP.LinearPolicyGraph(
    stages = 3,
    sense = :Min,
    lower_bound = 0.0,
    optimizer = GLPK.Optimizer,
) do subproblem, node
    # State variables
    @variable(subproblem, 0 &lt;= volume &lt;= 200, SDDP.State, initial_value = 200)
    # Control variables
    @variables(subproblem, begin
        thermal_generation &gt;= 0
        hydro_generation &gt;= 0
        hydro_spill &gt;= 0
    end)
    # Random variables
    @variable(subproblem, inflow)
    Ω = [0.0, 50.0, 100.0]
    P = [1 / 3, 1 / 3, 1 / 3]
    SDDP.parameterize(subproblem, Ω, P) do ω
        return JuMP.fix(inflow, ω)
    end
    # Transition function and constraints
    @constraints(
        subproblem,
        begin
            volume.out == volume.in - hydro_generation - hydro_spill + inflow
            demand_constraint, hydro_generation + thermal_generation == 150
        end
    )
    # Stage-objective
    if node == 1
        @stageobjective(subproblem, 50 * thermal_generation)
    elseif node == 2
        @stageobjective(subproblem, 100 * thermal_generation)
    else
        @assert node == 3
        @stageobjective(subproblem, 150 * thermal_generation)
    end
end</code></pre><pre><code class="nohighlight hljs">A policy graph with 3 nodes.
 Node indices: 1, 2, 3
</code></pre><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Julia&#39;s <code>do</code> syntax is just a different way of passing an anonymous function <code>inner</code> to some function <code>outer</code> which takes <code>inner</code> as the first argument. For example, given:</p><pre><code class="language-julia hljs">outer(inner::Function, x, y) = inner(x, y)</code></pre><p>then</p><pre><code class="language-julia hljs">outer(1, 2) do x, y
    return x^2 + y^2
end</code></pre><p>is equivalent to:</p><pre><code class="language-julia hljs">outer((x, y) -&gt; x^2 + y^2, 1, 2)</code></pre><p>For our purpose, <code>inner</code> is <code>subproblem_builder</code>, and <code>outer</code> is <a href="../../../apireference/#SDDP.PolicyGraph"><code>SDDP.PolicyGraph</code></a>.</p></div></div><h2 id="Training-a-policy"><a class="docs-heading-anchor" href="#Training-a-policy">Training a policy</a><a id="Training-a-policy-1"></a><a class="docs-heading-anchor-permalink" href="#Training-a-policy" title="Permalink"></a></h2><p>Now we have a model, which is a description of the policy graph, we need to train a policy. Models can be trained using the <a href="../../../apireference/#SDDP.train"><code>SDDP.train</code></a> function. It accepts a number of keyword arguments. <code>iteration_limit</code> terminates the training after the provided number of iterations.</p><pre><code class="language-julia hljs">SDDP.train(model; iteration_limit = 10)</code></pre><pre><code class="nohighlight hljs">------------------------------------------------------------------------------
                      SDDP.jl (c) Oscar Dowson, 2017-21

Problem
  Nodes           : 3
  State variables : 1
  Scenarios       : 2.70000e+01
  Existing cuts   : false
  Subproblem structure                      : (min, max)
    Variables                               : (7, 7)
    VariableRef in MOI.LessThan{Float64}    : (1, 2)
    VariableRef in MOI.GreaterThan{Float64} : (5, 5)
    AffExpr in MOI.EqualTo{Float64}         : (2, 2)
Options
  Solver          : serial mode
  Risk measure    : SDDP.Expectation()
  Sampling scheme : SDDP.InSampleMonteCarlo

Numerical stability report
  Non-zero Matrix range     [1e+00, 1e+00]
  Non-zero Objective range  [1e+00, 2e+02]
  Non-zero Bounds range     [2e+02, 2e+02]
  Non-zero RHS range        [2e+02, 2e+02]
No problems detected

 Iteration    Simulation       Bound         Time (s)    Proc. ID   # Solves
        1    7.500000e+03   5.000000e+03   1.044035e-03          1         12
        2    1.000000e+04   8.333333e+03   1.466990e-03          1         24
        3    8.750000e+03   8.333333e+03   1.829863e-03          1         36
        4    1.000000e+04   8.333333e+03   2.202034e-03          1         48
        5    2.500000e+03   8.333333e+03   2.660036e-03          1         60
        6    1.000000e+04   8.333333e+03   3.074884e-03          1         72
        7    1.250000e+04   8.333333e+03   3.514051e-03          1         84
        8    1.000000e+04   8.333333e+03   3.926992e-03          1         96
        9    1.500000e+04   8.333333e+03   4.356861e-03          1        108
       10    1.250000e+04   8.333333e+03   4.873037e-03          1        120

Terminating training
  Status         : iteration_limit
  Total time (s) : 4.873037e-03
  Total solves   : 120
  Best bound     :  8.333333e+03
  Simulation CI  :  9.875000e+03 ± 2.080497e+03
------------------------------------------------------------------------------
</code></pre><p>There&#39;s a lot going on in this printout! Let&#39;s break it down.</p><p>The first section (&quot;Problem&quot;) gives some problem statistics. In this example there are 3 nodes, 1 state variable, and 27 scenarios (<span>$3^3$</span>). We haven&#39;t solved this problem before so there are no existing cuts.</p><p>The &quot;Subproblem structure&quot; section also needs explaining. This looks at all of the nodes in the policy graph and reports the minimum and maximum number of variables and each constraint type in the corresponding subproblem. In this case each subproblem has 7 variables and various numbers of different constraint types. Note that the exact numbers may not correspond to the formulation as you wrote it, because SDDP.jl adds some extra variables for the cost-to-go function.</p><p>The &quot;Options&quot; section lists some options we are using to solve the problem. For more information on the numerical stability report, read the <a href="../06_warnings/#Numerical-stability-report">Numerical stability report</a> section.</p><p>Then comes the iteration log, which is the main part of the printout. It has the following columns:</p><ul><li><code>Iteration</code>: the SDDP iteration</li><li><code>Simulation</code>: the cost of the single forward pass simulation for that iteration. This value is stochastic and is not guaranteed to improve over time. However, it&#39;s useful to check that the units are reasonable, and that it is not deterministic if you intended for the problem to be stochastic, etc.</li><li><code>Bound</code>: this is a lower bound (upper if maximizing) for the value of the optimal policy. This should be monotonically improving (increasing if minimizing, decreasing if maximizing).</li><li><code>Time (s)</code>: the total number of seconds spent solving so far</li><li><code>Proc. ID</code>: the ID of the processor used to solve that iteration. This should be 1 unless you are using parallel computation.</li><li><code># Solves</code>: the total number of subproblem solves to date. This can be very large!</li></ul><p>The printout finishes with some summary statistics:</p><ul><li><code>Status</code>: why did the solver stop?</li><li><code>Total time (s)</code>, <code>Best bound</code>, and <code>Total solves</code> are the values from the last iteration of the solve.</li><li><code>Simulation CI</code>: a confidence interval that estimates the quality of the policy from the <code>Simulation</code> column.</li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>The <code>Simulation CI</code> result can be misleading if you run a small number of iterations, or if the initial simulations are very bad. On a more technical note, it is an <em>in-sample simulation</em>, which may not reflect the true performance of the policy. See <a href="#Obtaining-bounds">Obtaining bounds</a> for more details.</p></div></div><h2 id="Obtaining-the-decision-rule"><a class="docs-heading-anchor" href="#Obtaining-the-decision-rule">Obtaining the decision rule</a><a id="Obtaining-the-decision-rule-1"></a><a class="docs-heading-anchor-permalink" href="#Obtaining-the-decision-rule" title="Permalink"></a></h2><p>After training a policy, we can create a decision rule using <a href="../../../apireference/#SDDP.DecisionRule"><code>SDDP.DecisionRule</code></a>:</p><pre><code class="language-julia hljs">rule = SDDP.DecisionRule(model; node = 1)</code></pre><pre><code class="nohighlight hljs">A decision rule for node 1</code></pre><p>Then, to evaluate the decision rule, we use <a href="../../../apireference/#SDDP.evaluate"><code>SDDP.evaluate</code></a>:</p><pre><code class="language-julia hljs">solution = SDDP.evaluate(
    rule;
    incoming_state = Dict(:volume =&gt; 150.0),
    noise = 50.0,
    controls_to_record = [:hydro_generation, :thermal_generation],
)</code></pre><pre><code class="nohighlight hljs">(stage_objective = 7500.0, outgoing_state = Dict(:volume =&gt; 200.0), controls = Dict(:thermal_generation =&gt; 150.0, :hydro_generation =&gt; 0.0))</code></pre><h2 id="Simulating-the-policy"><a class="docs-heading-anchor" href="#Simulating-the-policy">Simulating the policy</a><a id="Simulating-the-policy-1"></a><a class="docs-heading-anchor-permalink" href="#Simulating-the-policy" title="Permalink"></a></h2><p>Once you have a trained policy, you can also simulate it using <a href="../../../apireference/#SDDP.simulate"><code>SDDP.simulate</code></a>. The return value from <code>simulate</code> is a vector with one element for each replication. Each element is itself a vector, with one element for each stage. Each element, corresponding to a particular stage in a particular replication, is a dictionary that records information from the simulation.</p><pre><code class="language-julia hljs">simulations = SDDP.simulate(
    # The trained model to simulate.
    model,
    # The number of replications.
    100,
    # A list of names to record the values of.
    [:volume, :thermal_generation, :hydro_generation, :hydro_spill],
)

replication = 1
stage = 2
simulations[replication][stage]</code></pre><pre><code class="nohighlight hljs">Dict{Symbol, Any} with 10 entries:
  :volume =&gt; State{Float64}(200.0, 100.0)
  :hydro_spill =&gt; 0.0
  :bellman_term =&gt; 2500.0
  :noise_term =&gt; 50.0
  :node_index =&gt; 2
  :stage_objective =&gt; 0.0
  :objective_state =&gt; nothing
  :thermal_generation =&gt; 0.0
  :hydro_generation =&gt; 150.0
  :belief =&gt; Dict(2=&gt;1.0)</code></pre><p>Ignore many of the entries for now;  they will be relevant later.</p><p>One element of iterest is <code>:volume</code>.</p><pre><code class="language-julia hljs">outgoing_volume = map(simulations[1]) do node
    return node[:volume].out
end</code></pre><pre><code class="nohighlight hljs">3-element Vector{Float64}:
 200.0
 100.0
   0.0</code></pre><p>Another is <code>:thermal_generation</code>.</p><pre><code class="language-julia hljs">thermal_generation = map(simulations[1]) do node
    return node[:thermal_generation]
end</code></pre><pre><code class="nohighlight hljs">3-element Vector{Float64}:
 150.0
   0.0
  50.0</code></pre><h2 id="Obtaining-bounds"><a class="docs-heading-anchor" href="#Obtaining-bounds">Obtaining bounds</a><a id="Obtaining-bounds-1"></a><a class="docs-heading-anchor-permalink" href="#Obtaining-bounds" title="Permalink"></a></h2><p>Because the optimal policy is stochastic, one common approach to quantify the quality of the policy is to construct a confidence interval for the expected cost by summing the stage objectives along each simulation.</p><pre><code class="language-julia hljs">objectives = map(simulations) do simulation
    return sum(stage[:stage_objective] for stage in simulation)
end

μ, ci = SDDP.confidence_interval(objectives)
println(&quot;Confidence interval: &quot;, μ, &quot; ± &quot;, ci)</code></pre><pre><code class="nohighlight hljs">Confidence interval: 8975.0 ± 977.7577884779508
</code></pre><p>This confidence interval is an estimate for an upper bound of the policy&#39;s quality. We can calculate the lower bound using <a href="../../../apireference/#SDDP.calculate_bound"><code>SDDP.calculate_bound</code></a>.</p><pre><code class="language-julia hljs">println(&quot;Lower bound: &quot;, SDDP.calculate_bound(model))</code></pre><pre><code class="nohighlight hljs">Lower bound: 8333.333333333332
</code></pre><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>The upper- and lower-bounds are reversed if maximizing, i.e., <a href="../../../apireference/#SDDP.calculate_bound"><code>SDDP.calculate_bound</code></a>. returns an upper bound.</p></div></div><h2 id="Custom-recorders"><a class="docs-heading-anchor" href="#Custom-recorders">Custom recorders</a><a id="Custom-recorders-1"></a><a class="docs-heading-anchor-permalink" href="#Custom-recorders" title="Permalink"></a></h2><p>In addition to simulating the primal values of variables, we can also pass custom recorder functions. Each of these functions takes one argument, the JuMP subproblem corresponding to each node. This function gets called after we have solved each node as we traverse the policy graph in the simulation.</p><p>For example, the dual of the demand constraint (which we named <code>demand_constraint</code>) corresponds to the price we should charge for electricity, since it represents the cost of each additional unit of demand. To calculate this, we can go:</p><pre><code class="language-julia hljs">simulations = SDDP.simulate(
    model,
    1,  ## Perform a single simulation
    custom_recorders = Dict{Symbol,Function}(
        :price =&gt; (sp::JuMP.Model) -&gt; JuMP.dual(sp[:demand_constraint]),
    ),
)

prices = map(simulations[1]) do node
    return node[:price]
end</code></pre><pre><code class="nohighlight hljs">3-element Vector{Float64}:
  50.0
 100.0
 150.0</code></pre><h2 id="Extracting-the-marginal-water-values"><a class="docs-heading-anchor" href="#Extracting-the-marginal-water-values">Extracting the marginal water values</a><a id="Extracting-the-marginal-water-values-1"></a><a class="docs-heading-anchor-permalink" href="#Extracting-the-marginal-water-values" title="Permalink"></a></h2><p>Finally, we can use <a href="../../../apireference/#SDDP.ValueFunction"><code>SDDP.ValueFunction</code></a> and <a href="../../../apireference/#SDDP.evaluate"><code>SDDP.evaluate</code></a> to obtain and evaluate the value function at different points in the state-space.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>By &quot;value function&quot; we mean <span>$\mathbb{E}_{j \in i^+, \varphi \in \Omega_j}[V_j(x^\prime, \varphi)]$</span>, note the function <span>$V_i(x, \omega)$</span>.</p></div></div><p>First, we construct a value function from the first subproblem:</p><pre><code class="language-julia hljs">V = SDDP.ValueFunction(model; node = 1)</code></pre><pre><code class="nohighlight hljs">A value function for node 1</code></pre><p>Then we can evaluate <code>V</code> at a point:</p><pre><code class="language-julia hljs">cost, price = SDDP.evaluate(V, Dict(&quot;volume&quot; =&gt; 10))</code></pre><pre><code class="nohighlight hljs">(19000.0, Dict(:volume =&gt; -99.99999999999999))</code></pre><p>This returns the cost-to-go (<code>cost</code>), and the gradient of the cost-to-go function with respect to each state variable. Note that since we are minimizing, the price has a negative sign: each additional unit of water leads to a decrease in the the expected long-run cost.</p><p>This concludes our first tutorial for <code>SDDP.jl</code>. In the next tutorial, <a href="../03_objective_uncertainty/#Uncertainty-in-the-objective-function">Uncertainty in the objective function</a>, we extend the uncertainty to the fuel cost.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../../">« Home</a><a class="docs-footer-nextpage" href="../03_objective_uncertainty/">Uncertainty in the objective function »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.10 on <span class="colophon-date" title="Saturday 11 December 2021 03:56">Saturday 11 December 2021</span>. Using Julia version 1.7.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
