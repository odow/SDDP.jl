<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Objective states · SDDP.jl</title><script data-outdated-warner src="../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../../"><img src="../../../assets/logo.png" alt="SDDP.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../../">SDDP.jl</a></span></div><form class="docs-search" action="../../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../../">Home</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox" checked/><label class="tocitem" for="menuitem-2"><span class="docs-label">Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><input class="collapse-toggle" id="menuitem-2-1" type="checkbox"/><label class="tocitem" for="menuitem-2-1"><span class="docs-label">Basic</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../basic/01_first_steps/">An introduction to SDDP.jl</a></li><li><a class="tocitem" href="../../basic/03_objective_uncertainty/">Uncertainty in the objective function</a></li><li><a class="tocitem" href="../../basic/04_markov_uncertainty/">Markovian policy graphs</a></li><li><a class="tocitem" href="../../basic/05_plotting/">Plotting tools</a></li><li><a class="tocitem" href="../../basic/06_warnings/">Words of warning</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox" checked/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Advanced</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Objective states</a><ul class="internal"><li><a class="tocitem" href="#One-dimensional-objective-states"><span>One-dimensional objective states</span></a></li><li><a class="tocitem" href="#Multi-dimensional-objective-states"><span>Multi-dimensional objective states</span></a></li><li><a class="tocitem" href="#objective_state_warnings"><span>Warnings</span></a></li></ul></li><li><a class="tocitem" href="../12_belief_states/">Belief states</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Theory</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../theory/21_theory_intro/">Introductory theory</a></li><li><a class="tocitem" href="../../theory/22_risk/">Risk aversion</a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">How-to guides</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../../guides/acess_previous_variables/">Access variables from a previous stage</a></li><li><a class="tocitem" href="../../../guides/add_a_multidimensional_state_variable/">Add a multi-dimensional state variable</a></li><li><a class="tocitem" href="../../../guides/add_a_risk_measure/">Add a risk measure</a></li><li><a class="tocitem" href="../../../guides/add_integrality/">Integrality</a></li><li><a class="tocitem" href="../../../guides/add_multidimensional_noise_Terms/">Add multi-dimensional noise terms</a></li><li><a class="tocitem" href="../../../guides/add_noise_in_the_constraint_matrix/">Add noise in the constraint matrix</a></li><li><a class="tocitem" href="../../../guides/choose_a_stopping_rule/">Choose a stopping rule</a></li><li><a class="tocitem" href="../../../guides/create_a_general_policy_graph/">Create a general policy graph</a></li><li><a class="tocitem" href="../../../guides/debug_a_model/">Debug a model</a></li><li><a class="tocitem" href="../../../guides/improve_computational_performance/">Improve computational performance</a></li><li><a class="tocitem" href="../../../guides/simulate_using_a_different_sampling_scheme/">Simulate using a different sampling scheme</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../../examples/FAST_hydro_thermal/">FAST: the hydro-thermal problem</a></li><li><a class="tocitem" href="../../../examples/FAST_production_management/">FAST: the production management problem</a></li><li><a class="tocitem" href="../../../examples/FAST_quickstart/">FAST: the quickstart problem</a></li><li><a class="tocitem" href="../../../examples/Hydro_thermal/">Hydro-thermal scheduling</a></li><li><a class="tocitem" href="../../../examples/StochDynamicProgramming.jl_multistock/">StochDynamicProgramming: the multistock problem</a></li><li><a class="tocitem" href="../../../examples/StochDynamicProgramming.jl_stock/">StochDynamicProgramming: the stock problem</a></li><li><a class="tocitem" href="../../../examples/StructDualDynProg.jl_prob5.2_2stages/">StructDualDynProg: Problem 5.2, 2 stages</a></li><li><a class="tocitem" href="../../../examples/StructDualDynProg.jl_prob5.2_3stages/">StructDualDynProg: Problem 5.2, 3 stages</a></li><li><a class="tocitem" href="../../../examples/agriculture_mccardle_farm/">The farm planning problem</a></li><li><a class="tocitem" href="../../../examples/air_conditioning/">Air conditioning</a></li><li><a class="tocitem" href="../../../examples/all_blacks/">Deterministic All Blacks</a></li><li><a class="tocitem" href="../../../examples/asset_management_simple/">Asset management</a></li><li><a class="tocitem" href="../../../examples/asset_management_stagewise/">Asset management with modifications</a></li><li><a class="tocitem" href="../../../examples/belief/">Partially observable inventory management</a></li><li><a class="tocitem" href="../../../examples/biobjective_hydro/">Biobjective hydro-thermal</a></li><li><a class="tocitem" href="../../../examples/booking_management/">Booking management</a></li><li><a class="tocitem" href="../../../examples/generation_expansion/">Generation expansion</a></li><li><a class="tocitem" href="../../../examples/hydro_valley/">Hydro valleys</a></li><li><a class="tocitem" href="../../../examples/infinite_horizon_hydro_thermal/">Infinite horizon hydro-thermal</a></li><li><a class="tocitem" href="../../../examples/infinite_horizon_trivial/">Infinite horizon trivial</a></li><li><a class="tocitem" href="../../../examples/no_strong_duality/">No strong duality</a></li><li><a class="tocitem" href="../../../examples/objective_state_newsvendor/">Newsvendor</a></li><li><a class="tocitem" href="../../../examples/sldp_example_one/">SLDP: example 1</a></li><li><a class="tocitem" href="../../../examples/sldp_example_two/">SLDP: example 2</a></li><li><a class="tocitem" href="../../../examples/stochastic_all_blacks/">Stochastic All Blacks</a></li><li><a class="tocitem" href="../../../examples/the_farmers_problem/">The farmer&#39;s problem</a></li><li><a class="tocitem" href="../../../examples/vehicle_location/">Vehicle location</a></li></ul></li><li><a class="tocitem" href="../../../apireference/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li><a class="is-disabled">Advanced</a></li><li class="is-active"><a href>Objective states</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Objective states</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/odow/SDDP.jl/blob/master/docs/src/tutorial/advanced/11_objective_states.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Objective-states"><a class="docs-heading-anchor" href="#Objective-states">Objective states</a><a id="Objective-states-1"></a><a class="docs-heading-anchor-permalink" href="#Objective-states" title="Permalink"></a></h1><p>There are many applications in which we want to model a price process that follows some auto-regressive process. Common examples include stock prices on financial exchanges and spot-prices in energy markets.</p><p>However, it is well known that these cannot be incorporated in to SDDP because they result in cost-to-go functions that are convex with respect to some state variables (e.g., the reservoir levels) and concave with respect to other state variables (e.g., the spot price in the current stage).</p><p>To overcome this problem, the approach in the literature has been to discretize the price process in order to model it using a Markovian policy graph like those discussed in <a href="../../basic/04_markov_uncertainty/#Markovian-policy-graphs">Markovian policy graphs</a>.</p><p>However, recent work offers a way to include stagewise-dependent objective uncertainty into the objective function of SDDP subproblems. Readers are directed to the following works for an introduction:</p><ul><li><p>Downward, A., Dowson, O., and Baucke, R. (2017). Stochastic dual dynamic programming with stagewise dependent objective uncertainty. Optimization Online. <a href="http://www.optimization-online.org/DB_HTML/2018/02/6454.html">link</a></p></li><li><p>Dowson, O. PhD Thesis. University of Auckland, 2018. <a href="https://researchspace.auckland.ac.nz/handle/2292/37700">link</a></p></li></ul><p>The method discussed in the above works introduces the concept of an <em>objective state</em> into SDDP. Unlike normal state variables in SDDP (e.g., the volume of water in the reservoir), the cost-to-go function is <em>concave</em> with respect to the objective states. Thus, the method builds an outer approximation of the cost-to-go function in the normal state-space, and an inner approximation of the cost-to-go function in the objective state-space.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Support for objective states in <code>SDDP.jl</code> is experimental. Models are considerably more computational intensive, the interface is less user-friendly, and there are <a href="#objective_state_warnings">subtle gotchas to be aware of</a>. Only use this if you have read and understood the theory behind the method.</p></div></div><h2 id="One-dimensional-objective-states"><a class="docs-heading-anchor" href="#One-dimensional-objective-states">One-dimensional objective states</a><a id="One-dimensional-objective-states-1"></a><a class="docs-heading-anchor-permalink" href="#One-dimensional-objective-states" title="Permalink"></a></h2><p>Let&#39;s assume that the fuel cost is not fixed, but instead evolves according to a multiplicative auto-regressive process: <code>fuel_cost[t] = ω * fuel_cost[t-1]</code>, where <code>ω</code> is drawn from the sample space <code>[0.75, 0.9, 1.1, 1.25]</code> with equal probability.</p><p>An objective state can be added to a subproblem using the <a href="../../../apireference/#SDDP.add_objective_state"><code>SDDP.add_objective_state</code></a> function. This can only be called once per subproblem. If you want to add a multi-dimensional objective state, read <a href="#Multi-dimensional-objective-states">Multi-dimensional objective states</a>. <a href="../../../apireference/#SDDP.add_objective_state"><code>SDDP.add_objective_state</code></a> takes a number of keyword arguments. The two required ones are</p><ul><li><p><code>initial_value</code>: the value of the objective state at the root node of the policy graph (i.e., identical to the <code>initial_value</code> when defining normal state variables.</p></li><li><p><code>lipschitz</code>: the Lipschitz constant of the cost-to-go function with respect to the objective state. In other words, this value is the maximum change in the cost-to-go function <em>at any point in the state space</em>, given a one-unit change in the objective state.</p></li></ul><p>There are also two optional keyword arguments: <code>lower_bound</code> and <code>upper_bound</code>, which give SDDP.jl hints (importantly, not constraints) about the domain of the objective state. Setting these bounds appropriately can improve the speed of convergence.</p><p>Finally, <a href="../../../apireference/#SDDP.add_objective_state"><code>SDDP.add_objective_state</code></a> requires an update function. This function takes two arguments. The first is the incoming value of the objective state, and the second is the realization of the stagewise-independent noise term (set using <a href="../../../apireference/#SDDP.parameterize"><code>SDDP.parameterize</code></a>). The function should return the value of the objective state to be used in the current subproblem.</p><p>This connection with the stagewise-independent noise term means that <a href="../../../apireference/#SDDP.parameterize"><code>SDDP.parameterize</code></a> <em>must</em> be called in a subproblem that defines an objective state. Inside <a href="../../../apireference/#SDDP.parameterize"><code>SDDP.parameterize</code></a>, the value of the objective state to be used in the current subproblem (i.e., after the update function), can be queried using <a href="../../../apireference/#SDDP.objective_state"><code>SDDP.objective_state</code></a>.</p><p>Here is the full model with the objective state.</p><pre><code class="language-julia hljs">using SDDP, GLPK

model = SDDP.LinearPolicyGraph(
    stages = 3,
    sense = :Min,
    lower_bound = 0.0,
    optimizer = GLPK.Optimizer,
) do subproblem, t
    @variable(subproblem, 0 &lt;= volume &lt;= 200, SDDP.State, initial_value = 200)
    @variables(subproblem, begin
        thermal_generation &gt;= 0
        hydro_generation &gt;= 0
        hydro_spill &gt;= 0
        inflow
    end)
    @constraints(
        subproblem,
        begin
            volume.out == volume.in + inflow - hydro_generation - hydro_spill
            demand_constraint, thermal_generation + hydro_generation == 150.0
        end
    )

    # Add an objective state. ω will be the same value that is called in
    # `SDDP.parameterize`.

    SDDP.add_objective_state(
        subproblem,
        initial_value = 50.0,
        lipschitz = 10_000.0,
        lower_bound = 50.0,
        upper_bound = 150.0,
    ) do fuel_cost, ω
        return ω.fuel * fuel_cost
    end

    # Create the cartesian product of a multi-dimensional random variable.

    Ω = [
        (fuel = f, inflow = w) for f in [0.75, 0.9, 1.1, 1.25] for
        w in [0.0, 50.0, 100.0]
    ]

    SDDP.parameterize(subproblem, Ω) do ω
        # Query the current fuel cost.
        fuel_cost = SDDP.objective_state(subproblem)
        @stageobjective(subproblem, fuel_cost * thermal_generation)
        return JuMP.fix(inflow, ω.inflow)
    end
end</code></pre><pre><code class="nohighlight hljs">A policy graph with 3 nodes.
 Node indices: 1, 2, 3
</code></pre><p>After creating our model, we can train and simulate as usual.</p><pre><code class="language-julia hljs">SDDP.train(model, iteration_limit = 10, run_numerical_stability_report = false)

simulations = SDDP.simulate(model, 1)

print(&quot;Finished training and simulating.&quot;)</code></pre><pre><code class="nohighlight hljs">------------------------------------------------------------------------------
                      SDDP.jl (c) Oscar Dowson, 2017-21

Problem
  Nodes           : 3
  State variables : 1
  Scenarios       : 1.72800e+03
  Existing cuts   : false
  Subproblem structure                      : (min, max)
    Variables                               : (8, 8)
    VariableRef in MOI.LessThan{Float64}    : (2, 3)
    AffExpr in MOI.GreaterThan{Float64}     : (2, 2)
    AffExpr in MOI.EqualTo{Float64}         : (2, 4)
    VariableRef in MOI.GreaterThan{Float64} : (6, 6)
Options
  Solver          : serial mode
  Risk measure    : SDDP.Expectation()
  Sampling scheme : SDDP.InSampleMonteCarlo

 Iteration    Simulation       Bound         Time (s)    Proc. ID   # Solves
        1    5.906250e+03   4.083333e+03   2.421260e-01          1         39
        2    1.051875e+04   4.294792e+03   2.431660e-01          1         78
        3    7.031250e+03   4.294792e+03   2.441571e-01          1        117
        4    3.598915e+03   4.357986e+03   2.452979e-01          1        156
        5    4.882812e+03   5.000000e+03   2.463419e-01          1        195
        6    3.847500e+03   5.025374e+03   2.473919e-01          1        234
        7    5.224500e+03   5.025374e+03   2.485220e-01          1        273
        8    3.796875e+03   5.025374e+03   2.497339e-01          1        312
        9    1.546875e+03   5.041707e+03   2.509670e-01          1        351
       10    9.680000e+03   5.041707e+03   2.521241e-01          1        390

Terminating training
  Status         : iteration_limit
  Total time (s) : 2.521241e-01
  Total solves   : 390
  Best bound     :  5.041707e+03
  Simulation CI  :  5.603373e+03 ± 1.733399e+03
------------------------------------------------------------------------------
Finished training and simulating.</code></pre><p>To demonstrate how the objective states are updated, consider the sequence of noise observations:</p><pre><code class="language-julia hljs">[stage[:noise_term] for stage in simulations[1]]</code></pre><pre><code class="nohighlight hljs">3-element Vector{NamedTuple{(:fuel, :inflow), Tuple{Float64, Float64}}}:
 (fuel = 0.75, inflow = 0.0)
 (fuel = 1.1, inflow = 50.0)
 (fuel = 0.75, inflow = 50.0)</code></pre><p>This, the fuel cost in the first stage should be <code>0.75 * 50 = 37.5</code>. The fuel cost in the second stage should be <code>0.9 * 37.5 = 33.75</code>. The fuel cost in the third stage should be <code>1.25 * 33.75 = 42.1875</code>.</p><p>To confirm this, the values of the objective state in a simulation can be queried using the <code>:objective_state</code> key.</p><pre><code class="language-julia hljs">[stage[:objective_state] for stage in simulations[1]]</code></pre><pre><code class="nohighlight hljs">3-element Vector{Float64}:
 37.5
 41.25
 30.9375</code></pre><h2 id="Multi-dimensional-objective-states"><a class="docs-heading-anchor" href="#Multi-dimensional-objective-states">Multi-dimensional objective states</a><a id="Multi-dimensional-objective-states-1"></a><a class="docs-heading-anchor-permalink" href="#Multi-dimensional-objective-states" title="Permalink"></a></h2><p>You can construct multi-dimensional price processes using <code>NTuple</code>s. Just replace every scalar value associated with the objective state by a tuple. For example, <code>initial_value = 1.0</code> becomes <code>initial_value = (1.0, 2.0)</code>.</p><p>Here is an example:</p><pre><code class="language-julia hljs">model = SDDP.LinearPolicyGraph(
    stages = 3,
    sense = :Min,
    lower_bound = 0.0,
    optimizer = GLPK.Optimizer,
) do subproblem, t
    @variable(subproblem, 0 &lt;= volume &lt;= 200, SDDP.State, initial_value = 200)
    @variables(subproblem, begin
        thermal_generation &gt;= 0
        hydro_generation &gt;= 0
        hydro_spill &gt;= 0
        inflow
    end)
    @constraints(
        subproblem,
        begin
            volume.out == volume.in + inflow - hydro_generation - hydro_spill
            demand_constraint, thermal_generation + hydro_generation == 150.0
        end
    )

    SDDP.add_objective_state(
        subproblem,
        initial_value = (50.0, 50.0),
        lipschitz = (10_000.0, 10_000.0),
        lower_bound = (50.0, 50.0),
        upper_bound = (150.0, 150.0),
    ) do fuel_cost, ω
        fuel_cost′ = fuel_cost[1] + 0.5 * (fuel_cost[1] - fuel_cost[2]) + ω.fuel
        return (fuel_cost′, fuel_cost[1])
    end

    Ω = [
        (fuel = f, inflow = w) for f in [-10.0, -5.0, 5.0, 10.0] for
        w in [0.0, 50.0, 100.0]
    ]

    SDDP.parameterize(subproblem, Ω) do ω
        (fuel_cost, fuel_cost_old) = SDDP.objective_state(subproblem)
        @stageobjective(subproblem, fuel_cost * thermal_generation)
        return JuMP.fix(inflow, ω.inflow)
    end
end

SDDP.train(model, iteration_limit = 10, run_numerical_stability_report = false)

simulations = SDDP.simulate(model, 1)

print(&quot;Finished training and simulating.&quot;)</code></pre><pre><code class="nohighlight hljs">------------------------------------------------------------------------------
                      SDDP.jl (c) Oscar Dowson, 2017-21

Problem
  Nodes           : 3
  State variables : 1
  Scenarios       : 1.72800e+03
  Existing cuts   : false
  Subproblem structure                      : (min, max)
    Variables                               : (9, 9)
    VariableRef in MOI.LessThan{Float64}    : (3, 4)
    AffExpr in MOI.GreaterThan{Float64}     : (4, 4)
    AffExpr in MOI.EqualTo{Float64}         : (2, 10)
    VariableRef in MOI.GreaterThan{Float64} : (7, 7)
Options
  Solver          : serial mode
  Risk measure    : SDDP.Expectation()
  Sampling scheme : SDDP.InSampleMonteCarlo

 Iteration    Simulation       Bound         Time (s)    Proc. ID   # Solves
        1    3.750000e+03   2.888258e+03   6.752119e-01          1         39
        2    8.250000e+03   4.138883e+03   6.827080e-01          1         78
        3    1.561186e+04   4.465912e+03   6.838331e-01          1        117
        4    4.202520e+03   4.690522e+03   6.848791e-01          1        156
        5    1.394362e+04   4.977124e+03   6.859989e-01          1        195
        6    4.377350e+03   4.988150e+03   6.871421e-01          1        234
        7    7.750000e+03   4.988150e+03   6.882470e-01          1        273
        8    9.001638e+03   4.988150e+03   6.892519e-01          1        312
        9    3.000000e+03   4.999490e+03   6.903830e-01          1        351
       10    9.562500e+03   4.999490e+03   6.916010e-01          1        390

Terminating training
  Status         : iteration_limit
  Total time (s) : 6.916010e-01
  Total solves   : 390
  Best bound     :  4.999490e+03
  Simulation CI  :  7.944949e+03 ± 2.673190e+03
------------------------------------------------------------------------------
Finished training and simulating.</code></pre><p>This time, since our objective state is two-dimensional, the objective states are tuples with two elements:</p><pre><code class="language-julia hljs">[stage[:objective_state] for stage in simulations[1]]</code></pre><pre><code class="nohighlight hljs">3-element Vector{Tuple{Float64, Float64}}:
 (40.0, 50.0)
 (45.0, 40.0)
 (57.5, 45.0)</code></pre><h2 id="objective_state_warnings"><a class="docs-heading-anchor" href="#objective_state_warnings">Warnings</a><a id="objective_state_warnings-1"></a><a class="docs-heading-anchor-permalink" href="#objective_state_warnings" title="Permalink"></a></h2><p>There are number of things to be aware of when using objective states.</p><ul><li><p>The key assumption is that price is independent of the states and actions in the model.</p><p>That means that the price cannot appear in any <code>@constraint</code>s. Nor can you use any <code>@variable</code>s in the update function.</p></li><li><p>Choosing an appropriate Lipschitz constant is difficult.</p><p>The points discussed in <a href="../../basic/06_warnings/#Choosing-an-initial-bound">Choosing an initial bound</a> are relevant. The Lipschitz constant should not be chosen as large as possible (since this will help with convergence and the numerical issues discussed above), but if chosen to small, it may cut of the feasible region and lead to a sub-optimal solution.</p></li><li><p>You need to ensure that the cost-to-go function is concave with respect to the objective state <em>before</em> the update.</p><p>If the update function is linear, this is always the case. In some situations, the update function can be nonlinear (e.g., multiplicative as we have above). In general, placing constraints on the price (e.g., <code>clamp(price, 0, 1)</code>) will destroy concavity. <a href="https://en.wikipedia.org/wiki/Caveat_emptor">Caveat emptor</a>. It&#39;s up to you if this is a problem. If it isn&#39;t you&#39;ll get a good heuristic with no guarantee of global optimality.</p></li></ul><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../basic/06_warnings/">« Words of warning</a><a class="docs-footer-nextpage" href="../12_belief_states/">Belief states »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.10 on <span class="colophon-date" title="Saturday 11 December 2021 03:56">Saturday 11 December 2021</span>. Using Julia version 1.7.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
