{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Auto-regressive stochastic processes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "SDDP.jl assumes that the random variable in each node is independent of the\n",
    "random variables in all other nodes. However, a common request is to model\n",
    "the random variables by some auto-regressive process."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are two ways to do this:\n",
    " 1. model the random variable as a Markov chain\n",
    " 2. use the \"state-space expansion\" trick"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Info**\n",
    ">\n",
    "> This tutorial is in the context of a hydro-thermal scheduling example, but\n",
    "> it should be apparent how the ideas transfer to other applications."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using SDDP\n",
    "import HiGHS"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The state-space expansion trick"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In An introduction to SDDP.jl, we assumed that the inflows were\n",
    "stagewise-independent. However, in many cases this is not correct, and inflow\n",
    "models are more accurately described by an auto-regressive process such as:\n",
    "$$\n",
    "inflow_{t} = inflow_{t-1} + \\varepsilon\n",
    "$$\n",
    "Here $\\varepsilon$ is a random variable, and the inflow in stage $t$ is\n",
    "the inflow in stage $t-1$ plus $\\varepsilon$ (which might be negative)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For simplicity, we omit any coefficients and other terms, but this could\n",
    "easily be extended to a model like\n",
    "$$\n",
    "inflow_{t} = a \\times inflow_{t-1} + b + \\varepsilon\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In practice, you can estimate a distribution for $\\varepsilon$ by fitting\n",
    "the chosen statistical model to historical data, and then using the empirical\n",
    "residuals."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To implement the auto-regressive model in SDDP.jl, we introduce `inflow` as a\n",
    "state variable."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Tip**\n",
    ">\n",
    "> Our rule of thumb for \"when is something a state variable?\" is: if you\n",
    "> need the value of a variable from a previous stage to compute something in\n",
    "> stage $t$, then that variable is a state variable."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.LinearPolicyGraph(;\n",
    "    stages = 3,\n",
    "    sense = :Min,\n",
    "    lower_bound = 0.0,\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ") do sp, t\n",
    "    @variable(sp, 0 <= x <= 200, SDDP.State, initial_value = 200)\n",
    "    @variable(sp, g_t >= 0)\n",
    "    @variable(sp, g_h >= 0)\n",
    "    @variable(sp, s >= 0)\n",
    "    @constraint(sp, g_h + g_t == 150)\n",
    "    c = [50, 100, 150]\n",
    "    @stageobjective(sp, c[t] * g_t)\n",
    "    # =========================================================================\n",
    "    # New stuff below Here\n",
    "    # Add inflow as a state\n",
    "    @variable(sp, inflow, SDDP.State, initial_value = 50.0)\n",
    "    # Add the random variable as a control variable\n",
    "    @variable(sp, ε)\n",
    "    # The equation describing our statistical model\n",
    "    @constraint(sp, inflow.out == inflow.in + ε)\n",
    "    # The new water balance constraint using the state variable\n",
    "    @constraint(sp, x.out == x.in - g_h - s + inflow.out)\n",
    "    # Assume we have some empirical residuals:\n",
    "    Ω = [-10.0, 0.1, 9.6]\n",
    "    SDDP.parameterize(sp, Ω) do ω\n",
    "        return JuMP.fix(ε, ω)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### When can this trick be used?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The state-space expansion trick should be used when:\n",
    "\n",
    " * The random variable appears additively in the objective or in the\n",
    "   constraints. Something like `inflow * decision_variable` will _not_ work.\n",
    " * The statistical model is linear, or can be written using the JuMP\n",
    "   `@constraint` macro.\n",
    " * The dimension of the random variable is small (see\n",
    "   Vector auto-regressive models for the multi-variate case)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Markov chain approach"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the Markov chain approach, we model the stochastic process for inflow by a\n",
    "discrete Markov chain. Markov chains are nodes with transition probabilities\n",
    "between the nodes. SDDP.jl has good support for solving problems in which the\n",
    "uncertainty is formulated as a Markov chain."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first step of the Markov chain approach is to write a function which\n",
    "simulates the stochastic process. Here is a simulator for our inflow model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function simulator()\n",
    "    inflow = zeros(3)\n",
    "    current = 50.0\n",
    "    Ω = [-10.0, 0.1, 9.6]\n",
    "    for t in 1:3\n",
    "        current += rand(Ω)\n",
    "        inflow[t] = current\n",
    "    end\n",
    "    return inflow\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "When called with no arguments, it produces a vector of inflows:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulator()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Warning**\n",
    ">\n",
    "> The `simulator` must return a `Vector{Float64}`, so it is limited to a\n",
    "> uni-variate random variable. It is possible to do something similar for\n",
    "> multi-variate random variable, but you'll have to manually construct the\n",
    "> Markov transition matrix, and solution times scale poorly, even in the\n",
    "> two-dimensional case."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next step is to call `SDDP.MarkovianGraph` with our simulator.\n",
    "This function will attempt to fit a Markov chain to the stochastic process\n",
    "produced by your `simulator`. There are two key arguments:\n",
    " * `budget` is the total number of nodes we want in the Markov chain\n",
    " * `scenarios` is a limit on the number of times we can call `simulator`"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "graph = SDDP.MarkovianGraph(simulator; budget = 8, scenarios = 30)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we can see we have created a MarkovianGraph with nodes like `(2, 59.7)`.\n",
    "The first element of each node is the stage, and the second element is the\n",
    "inflow."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a `SDDP.PolicyGraph` using `graph` as follows:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.PolicyGraph(\n",
    "    graph;  # <--- New stuff\n",
    "    sense = :Min,\n",
    "    lower_bound = 0.0,\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ") do sp, node\n",
    "    t, inflow = node  # <--- New stuff\n",
    "    @variable(sp, 0 <= x <= 200, SDDP.State, initial_value = 200)\n",
    "    @variable(sp, g_t >= 0)\n",
    "    @variable(sp, g_h >= 0)\n",
    "    @variable(sp, s >= 0)\n",
    "    @constraint(sp, g_h + g_t == 150)\n",
    "    c = [50, 100, 150]\n",
    "    @stageobjective(sp, c[t] * g_t)\n",
    "    # The new water balance constraint using the node:\n",
    "    @constraint(sp, x.out == x.in - g_h - s + inflow)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### When can this trick be used?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Markov chain approach should be used when:\n",
    "\n",
    " * The random variable is uni-variate\n",
    " * The random variable appears in the objective function or as a variable\n",
    "   coefficient in the constraint matrix\n",
    " * It's non-trivial to write the stochastic process as a series of constraints\n",
    "   (for example, it uses nonlinear terms)\n",
    " * The number of nodes is modest (for example, a budget of hundreds, up to\n",
    "   perhaps 1000)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vector auto-regressive models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The state-space expansion section assumed that\n",
    "the random variable was uni-variate. However, the approach naturally extends\n",
    "to vector auto-regressive models. For example, if `inflow` is a 2-dimensional\n",
    "vector, then we can model a vector auto-regressive model to it as follows:\n",
    "$$\n",
    "inflow_{t} = A \\times inflow_{t-1} + b + \\varepsilon\n",
    "$$\n",
    "Here `A` is a 2-by-2 matrix, and `b` and $\\varepsilon$ are 2-by-1 vectors."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.LinearPolicyGraph(;\n",
    "    stages = 3,\n",
    "    sense = :Min,\n",
    "    lower_bound = 0.0,\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ") do sp, t\n",
    "    @variable(sp, 0 <= x <= 200, SDDP.State, initial_value = 200)\n",
    "    @variable(sp, g_t >= 0)\n",
    "    @variable(sp, g_h >= 0)\n",
    "    @variable(sp, s >= 0)\n",
    "    @constraint(sp, g_h + g_t == 150)\n",
    "    c = [50, 100, 150]\n",
    "    @stageobjective(sp, c[t] * g_t)\n",
    "    # =========================================================================\n",
    "    # New stuff below Here\n",
    "    # Add inflow as a state\n",
    "    @variable(sp, inflow[1:2], SDDP.State, initial_value = 50.0)\n",
    "    # Add the random variable as a control variable\n",
    "    @variable(sp, ε[1:2])\n",
    "    # The equation describing our statistical model\n",
    "    A = [0.8 0.2; 0.2 0.8]\n",
    "    @constraint(\n",
    "        sp,\n",
    "        [i = 1:2],\n",
    "        inflow[i].out == sum(A[i, j] * inflow[j].in for j in 1:2) + ε[i],\n",
    "    )\n",
    "    # The new water balance constraint using the state variable\n",
    "    @constraint(sp, x.out == x.in - g_h - s + inflow[1].out + inflow[2].out)\n",
    "    # Assume we have some empirical residuals:\n",
    "    Ω₁ = [-10.0, 0.1, 9.6]\n",
    "    Ω₂ = [-10.0, 0.1, 9.6]\n",
    "    Ω = [(ω₁, ω₂) for ω₁ in Ω₁ for ω₂ in Ω₂]\n",
    "    SDDP.parameterize(sp, Ω) do ω\n",
    "        JuMP.fix(ε[1], ω[1])\n",
    "        JuMP.fix(ε[2], ω[2])\n",
    "        return\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  },
  "kernelspec": {
   "name": "julia-1.11",
   "display_name": "Julia 1.11.4",
   "language": "julia"
  }
 },
 "nbformat": 4
}
