<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Alternative forward models · SDDP.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="SDDP.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">SDDP.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox" checked/><label class="tocitem" for="menuitem-2"><span class="docs-label">Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../first_steps/">An introduction to SDDP.jl</a></li><li><a class="tocitem" href="../objective_uncertainty/">Uncertainty in the objective function</a></li><li><a class="tocitem" href="../markov_uncertainty/">Markovian policy graphs</a></li><li><a class="tocitem" href="../plotting/">Plotting tools</a></li><li><a class="tocitem" href="../warnings/">Words of warning</a></li><li><a class="tocitem" href="../arma/">Auto-regressive stochastic processes</a></li><li><a class="tocitem" href="../decision_hazard/">Here-and-now and hazard-decision</a></li><li><a class="tocitem" href="../objective_states/">Objective states</a></li><li class="is-active"><a class="tocitem" href>Alternative forward models</a><ul class="internal"><li><a class="tocitem" href="#Formulation"><span>Formulation</span></a></li><li><a class="tocitem" href="#Training-a-convex-model"><span>Training a convex model</span></a></li><li><a class="tocitem" href="#Training-a-non-convex-model"><span>Training a non-convex model</span></a></li><li><a class="tocitem" href="#Combining-convex-and-non-convex-models"><span>Combining convex and non-convex models</span></a></li></ul></li><li><a class="tocitem" href="../mdps/">Example: Markov Decision Processes</a></li><li><a class="tocitem" href="../example_newsvendor/">Example: Two-stage Newsvendor</a></li><li><a class="tocitem" href="../example_reservoir/">Example: deterministic to stochastic</a></li><li><a class="tocitem" href="../example_milk_producer/">Example: the milk producer</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">How-to guides</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../guides/access_previous_variables/">Access variables from a previous stage</a></li><li><a class="tocitem" href="../../guides/add_a_multidimensional_state_variable/">Add a multi-dimensional state variable</a></li><li><a class="tocitem" href="../../guides/add_a_risk_measure/">Add a risk measure</a></li><li><a class="tocitem" href="../../guides/add_integrality/">Integrality</a></li><li><a class="tocitem" href="../../guides/add_multidimensional_noise/">Add multi-dimensional noise terms</a></li><li><a class="tocitem" href="../../guides/add_noise_in_the_constraint_matrix/">Add noise in the constraint matrix</a></li><li><a class="tocitem" href="../../guides/choose_a_stopping_rule/">Choose a stopping rule</a></li><li><a class="tocitem" href="../../guides/create_a_general_policy_graph/">Create a general policy graph</a></li><li><a class="tocitem" href="../../guides/debug_a_model/">Debug a model</a></li><li><a class="tocitem" href="../../guides/improve_computational_performance/">Improve computational performance</a></li><li><a class="tocitem" href="../../guides/simulate_using_a_different_sampling_scheme/">Simulate using a different sampling scheme</a></li><li><a class="tocitem" href="../../guides/create_a_belief_state/">Create a belief state</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">Explanation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../explanation/theory_intro/">Introductory theory</a></li><li><a class="tocitem" href="../../explanation/risk/">Risk aversion</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/FAST_hydro_thermal/">FAST: the hydro-thermal problem</a></li><li><a class="tocitem" href="../../examples/FAST_production_management/">FAST: the production management problem</a></li><li><a class="tocitem" href="../../examples/FAST_quickstart/">FAST: the quickstart problem</a></li><li><a class="tocitem" href="../../examples/Hydro_thermal/">Hydro-thermal scheduling</a></li><li><a class="tocitem" href="../../examples/StochDynamicProgramming.jl_multistock/">StochDynamicProgramming: the multistock problem</a></li><li><a class="tocitem" href="../../examples/StochDynamicProgramming.jl_stock/">StochDynamicProgramming: the stock problem</a></li><li><a class="tocitem" href="../../examples/StructDualDynProg.jl_prob5.2_2stages/">StructDualDynProg: Problem 5.2, 2 stages</a></li><li><a class="tocitem" href="../../examples/StructDualDynProg.jl_prob5.2_3stages/">StructDualDynProg: Problem 5.2, 3 stages</a></li><li><a class="tocitem" href="../../examples/agriculture_mccardle_farm/">The farm planning problem</a></li><li><a class="tocitem" href="../../examples/air_conditioning/">Air conditioning</a></li><li><a class="tocitem" href="../../examples/air_conditioning_forward/">Training with a different forward model</a></li><li><a class="tocitem" href="../../examples/all_blacks/">Deterministic All Blacks</a></li><li><a class="tocitem" href="../../examples/asset_management_simple/">Asset management</a></li><li><a class="tocitem" href="../../examples/asset_management_stagewise/">Asset management with modifications</a></li><li><a class="tocitem" href="../../examples/belief/">Partially observable inventory management</a></li><li><a class="tocitem" href="../../examples/biobjective_hydro/">Biobjective hydro-thermal</a></li><li><a class="tocitem" href="../../examples/booking_management/">Booking management</a></li><li><a class="tocitem" href="../../examples/generation_expansion/">Generation expansion</a></li><li><a class="tocitem" href="../../examples/hydro_valley/">Hydro valleys</a></li><li><a class="tocitem" href="../../examples/infinite_horizon_hydro_thermal/">Infinite horizon hydro-thermal</a></li><li><a class="tocitem" href="../../examples/infinite_horizon_trivial/">Infinite horizon trivial</a></li><li><a class="tocitem" href="../../examples/no_strong_duality/">No strong duality</a></li><li><a class="tocitem" href="../../examples/objective_state_newsvendor/">Newsvendor</a></li><li><a class="tocitem" href="../../examples/sldp_example_one/">SLDP: example 1</a></li><li><a class="tocitem" href="../../examples/sldp_example_two/">SLDP: example 2</a></li><li><a class="tocitem" href="../../examples/stochastic_all_blacks/">Stochastic All Blacks</a></li><li><a class="tocitem" href="../../examples/the_farmers_problem/">The farmer&#39;s problem</a></li><li><a class="tocitem" href="../../examples/vehicle_location/">Vehicle location</a></li></ul></li><li><a class="tocitem" href="../../apireference/">API Reference</a></li><li><a class="tocitem" href="../../release_notes/">Release notes</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Alternative forward models</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Alternative forward models</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/odow/SDDP.jl/blob/master/docs/src/tutorial/pglib_opf.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Alternative-forward-models"><a class="docs-heading-anchor" href="#Alternative-forward-models">Alternative forward models</a><a id="Alternative-forward-models-1"></a><a class="docs-heading-anchor-permalink" href="#Alternative-forward-models" title="Permalink"></a></h1><p><a href="https://mybinder.org/v2/gh/odow/SDDP.jl/gh-pages?filepath=previews/PR657/tutorial/pglib_opf.ipynb"><img src="https://mybinder.org/badge_logo.svg" alt/></a> <a href="https://nbviewer.jupyter.org/github/odow/SDDP.jl/blob/gh-pages/previews/PR657/tutorial/pglib_opf.ipynb"><img src="https://img.shields.io/badge/show-nbviewer-579ACA.svg" alt/></a></p><p>This example demonstrates how to train convex and non-convex models.</p><p>This example uses the following packages:</p><pre><code class="language-julia hljs">using SDDP
import Ipopt
import PowerModels
import Test</code></pre><h2 id="Formulation"><a class="docs-heading-anchor" href="#Formulation">Formulation</a><a id="Formulation-1"></a><a class="docs-heading-anchor-permalink" href="#Formulation" title="Permalink"></a></h2><p>For our model, we build a simple optimal power flow model with a single hydro-electric generator.</p><p>The formulation of our optimal power flow problem depends on <code>model_type</code>, which must be one of the <code>PowerModels</code> formulations.</p><pre><code class="language-julia hljs">function build_model(model_type)
    filename = joinpath(@__DIR__, &quot;pglib_opf_case5_pjm.m&quot;)
    data = PowerModels.parse_file(filename)
    return SDDP.PolicyGraph(
        SDDP.UnicyclicGraph(0.95);
        sense = :Min,
        lower_bound = 0.0,
        optimizer = Ipopt.Optimizer,
    ) do sp, t
        power_model = PowerModels.instantiate_model(
            data,
            model_type,
            PowerModels.build_opf;
            jump_model = sp,
        )
        # Now add hydro power models. Assume that generator 5 is hydro, and the
        # rest are thermal.
        pg = power_model.var[:it][:pm][:nw][0][:pg][5]
        sp[:pg] = pg
        @variable(sp, x &gt;= 0, SDDP.State, initial_value = 10.0)
        @variable(sp, deficit &gt;= 0)
        @constraint(sp, balance, x.out == x.in - pg + deficit)
        @stageobjective(sp, objective_function(sp) + 1e6 * deficit)
        SDDP.parameterize(sp, [0, 2, 5]) do ω
            return SDDP.set_normalized_rhs(balance, ω)
        end
        return
    end
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">build_model (generic function with 1 method)</code></pre><h2 id="Training-a-convex-model"><a class="docs-heading-anchor" href="#Training-a-convex-model">Training a convex model</a><a id="Training-a-convex-model-1"></a><a class="docs-heading-anchor-permalink" href="#Training-a-convex-model" title="Permalink"></a></h2><p>We can build and train a convex approximation of the optimal power flow problem.</p><p>The problem with the convex model is that it does not accurately simulate the true dynamics of the problem. Therefore, it under-estimates the true cost of operation.</p><pre><code class="language-julia hljs">convex = build_model(PowerModels.DCPPowerModel)
SDDP.train(convex; iteration_limit = 10)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-------------------------------------------------------------------
         SDDP.jl (c) Oscar Dowson and contributors, 2017-23
-------------------------------------------------------------------
problem
  nodes           : 1
  state variables : 1
  scenarios       : Inf
  existing cuts   : false
options
  solver          : serial mode
  risk measure    : SDDP.Expectation()
  sampling scheme : SDDP.InSampleMonteCarlo
subproblem structure
  VariableRef                             : [20, 20]
  AffExpr in MOI.EqualTo{Float64}         : [13, 13]
  AffExpr in MOI.GreaterThan{Float64}     : [6, 6]
  AffExpr in MOI.LessThan{Float64}        : [6, 6]
  VariableRef in MOI.GreaterThan{Float64} : [14, 14]
  VariableRef in MOI.LessThan{Float64}    : [11, 11]
numerical stability report
  matrix range     [1e+00, 2e+02]
  objective range  [1e+00, 1e+06]
  bounds range     [4e-01, 6e+00]
  rhs range        [5e-01, 5e+00]
-------------------------------------------------------------------
 iteration    simulation      bound        time (s)     solves  pid
-------------------------------------------------------------------
         1   4.925360e+06  6.703028e+04  9.342000e-01       127   1
         2   3.495977e+04  9.959109e+04  2.371727e+01      2195   1
         8   1.838922e+06  3.560835e+05  3.131486e+01      2797   1
        10   6.484782e+05  3.785729e+05  3.496293e+01      2999   1
-------------------------------------------------------------------
status         : iteration_limit
total time (s) : 3.496293e+01
total solves   : 2999
best bound     :  3.785729e+05
simulation ci  :  1.518919e+06 ± 1.048752e+06
numeric issues : 0
-------------------------------------------------------------------</code></pre><p>To more accurately simulate the dynamics of the problem, a common approach is to write the cuts representing the policy to a file, and then read them into a non-convex model:</p><pre><code class="language-julia hljs">SDDP.write_cuts_to_file(convex, &quot;convex.cuts.json&quot;)
non_convex = build_model(PowerModels.ACPPowerModel)
SDDP.read_cuts_from_file(non_convex, &quot;convex.cuts.json&quot;)</code></pre><p>Now we can simulate <code>non_convex</code> to evaluate the policy.</p><pre><code class="language-julia hljs">result = SDDP.simulate(non_convex, 1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1-element Vector{Vector{Dict{Symbol, Any}}}:
 [Dict(:bellman_term =&gt; 358059.06377165637, :noise_term =&gt; 2, :node_index =&gt; 1, :stage_objective =&gt; 21433.3755758194, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 359876.37985014735, :noise_term =&gt; 2, :node_index =&gt; 1, :stage_objective =&gt; 21433.3755758194, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 361693.6959286384, :noise_term =&gt; 2, :node_index =&gt; 1, :stage_objective =&gt; 21433.3755758194, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 368535.48768142285, :noise_term =&gt; 0, :node_index =&gt; 1, :stage_objective =&gt; 21433.375575819413, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 370352.8037599138, :noise_term =&gt; 2, :node_index =&gt; 1, :stage_objective =&gt; 21433.375575819427, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 364633.40632696473, :noise_term =&gt; 5, :node_index =&gt; 1, :stage_objective =&gt; 21433.375575819406, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 358914.0088940157, :noise_term =&gt; 5, :node_index =&gt; 1, :stage_objective =&gt; 21433.3755758194, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 365755.80064680014, :noise_term =&gt; 0, :node_index =&gt; 1, :stage_objective =&gt; 21433.375575819417, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 372597.59239958244, :noise_term =&gt; 0, :node_index =&gt; 1, :stage_objective =&gt; 21433.37557581863, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 366878.1949666334, :noise_term =&gt; 5, :node_index =&gt; 1, :stage_objective =&gt; 21433.375575819413, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0))  …  Dict(:bellman_term =&gt; 360851.290331146, :noise_term =&gt; 2, :node_index =&gt; 1, :stage_objective =&gt; 21433.3755758194, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 355131.8928981969, :noise_term =&gt; 5, :node_index =&gt; 1, :stage_objective =&gt; 21433.37557581939, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 349412.49546524783, :noise_term =&gt; 5, :node_index =&gt; 1, :stage_objective =&gt; 21433.375575819406, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 356254.2872180322, :noise_term =&gt; 0, :node_index =&gt; 1, :stage_objective =&gt; 21433.37557581939, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 350534.8897850831, :noise_term =&gt; 5, :node_index =&gt; 1, :stage_objective =&gt; 21433.375575819384, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 344815.49235213414, :noise_term =&gt; 5, :node_index =&gt; 1, :stage_objective =&gt; 21433.375575819362, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 346632.8084306251, :noise_term =&gt; 2, :node_index =&gt; 1, :stage_objective =&gt; 21433.375575819377, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 340913.41099767626, :noise_term =&gt; 5, :node_index =&gt; 1, :stage_objective =&gt; 21433.37557581935, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 347755.2027504606, :noise_term =&gt; 0, :node_index =&gt; 1, :stage_objective =&gt; 21433.375575819377, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 354596.99450324505, :noise_term =&gt; 0, :node_index =&gt; 1, :stage_objective =&gt; 21433.375575819395, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0))]</code></pre><p>A problem with reading and writing the cuts to file is that the cuts have been generated from trial points of the convex model. Therefore, the policy may be arbitrarily bad at points visited by the non-convex model.</p><h2 id="Training-a-non-convex-model"><a class="docs-heading-anchor" href="#Training-a-non-convex-model">Training a non-convex model</a><a id="Training-a-non-convex-model-1"></a><a class="docs-heading-anchor-permalink" href="#Training-a-non-convex-model" title="Permalink"></a></h2><p>We can also build and train a non-convex formulation of the optimal power flow problem.</p><p>The problem with the non-convex model is that because it is non-convex, SDDP.jl may find a sub-optimal policy. Therefore, it may over-estimate the true cost of operation.</p><pre><code class="language-julia hljs">non_convex = build_model(PowerModels.ACPPowerModel)
SDDP.train(non_convex; iteration_limit = 10)
result = SDDP.simulate(non_convex, 1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1-element Vector{Vector{Dict{Symbol, Any}}}:
 [Dict(:bellman_term =&gt; 390207.7911293444, :noise_term =&gt; 5, :node_index =&gt; 1, :stage_objective =&gt; 17573.83196611601, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 389800.6650864994, :noise_term =&gt; 5, :node_index =&gt; 1, :stage_objective =&gt; 17573.83196586759, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 395936.59347707906, :noise_term =&gt; 0, :node_index =&gt; 1, :stage_objective =&gt; 17575.550298709524, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 399580.111707467, :noise_term =&gt; 2, :node_index =&gt; 1, :stage_objective =&gt; 17578.2102386555, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 402430.7644447604, :noise_term =&gt; 0, :node_index =&gt; 1, :stage_objective =&gt; 23762.234496242076, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 484313.24770937266, :noise_term =&gt; 0, :node_index =&gt; 1, :stage_objective =&gt; 27420.55350585842, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 686298.3524279319, :noise_term =&gt; 0, :node_index =&gt; 1, :stage_objective =&gt; 117034.98538408938, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 407279.3398372046, :noise_term =&gt; 2, :node_index =&gt; 1, :stage_objective =&gt; 27420.553505870113, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0)), Dict(:bellman_term =&gt; 402021.3041479608, :noise_term =&gt; 5, :node_index =&gt; 1, :stage_objective =&gt; 17578.226291715368, :objective_state =&gt; nothing, :belief =&gt; Dict(1 =&gt; 1.0))]</code></pre><h2 id="Combining-convex-and-non-convex-models"><a class="docs-heading-anchor" href="#Combining-convex-and-non-convex-models">Combining convex and non-convex models</a><a id="Combining-convex-and-non-convex-models-1"></a><a class="docs-heading-anchor-permalink" href="#Combining-convex-and-non-convex-models" title="Permalink"></a></h2><p>To summarize, training with the convex model constructs cuts at points that may never be visited by the non-convex model, and training with the non-convex model may construct arbitrarily poor cuts because a key assumption of SDDP is convexity.</p><p>As a compromise, we can train a policy using a combination of the convex and non-convex models; we&#39;ll use the non-convex model to generate trial points on the forward pass, and we&#39;ll use the convex model to build cuts on the backward pass.</p><pre><code class="language-julia hljs">convex = build_model(PowerModels.DCPPowerModel)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">A policy graph with 1 nodes.
 Node indices: 1
</code></pre><pre><code class="language-julia hljs">non_convex = build_model(PowerModels.ACPPowerModel)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">A policy graph with 1 nodes.
 Node indices: 1
</code></pre><p>To do so, we train <code>convex</code> using the <a href="../../apireference/#SDDP.AlternativeForwardPass"><code>SDDP.AlternativeForwardPass</code></a> forward pass, which simulates the model using <code>non_convex</code>, and we use <a href="../../apireference/#SDDP.AlternativePostIterationCallback"><code>SDDP.AlternativePostIterationCallback</code></a> as a post-iteration callback, which copies cuts from the <code>convex</code> model back into the <code>non_convex</code> model.</p><pre><code class="language-julia hljs">SDDP.train(
    convex;
    forward_pass = SDDP.AlternativeForwardPass(non_convex),
    post_iteration_callback = SDDP.AlternativePostIterationCallback(non_convex),
    iteration_limit = 10,
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-------------------------------------------------------------------
         SDDP.jl (c) Oscar Dowson and contributors, 2017-23
-------------------------------------------------------------------
problem
  nodes           : 1
  state variables : 1
  scenarios       : Inf
  existing cuts   : false
options
  solver          : serial mode
  risk measure    : SDDP.Expectation()
  sampling scheme : SDDP.InSampleMonteCarlo
subproblem structure
  VariableRef                             : [20, 20]
  AffExpr in MOI.EqualTo{Float64}         : [13, 13]
  AffExpr in MOI.GreaterThan{Float64}     : [6, 6]
  AffExpr in MOI.LessThan{Float64}        : [6, 6]
  VariableRef in MOI.GreaterThan{Float64} : [14, 14]
  VariableRef in MOI.LessThan{Float64}    : [11, 11]
numerical stability report
  matrix range     [1e+00, 2e+02]
  objective range  [1e+00, 1e+06]
  bounds range     [4e-01, 6e+00]
  rhs range        [5e-01, 5e+00]
-------------------------------------------------------------------
 iteration    simulation      bound        time (s)     solves  pid
-------------------------------------------------------------------
         1   9.454728e+05  1.156855e+05  6.111290e-01        39   1
         2   1.053587e+05  1.442455e+05  2.339902e+01      2097   1
        10   3.414288e+05  3.694155e+05  2.879720e+01      2349   1
-------------------------------------------------------------------
status         : iteration_limit
total time (s) : 2.879720e+01
total solves   : 2349
best bound     :  3.694155e+05
simulation ci  :  3.045778e+05 ± 1.714521e+05
numeric issues : 0
-------------------------------------------------------------------</code></pre><p>In practice, if we were to simulate <code>non_convex</code> now, we should obtain a better policy than either of the two previous approaches.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../objective_states/">« Objective states</a><a class="docs-footer-nextpage" href="../mdps/">Example: Markov Decision Processes »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Saturday 26 August 2023 22:10">Saturday 26 August 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
