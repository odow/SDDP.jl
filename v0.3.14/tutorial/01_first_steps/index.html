<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Basic I: first steps · SDDP.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="SDDP.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">SDDP.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox" checked/><label class="tocitem" for="menuitem-2"><span class="docs-label">Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Basic I: first steps</a><ul class="internal"><li><a class="tocitem" href="#Background-theory"><span>Background theory</span></a></li><li><a class="tocitem" href="#Example:-hydro-thermal-scheduling"><span>Example: hydro-thermal scheduling</span></a></li><li><a class="tocitem" href="#Training-a-policy"><span>Training a policy</span></a></li><li><a class="tocitem" href="#Obtaining-the-decision-rule"><span>Obtaining the decision rule</span></a></li><li><a class="tocitem" href="#Simulating-the-policy"><span>Simulating the policy</span></a></li><li><a class="tocitem" href="#Extracting-the-water-values"><span>Extracting the water values</span></a></li></ul></li><li><a class="tocitem" href="../02_adding_uncertainty/">Basic II: adding uncertainty</a></li><li><a class="tocitem" href="../03_objective_uncertainty/">Basic III: objective uncertainty</a></li><li><a class="tocitem" href="../04_markov_uncertainty/">Basic IV: Markov uncertainty</a></li><li><a class="tocitem" href="../05_plotting/">Basic V: plotting</a></li><li><a class="tocitem" href="../06_warnings/">Basic VI: words of warning</a></li><li><a class="tocitem" href="../11_objective_states/">Advanced I: objective states</a></li><li><a class="tocitem" href="../12_belief_states/">Advanced II: belief states</a></li><li><a class="tocitem" href="../13_integrality/">Advanced III: integrality</a></li><li><a class="tocitem" href="../21_theory_intro/">Theory I: an intro to SDDP</a></li><li><a class="tocitem" href="../22_risk/">Theory II: risk aversion</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">How-to guides</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../guides/add_a_multidimensional_state_variable/">Add a multi-dimensional state variable</a></li><li><a class="tocitem" href="../../guides/add_a_risk_measure/">Add a risk measure</a></li><li><a class="tocitem" href="../../guides/add_multidimensional_noise_Terms/">Add multi-dimensional noise terms</a></li><li><a class="tocitem" href="../../guides/add_noise_in_the_constraint_matrix/">Add noise in the constraint matrix</a></li><li><a class="tocitem" href="../../guides/choose_a_stopping_rule/">Choose a stopping rule</a></li><li><a class="tocitem" href="../../guides/create_a_general_policy_graph/">Create a general policy graph</a></li><li><a class="tocitem" href="../../guides/debug_a_model/">Debug a model</a></li><li><a class="tocitem" href="../../guides/improve_computational_performance/">Improve computational performance</a></li><li><a class="tocitem" href="../../guides/simulate_using_a_different_sampling_scheme/">Simulate using a different sampling scheme</a></li><li><a class="tocitem" href="../../guides/upgrade_from_the_old_sddp/">Upgrade from the old SDDP.jl</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/FAST_hydro_thermal/">FAST: the hydro-thermal problem</a></li><li><a class="tocitem" href="../../examples/FAST_production_management/">FAST: the production management problem</a></li><li><a class="tocitem" href="../../examples/FAST_quickstart/">FAST: the quickstart problem</a></li><li><a class="tocitem" href="../../examples/Hydro_thermal/">Hydro-thermal scheduling</a></li><li><a class="tocitem" href="../../examples/StochDynamicProgramming.jl_multistock/">StochDynamicProgramming: the multistock problem</a></li><li><a class="tocitem" href="../../examples/StochDynamicProgramming.jl_stock/">StochDynamicProgramming: the stock problem</a></li><li><a class="tocitem" href="../../examples/StructDualDynProg.jl_prob5.2_2stages/">StructDualDynProg: Problem 5.2, 2 stages</a></li><li><a class="tocitem" href="../../examples/StructDualDynProg.jl_prob5.2_3stages/">StructDualDynProg: Problem 5.2, 3 stages</a></li><li><a class="tocitem" href="../../examples/agriculture_mccardle_farm/">The farm planning problem</a></li><li><a class="tocitem" href="../../examples/air_conditioning/">Air conditioning</a></li><li><a class="tocitem" href="../../examples/all_blacks/">Deterministic All Blacks</a></li><li><a class="tocitem" href="../../examples/asset_management_simple/">Asset management</a></li><li><a class="tocitem" href="../../examples/asset_management_stagewise/">Asset management with modifications</a></li><li><a class="tocitem" href="../../examples/belief/">Partially observable inventory management</a></li><li><a class="tocitem" href="../../examples/biobjective_hydro/">Biobjective hydro-thermal</a></li><li><a class="tocitem" href="../../examples/booking_management/">Booking management</a></li><li><a class="tocitem" href="../../examples/generation_expansion/">Generation expansion</a></li><li><a class="tocitem" href="../../examples/hydro_valley/">Hydro valleys</a></li><li><a class="tocitem" href="../../examples/infinite_horizon_hydro_thermal/">Infinite horizon hydro-thermal</a></li><li><a class="tocitem" href="../../examples/infinite_horizon_trivial/">Infinite horizon trivial</a></li><li><a class="tocitem" href="../../examples/no_strong_duality/">No strong duality</a></li><li><a class="tocitem" href="../../examples/objective_state_newsvendor/">Newsvendor</a></li><li><a class="tocitem" href="../../examples/sldp_example_one/">SLDP: example 1</a></li><li><a class="tocitem" href="../../examples/sldp_example_two/">SLDP: example 2</a></li><li><a class="tocitem" href="../../examples/stochastic_all_blacks/">Stochastic All Blacks</a></li><li><a class="tocitem" href="../../examples/the_farmers_problem/">The farmer&#39;s problem</a></li><li><a class="tocitem" href="../../examples/vehicle_location/">Vehicle location</a></li></ul></li><li><a class="tocitem" href="../../apireference/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Basic I: first steps</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Basic I: first steps</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/odow/SDDP.jl/blob/master/docs/src/tutorial/01_first_steps.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Basic-I:-first-steps"><a class="docs-heading-anchor" href="#Basic-I:-first-steps">Basic I: first steps</a><a id="Basic-I:-first-steps-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-I:-first-steps" title="Permalink"></a></h1><p>SDDP.jl is a solver for multistage stochastic optimization problems. By <strong>multistage</strong>, we mean problems in which the agent makes a sequence of decisions over time. By <strong>stochastic</strong>, we mean that our agent is making decisions in the presence of uncertainty that is gradually revealed over the multiple stages.</p><h2 id="Background-theory"><a class="docs-heading-anchor" href="#Background-theory">Background theory</a><a id="Background-theory-1"></a><a class="docs-heading-anchor-permalink" href="#Background-theory" title="Permalink"></a></h2><p>Multistage stochastic programming is complicated, and the literature has not settled upon standard naming conventions, so we must begin with some unavoidable theory and notation.</p><h3 id="Policy-graphs"><a class="docs-heading-anchor" href="#Policy-graphs">Policy graphs</a><a id="Policy-graphs-1"></a><a class="docs-heading-anchor-permalink" href="#Policy-graphs" title="Permalink"></a></h3><p>A multistage stochastic program can be modeled by a <strong>policy graph</strong>. A policy graph is a graph with nodes and arcs. The simplest type of policy graph is a linear graph. Here&#39;s a linear graph with three nodes:</p><p><img src="../../assets/stochastic_linear_policy_graph.png" alt="Linear policy graph"/></p><p>In addition to nodes 1, 2, and 3, there is also a root node (the circle), and three arcs. Each arc has an origin node and a destination node, like <code>1 =&gt; 2</code>, and a corresponding probability of transitioning from the origin to the destination. Unless specified, we assume that the arc probabilities are uniform over the number of outgoing arcs. Thus, in this picture the arc probabilities are all 1.0. The squiggly lines denote random variables that we will discuss shortly.</p><p>We denote the set of nodes by <span>$\mathcal{N}$</span>, the root node by <span>$R$</span>, and the probability of transitioning from node <span>$i$</span> to node <span>$j$</span> by <span>$p_{ij}$</span>. (If no arc exists, then <span>$p_{ij} = 0$</span>.) We define the set of successors of node <span>$i$</span> as <span>$i^+ = \{j \in \mathcal{N} | p_{ij} &gt; 0\}$</span>.</p><p>Each square node in the graph corresponds to a place at which the agent makes a decision, and we call moments in time at which the agent makes a decision <strong>stages</strong>. By convention, we try to draw policy graphs from left-to-right, with the stages as columns. There can be more than one node in a stage! Here&#39;s an example, taken from the paper <a href="https://doi.org/10.1002/net.21932">Dowson (2020)</a>:</p><p><img src="../../assets/powder_policy_graph.png" alt="Markovian policy graph"/></p><p>The columns represent time, and the rows represent different states of the world. In this case, the rows represent different prices that milk can be sold for at the end of each year. The squiggly lines denote a multivariate random variable that models the weekly amount of rainfall that occurs. You can think of the nodes as forming a Markov chain, therefore, we call problems with a structure like this <strong>Markovian policy graphs</strong>. Moreover, note that policy graphs can have cycles! This allows them to model infinite horizon problems.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The sum of probabilities on the outgoing arcs of node <span>$i$</span> can be less than 1, i.e., <span>$\sum\limits_{j\in i^+} p_{ij} \le 1$</span>. What does this mean? One interpretation is that the probability is a <a href="https://en.wikipedia.org/wiki/Discounting">discount factor</a>. Another interpretation is that there is an implicit &quot;zero&quot; node that we have not modeled, with <span>$p_{i0} = 1 - \sum\limits_{j\in i^+} p_{ij}$</span>. This zero node has <span>$C_0(x, u, \omega) = 0$</span>, and <span>$0^+ = \varnothing$</span>.</p></div></div><h3 id="Problem-notation"><a class="docs-heading-anchor" href="#Problem-notation">Problem notation</a><a id="Problem-notation-1"></a><a class="docs-heading-anchor-permalink" href="#Problem-notation" title="Permalink"></a></h3><p>A common feature of multistage stochastic optimization problems is that they model an agent controlling a system over time. This system can be described by three types of variables.</p><ol><li><p><strong>State</strong> variables track a property of the system over time.</p><p>Each node has an associated <em>incoming</em> state variable (the value of the state at the start of the node), and an <em>outgoing</em> state variable (the value of the state at the end of the node).</p><p>Examples of state variables include the volume of water in a reservoir, the number of units of inventory in a warehouse, or the spatial position of a moving vehicle.</p><p>Because state variables track the system over time, each node must have the same set of state variables.</p><p>We denote state variables by the letter <span>$x$</span> for the incoming state variable and <span>$x^\prime$</span> for the outgoing state variable.</p></li><li><p><strong>Control</strong> variables are actions taken (implicitly or explicitly) by the agent within a node which modify the state variables.</p><p>Examples of control variables include releases of water from the reservoir, sales or purchasing decisions, and acceleration or braking of the vehicle.</p><p>Control variables are local to a node <span>$i$</span>, and they can differ between nodes. For example, some control variables may be available within certain nodes.</p><p>We denote control variables by the letter <span>$u$</span>.</p></li><li><p><strong>Random</strong> variables are finite, discrete, exogenous random variables that the agent observes at the start of a node, before the control variables are decided.</p><p>Examples of random variables include rainfall inflow into a reservoir, probalistic perishing of inventory, and steering errors in a vehicle.</p><p>Random variables are local to a node <span>$i$</span>, and they can differ between nodes. For example, some nodes may have random variables, and some nodes may not.</p><p>We denote random variables by the Greek letter <span>$\omega$</span> and the sample space from which they are drawn by <span>$\Omega_i$</span>. The probability of sampling <span>$\omega$</span> is denoted <span>$p_{\omega}$</span> for simplicity.</p><p>Importantly, the random variable associated with node <span>$i$</span> is independent of the random variables in all other nodes.</p></li></ol><p>In a node <span>$i$</span>, the three variables are related by a <strong>transition function</strong>, which maps the incoming state, the controls, and the random variables to the outgoing state as follows: <span>$x^\prime = T_i(x, u, \omega)$</span>.</p><p>As a result of entering a node <span>$i$</span> with the incoming state <span>$x$</span>, observing random variable <span>$\omega$</span>, and choosing control <span>$u$</span>, the agent incurs a cost <span>$C_i(x, u, \omega)$</span>. (If the agent is a maximizer, this can be a profit, or a negative cost.) We call <span>$C_i$</span> the <strong>stage objective</strong>.</p><p>To choose their control variables in node <span>$i$</span>, the agent uses a <strong>decision</strong> <strong>rule</strong> <span>$u = \pi_i(x, \omega)$</span>, which is a function that maps the incoming state variable and observation of the random variable to a control <span>$u$</span>. This control must satisfy some feasibilty requirements <span>$u \in U_i(x, \omega)$</span>.</p><p>The set of decision rules, with one element for each node in the policy graph, is called a <strong>policy</strong>.</p><p>The goal of the agent is to find a policy that minimizes the expected cost of starting at the root node with some initial condition <span>$x_R$</span>, and proceeding from node to node along the probabilistic arcs until they reach a node with no outgoing arcs (or it reaches an implicit &quot;zero&quot; node).</p><p class="math-container">\[\min_{\pi} \mathbb{E}_{i \in R^+, \omega \in \Omega_i}[V_i^\pi(x_R, \omega)],\]</p><p>where</p><p class="math-container">\[V_i^\pi(x, \omega) = C_i(x, u, \omega) + \mathbb{E}_{j \in i^+, \varphi \in \Omega_j}[V_j(x^\prime, \varphi)],\]</p><p>where <span>$u = \pi_i(x, \omega) \in U_i(x, \omega)$</span>, and <span>$x^\prime = T_i(x, u, \omega)$</span>.</p><p>The expectations are a bit complicated, but they are equivalent to:</p><p class="math-container">\[\mathbb{E}_{j \in i^+, \varphi \in \Omega_j}[V_j(x^\prime, \varphi)] = \sum\limits_{j \in i^+} p_{ij} \sum\limits_{\varphi \in \Omega_j} p_{\varphi}V_j(x^\prime, \varphi).\]</p><p>An optimal policy is the set of decision rules that the agent can use to make decisions and achieve the smallest expected cost.</p><h3 id="Assumptions"><a class="docs-heading-anchor" href="#Assumptions">Assumptions</a><a id="Assumptions-1"></a><a class="docs-heading-anchor-permalink" href="#Assumptions" title="Permalink"></a></h3><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>This section is important!</p></div></div><p>The space of problems you can model with this framework is very large. Too large, in fact, for us to form tractable solution algorithms for! Stochastic dual dynamic programming requires the following assumptions in order to work:</p><p><strong>Assumption 1: finite nodes</strong></p><p>There is a finite number of nodes in <span>$\mathcal{N}$</span>.</p><p><strong>Assumption 2: finite random variables</strong></p><p>The sample space <span>$\Omega_i$</span> is finite and discrete for each node <span>$i\in\mathcal{N}$</span>.</p><p><strong>Assumption 3: convex problems</strong></p><p>Given fixed <span>$\omega$</span>, <span>$C_i(x, u, \omega)$</span> is a convex function, <span>$T_i(x, u, \omega)$</span> is linear, and  <span>$U_i(x, u, \omega)$</span> is a non-empty, bounded convex set with respect to <span>$x$</span> and <span>$u$</span>.</p><p><strong>Assumption 4: no infinite loops</strong></p><p>For all loops in the policy graph, the product of the arc transition probabilities around the loop is strictly less than 1.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>SDDP.jl relaxes assumption (3) to allow for integer state and control variables, but we won&#39;t go into the details here. Assumption (4) essentially means that we obtain a discounted-cost solution for infinite-horizon problems, instead of an average-cost solution; see <a href="https://doi.org/10.1002/net.21932">Dowson (2020)</a> for details.</p></div></div><h3 id="Dynamic-programming-and-subproblems"><a class="docs-heading-anchor" href="#Dynamic-programming-and-subproblems">Dynamic programming and subproblems</a><a id="Dynamic-programming-and-subproblems-1"></a><a class="docs-heading-anchor-permalink" href="#Dynamic-programming-and-subproblems" title="Permalink"></a></h3><p>Now that we have formulated our problem, we need some ways of computing optimal decision rules. One way is to just use a heuristic like &quot;choose a control randomally from the set of feasible controls.&quot; However, such a policy is unlikely to be optimal.</p><p>A better way of obtaining an optimal policy is to use <a href="https://en.wikipedia.org/wiki/Bellman_equation#Bellman&#39;s_principle_of_optimality">Bellman&#39;s principle of optimality</a>, a.k.a Dynamic Programming, and define a recursive <strong>subproblem</strong> as follows:</p><p class="math-container">\[\begin{aligned}
V_i(x, \omega) = \min\limits_{\bar{x}, x^\prime, u} \;\; &amp; C_i(\bar{x}, u, \omega) + \mathbb{E}_{j \in i^+, \varphi \in \Omega_j}[V_j(x^\prime, \varphi)]\\
&amp; x^\prime = T_i(\bar{x}, u, \omega) \\
&amp; u \in U_i(\bar{x}, \omega) \\
&amp; \bar{x} = x.
\end{aligned}\]</p><p>Our decision rule, <span>$\pi_i(x, \omega)$</span>, solves this optimization problem and returns a <span>$u^*$</span> corresponding to an optimal solution.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>We add <span>$\bar{x}$</span> as a decision variable, along with the fishing constraint <span>$\bar{x} = x$</span> for two reasons: it makes it obvious that formulating a problem with <span>$x \times u$</span> results in a bilinear program instead of a linear program (see Assumption 3), and it simplifies the implementation of the SDDP algorithm.</p></div></div><p>These subproblems are very difficult to solve exactly, because they involve recursive optimization problems with lots of nested expectations.</p><p>Therefore, instead of solving them exactly, SDDP.jl works by iteratively approximating the expectation term of each subproblem, which is also called the cost-to-go term. For now, you don&#39;t need to understand the details, other than that there is a nasty cost-to-go term that we deal with behind-the-scenes.</p><p>The subproblem view of a multistage stochastic program is also important, because it provides a convienient way of communicating the different parts of the broader problem, and it is how we will communicate the problem to SDDP.jl. All we need to do is drop the cost-to-go term and fishing constraint, and define a new subproblem <code>SP</code> as:</p><p class="math-container">\[\begin{aligned}
\texttt{SP}_i(x, \omega) : \min\limits_{\bar{x}, x^\prime, u} \;\; &amp; C_i(\bar{x}, u, \omega) \\
&amp; x^\prime = T_i(\bar{x}, u, \omega) \\
&amp; u \in U_i(\bar{x}, \omega).
\end{aligned}\]</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>When we talk about formulating a <strong>subproblem</strong> with SDDP.jl, this is the formulation we mean.</p></div></div><p>We&#39;ve retained the transition function and uncertainty set because they help to motivate the different components of the subproblem. However, in general, the subproblem can be more general. A better (less restrictive) representation might be:</p><p class="math-container">\[\begin{aligned}
\texttt{SP}_i(x, \omega) : \min\limits_{\bar{x}, x^\prime, u} \;\; &amp; C_i(\bar{x}, x^\prime, u, \omega) \\
&amp; (\bar{x}, x^\prime, u) \in \mathcal{X}_i(\omega).
\end{aligned}\]</p><p>Note that the outgoing state variable can appear in the objective, and we can add constraints involving the incoming and outgoing state variables. It should be obvious how to map between the two representations.</p><h2 id="Example:-hydro-thermal-scheduling"><a class="docs-heading-anchor" href="#Example:-hydro-thermal-scheduling">Example: hydro-thermal scheduling</a><a id="Example:-hydro-thermal-scheduling-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-hydro-thermal-scheduling" title="Permalink"></a></h2><p>Hydrothermal scheduling is the most common application of stochastic dual dynamic programming. To illustrate some of the basic functionality of <code>SDDP.jl</code>, we implement a very simple model of the hydrothermal scheduling problem. To make things even simpler to start with, we&#39;re not going to include any uncertainty; that will come in the next tutorial.</p><h3 id="Problem-statement"><a class="docs-heading-anchor" href="#Problem-statement">Problem statement</a><a id="Problem-statement-1"></a><a class="docs-heading-anchor-permalink" href="#Problem-statement" title="Permalink"></a></h3><p>We consider the problem of scheduling electrical generation over three weeks in order to meet a known demand of 150 MWh in each week.</p><p>There are two generators: a thermal generator, and a hydro generator. In each week, the agent needs to decide how much energy to generate from thermal, and how much energy to generate from hydro.</p><p>The thermal generator has a short-run marginal cost of \$50/MWh in the first stage, \$100/MWh in the second stage, and \$150/MWh in the third stage.</p><p>The hydro generator has a short-run marginal cost of \$0/MWh.</p><p>The hydro generator draws water from a reservoir which has a maximum capacity of 200 MWh. (Although water is usually measured in m³, we measure it in the energy-equivalent MWh to simplify things. In practice, there is a conversion function between m³ flowing throw the turbine and MWh.) At the start of the first time period, the reservoir is full.</p><p>In addition to the ability to generate electricity by passing water through the hydroelectric turbine, the hydro generator can also spill water down a spillway (bypassing the turbine) in order to prevent the water from over-topping the dam. We assume that there is no cost of spillage.</p><p>The goal of the agent is to minimize the expected cost of generation over the three weeks.</p><h3 id="Formulating-the-problem"><a class="docs-heading-anchor" href="#Formulating-the-problem">Formulating the problem</a><a id="Formulating-the-problem-1"></a><a class="docs-heading-anchor-permalink" href="#Formulating-the-problem" title="Permalink"></a></h3><p>Before going further, we need to load SDDP.jl:</p><pre><code class="language-julia">using SDDP</code></pre><h4 id="Graph-structure"><a class="docs-heading-anchor" href="#Graph-structure">Graph structure</a><a id="Graph-structure-1"></a><a class="docs-heading-anchor-permalink" href="#Graph-structure" title="Permalink"></a></h4><p>First, we need to identify the structre of the policy graph. From the problem statement, we want to model the problem over three weeks in weekly stages. Therefore, the policy graph is a linear graph with three stages:</p><pre><code class="language-julia">graph = SDDP.LinearGraph(3)</code></pre><pre><code class="language-none">Root
 0
Nodes
 1
 2
 3
Arcs
 0 =&gt; 1 w.p. 1.0
 1 =&gt; 2 w.p. 1.0
 2 =&gt; 3 w.p. 1.0
</code></pre><h4 id="Building-the-subproblem"><a class="docs-heading-anchor" href="#Building-the-subproblem">Building the subproblem</a><a id="Building-the-subproblem-1"></a><a class="docs-heading-anchor-permalink" href="#Building-the-subproblem" title="Permalink"></a></h4><p>Next, we need to construct the associated subproblem for each node in <code>graph</code>. To do so, we need to provide SDDP.jl a function which takes two arguments. The first is <code>subproblem::Model</code>, which is an empty JuMP model. The second is <code>node</code>, which is the name of each node in the policy graph. If the graph is linear, SDDP defaults to naming the nodes using the integers in <code>1:T</code>. Here&#39;s an example that we are going to flesh out over the next few paragraphs:</p><pre><code class="language-julia">function subproblem_builder(subproblem::Model, node::Int)
    # ... stuff to go here ...
    return subproblem
end</code></pre><pre><code class="language-none">subproblem_builder (generic function with 1 method)</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>We don&#39;t need to add the fishing constraint <span>$\bar{x} = x$</span>; SDDP.jl does this automatically.</p></div></div><h4 id="State-variables"><a class="docs-heading-anchor" href="#State-variables">State variables</a><a id="State-variables-1"></a><a class="docs-heading-anchor-permalink" href="#State-variables" title="Permalink"></a></h4><p>The first part of the subproblem we need to identify are the state variables. Since we only have one reservoir, there is only one state variable, <code>volume</code>, the volume of water in the reservoir [MWh].</p><p>The volume had bounds of <code>[0, 200]</code>, and the reservoir was full at the start of time, so <span>$x_R = 200$</span>.</p><p>We add state variables to our <code>subproblem</code> using JuMP&#39;s <code>@variable</code> macro. However, in addition to the usual syntax, we also pass <code>SDDP.State</code>, and we need to provide the initial value (<span>$x_R$</span>) using the <code>initial_value</code> keyword.</p><pre><code class="language-julia">function subproblem_builder(subproblem::Model, node::Int)
    # State variables
    @variable(subproblem, 0 &lt;= volume &lt;= 200, SDDP.State, initial_value = 200)
    return subproblem
end</code></pre><pre><code class="language-none">subproblem_builder (generic function with 1 method)</code></pre><p>The syntax for adding a state variable is a little obtuse, because <code>volume</code> is not single JuMP variable. Instead, <code>volume</code> is a struct with two fields, <code>.in</code> and <code>.out</code>, corresponding to the incoming and outgoing state variables respectively.</p><h4 id="Control-variables"><a class="docs-heading-anchor" href="#Control-variables">Control variables</a><a id="Control-variables-1"></a><a class="docs-heading-anchor-permalink" href="#Control-variables" title="Permalink"></a></h4><p>The next part of the subproblem we need to identiy are the control variables. The control variables for our problem are:</p><ul><li><code>thermal_generation</code>: the quantity of energy generated from thermal [MWh/week]</li><li><code>hydro_generation</code>: the quantity of energy generated from hydro [MWh/week]</li><li><code>hydro_spill</code>: the volume of water spilled from the reservoir in each week [MWh/week]</li></ul><p>Each of these variables is non-negative.</p><p>We add control variables to our <code>subproblem</code> as normal JuMP variables, using <code>@variable</code> or <code>@variables</code>:</p><pre><code class="language-julia">function subproblem_builder(subproblem::Model, node::Int)
    # State variables
    @variable(subproblem, 0 &lt;= volume &lt;= 200, SDDP.State, initial_value = 200)
    # Control variables
    @variables(subproblem, begin
        thermal_generation &gt;= 0
        hydro_generation   &gt;= 0
        hydro_spill        &gt;= 0
    end)
    return subproblem
end</code></pre><pre><code class="language-none">subproblem_builder (generic function with 1 method)</code></pre><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>Modeling is an art, and a tricky part of that art is figuring out which variables are state variables, and which are control variables. A good rule is: if you need a value of a control variable in some future node to make a decision, it is a state variable instead.</p></div></div><h4 id="Random-variables"><a class="docs-heading-anchor" href="#Random-variables">Random variables</a><a id="Random-variables-1"></a><a class="docs-heading-anchor-permalink" href="#Random-variables" title="Permalink"></a></h4><p>The next step is to identify any random variables. In this simple example, there are none, so we can skip it.</p><h4 id="Transition-function-and-contraints"><a class="docs-heading-anchor" href="#Transition-function-and-contraints">Transition function and contraints</a><a id="Transition-function-and-contraints-1"></a><a class="docs-heading-anchor-permalink" href="#Transition-function-and-contraints" title="Permalink"></a></h4><p>Now that we&#39;ve identified our variables, we can define the transition function and the constraints.</p><p>For our problem, the state variable is the volume of water in the reservoir. The volume of water decreases in response to water being used for hydro generation and spillage. So the transition function is: <code>volume.out = volume.in - hydro_generation - hydro_spill</code>. (Note how we use <code>volume.in</code> and <code>volume.out</code> to refer to the incoming and outgoing state variables.)</p><p>There is also a constraint that the total generation must sum to 150 MWh.</p><p>Both the transition function and any additional constraint are added using JuMP&#39;s <code>@constraint</code> and <code>@constraints</code> macro.</p><pre><code class="language-julia">function subproblem_builder(subproblem::Model, node::Int)
    # State variables
    @variable(subproblem, 0 &lt;= volume &lt;= 200, SDDP.State, initial_value = 200)
    # Control variables
    @variables(subproblem, begin
        thermal_generation &gt;= 0
        hydro_generation   &gt;= 0
        hydro_spill        &gt;= 0
    end)
    # Transition function and constraints
    @constraints(subproblem, begin
        volume.out == volume.in - hydro_generation - hydro_spill
        hydro_generation + thermal_generation == 150
    end)
    return subproblem
end</code></pre><pre><code class="language-none">subproblem_builder (generic function with 1 method)</code></pre><h4 id="Objective-function"><a class="docs-heading-anchor" href="#Objective-function">Objective function</a><a id="Objective-function-1"></a><a class="docs-heading-anchor-permalink" href="#Objective-function" title="Permalink"></a></h4><p>Finally, we need to add an objective function using <code>@stageobjective</code>. The objective of the agent is to minimize the cost of thermal generation. This is complicated by a fuel cost that depends on the <code>node</code>.</p><p>One possibility is to use an <code>if</code> statement on <code>node</code> to define the correct objective:</p><pre><code class="language-julia">function subproblem_builder(subproblem::Model, node::Int)
    # State variables
    @variable(subproblem, 0 &lt;= volume &lt;= 200, SDDP.State, initial_value = 200)
    # Control variables
    @variables(subproblem, begin
        thermal_generation &gt;= 0
        hydro_generation   &gt;= 0
        hydro_spill        &gt;= 0
    end)
    # Transition function and constraints
    @constraints(subproblem, begin
        volume.out == volume.in - hydro_generation - hydro_spill
        hydro_generation + thermal_generation == 150
    end)
    # Stage-objective
    if node == 1
        @stageobjective(subproblem, 50 * thermal_generation)
    elseif node == 2
        @stageobjective(subproblem, 100 * thermal_generation)
    else
        @assert node == 3
        @stageobjective(subproblem, 150 * thermal_generation)
    end
    return subproblem
end</code></pre><pre><code class="language-none">subproblem_builder (generic function with 1 method)</code></pre><p>A second possibility is to use an array of fuel costs, and use <code>node</code> to index the correct value:</p><pre><code class="language-julia">function subproblem_builder(subproblem::Model, node::Int)
    # State variables
    @variable(subproblem, 0 &lt;= volume &lt;= 200, SDDP.State, initial_value = 200)
    # Control variables
    @variables(subproblem, begin
        thermal_generation &gt;= 0
        hydro_generation   &gt;= 0
        hydro_spill        &gt;= 0
    end)
    # Transition function and constraints
    @constraints(subproblem, begin
        volume.out == volume.in - hydro_generation - hydro_spill
        hydro_generation + thermal_generation == 150
    end)
    # Stage-objective
    fuel_cost = [50, 100, 150]
    @stageobjective(subproblem, fuel_cost[node] * thermal_generation)
    return subproblem
end</code></pre><pre><code class="language-none">subproblem_builder (generic function with 1 method)</code></pre><h3 id="Contructing-the-model"><a class="docs-heading-anchor" href="#Contructing-the-model">Contructing the model</a><a id="Contructing-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#Contructing-the-model" title="Permalink"></a></h3><p>Now that we&#39;ve written our subproblem, we need to contruct the full model. For that, we&#39;re going to need a linear solver. Let&#39;s choose GLPK:</p><pre><code class="language-julia">using GLPK</code></pre><p>Then, we can create a full model using <code>SDDP.PolicyGraph</code>, passing our <code>subproblem_builder</code> function as the first argument, and our <code>graph</code> as the second:</p><pre><code class="language-julia">model = SDDP.PolicyGraph(
    subproblem_builder,
    graph;
    sense = :Min,
    lower_bound = 0.0,
    optimizer = GLPK.Optimizer,
)</code></pre><pre><code class="language-none">A policy graph with 3 nodes.
 Node indices: 1, 2, 3
</code></pre><ul><li><code>sense</code>: the optimization sense. Must be <code>:Min</code> or <code>:Max</code>.</li><li><code>lower_bound</code>: you <em>must</em> supply a valid bound on the objective. For our problem, we know that we cannot incur a negative cost so \$0 is a valid lower bound.</li><li><code>optimizer</code>: This is borrowed directly from JuMP&#39;s <code>Model</code> constructor: <code>Model(GLPK.Optimizer)</code></li></ul><p>Because linear policy graphs are the most commonly used structure, we can use <code>SDDP.LinearPolicyGraph(; stages)</code> instead of passing <code>SDDP.LinearGraph(3)</code> to <code>SDDP.PolicyGraph</code>.</p><pre><code class="language-julia">model = SDDP.LinearPolicyGraph(
    subproblem_builder;
    stages = 3,
    sense = :Min,
    lower_bound = 0.0,
    optimizer = GLPK.Optimizer,
)</code></pre><pre><code class="language-none">A policy graph with 3 nodes.
 Node indices: 1, 2, 3
</code></pre><p>There is also the option is to use Julia&#39;s <code>do</code> syntax to avoid needing to define a <code>subproblem_builder</code> function separately:</p><pre><code class="language-julia">model = SDDP.LinearPolicyGraph(
    stages = 3,
    sense = :Min,
    lower_bound = 0.0,
    optimizer = GLPK.Optimizer,
) do subproblem, node
    # State variables
    @variable(subproblem, 0 &lt;= volume &lt;= 200, SDDP.State, initial_value = 200)
    # Control variables
    @variables(subproblem, begin
        thermal_generation &gt;= 0
        hydro_generation   &gt;= 0
        hydro_spill        &gt;= 0
    end)
    # Transition function and constraints
    @constraints(subproblem, begin
        volume.out == volume.in - hydro_generation - hydro_spill
        hydro_generation + thermal_generation == 150
    end)
    # Stage-objective
    if node == 1
        @stageobjective(subproblem, 50 * thermal_generation)
    elseif node == 2
        @stageobjective(subproblem, 100 * thermal_generation)
    else
        @assert node == 3
        @stageobjective(subproblem, 150 * thermal_generation)
    end
end</code></pre><pre><code class="language-none">A policy graph with 3 nodes.
 Node indices: 1, 2, 3
</code></pre><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Julia&#39;s <code>do</code> syntax is just a different way of passing an anonymous function <code>inner</code> to some function <code>outer</code> which takes <code>inner</code> as the first argument. For example, given:</p><pre><code class="language-julia">outer(inner::Function, x, y) = inner(x, y)</code></pre><p>then</p><pre><code class="language-julia">outer(1, 2) do x, y
    return x^2 + y^2
end</code></pre><p>is equivalent to:</p><pre><code class="language-julia">outer((x, y) -&gt; x^2 + y^2, 1, 2)</code></pre><p>For our purpose, <code>inner</code> is <code>subproblem_builder</code>, and <code>outer</code> is <code>SDDP.PolicyGraph</code>.</p></div></div><h2 id="Training-a-policy"><a class="docs-heading-anchor" href="#Training-a-policy">Training a policy</a><a id="Training-a-policy-1"></a><a class="docs-heading-anchor-permalink" href="#Training-a-policy" title="Permalink"></a></h2><p>Now we have a model, which is a description of the policy graph, we need to train a policy. Models can be trained using the <a href="../../apireference/#SDDP.train"><code>SDDP.train</code></a> function. It accepts a number of keyword arguments. <code>iteration_limit</code> terminates the training after the provided number of iterations.</p><pre><code class="language-julia">SDDP.train(model; iteration_limit = 3)</code></pre><pre><code class="language-none">------------------------------------------------------------------------------
                      SDDP.jl (c) Oscar Dowson, 2017-21

Problem
  Nodes           : 3
  State variables : 1
  Scenarios       : 1.00000e+00
  Solver          : serial mode

Numerical stability report
  Non-zero Matrix range     [1e+00, 1e+00]
  Non-zero Objective range  [1e+00, 2e+02]
  Non-zero Bounds range     [2e+02, 2e+02]
  Non-zero RHS range        [2e+02, 2e+02]
No problems detected

 Iteration    Simulation       Bound         Time (s)    Proc. ID   # Solves
        1    3.250000e+04   1.500000e+04   1.580000e-03          1          6
        2    1.750000e+04   1.750000e+04   1.878023e-03          1         12
        3    1.750000e+04   1.750000e+04   2.119064e-03          1         18

Terminating training with status: iteration_limit
------------------------------------------------------------------------------
</code></pre><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>For more information on the numerical stability report, read the <a href="../06_warnings/#Numerical-stability-report">Numerical stability report</a> section.</p></div></div><h2 id="Obtaining-the-decision-rule"><a class="docs-heading-anchor" href="#Obtaining-the-decision-rule">Obtaining the decision rule</a><a id="Obtaining-the-decision-rule-1"></a><a class="docs-heading-anchor-permalink" href="#Obtaining-the-decision-rule" title="Permalink"></a></h2><p>After training a policy, we can create a decision rule using <a href="../../apireference/#SDDP.DecisionRule"><code>SDDP.DecisionRule</code></a>:</p><pre><code class="language-julia">rule = SDDP.DecisionRule(model; node = 1)</code></pre><pre><code class="language-none">A decision rule for node 1</code></pre><p>Then, to evalute the decision rule, we use <a href="../../apireference/#SDDP.evaluate"><code>SDDP.evaluate</code></a>:</p><pre><code class="language-julia">solution = SDDP.evaluate(
    rule;
    incoming_state = Dict(:volume =&gt; 150.0),
    controls_to_record = [:hydro_generation, :thermal_generation],
)</code></pre><pre><code class="language-none">(stage_objective = 7500.0, outgoing_state = Dict(:volume=&gt;150.0), controls = Dict(:thermal_generation=&gt;150.0,:hydro_generation=&gt;0.0))</code></pre><h2 id="Simulating-the-policy"><a class="docs-heading-anchor" href="#Simulating-the-policy">Simulating the policy</a><a id="Simulating-the-policy-1"></a><a class="docs-heading-anchor-permalink" href="#Simulating-the-policy" title="Permalink"></a></h2><p>Once you have a trained policy, you can also simulate it using <a href="../../apireference/#SDDP.simulate"><code>SDDP.simulate</code></a>. The return value from <code>simulate</code> is a vector with one element for each replication. Each element is itself a vector, with one element for each stage. Each element, corresponding to a particular stage in a particular replication, is a dictionary that records information from the simulation.</p><pre><code class="language-julia">simulations = SDDP.simulate(
    # The trained model to simulate.
    model,
    # The number of replications.
    1,
    # A list of names to record the values of.
    [:volume, :thermal_generation, :hydro_generation, :hydro_spill]
)

replication = 1
stage = 2
simulations[replication][stage]</code></pre><pre><code class="language-none">Dict{Symbol,Any} with 10 entries:
  :volume =&gt; State{Float64}(200.0, 150.0)
  :hydro_spill =&gt; 0.0
  :bellman_term =&gt; 0.0
  :noise_term =&gt; nothing
  :node_index =&gt; 2
  :stage_objective =&gt; 10000.0
  :objective_state =&gt; nothing
  :thermal_generation =&gt; 100.0
  :hydro_generation =&gt; 50.0
  :belief =&gt; Dict(2=&gt;1.0)</code></pre><p>Ignore many of the entries for now;  they will be relevant later.</p><p>One element of iterest is <code>:volume</code>.</p><pre><code class="language-julia">outgoing_volume = [stage[:volume].out for stage in simulations[1]]</code></pre><pre><code class="language-none">3-element Array{Float64,1}:
 200.0
 150.0
   0.0</code></pre><p>Another is <code>:thermal_generation</code>.</p><pre><code class="language-julia">thermal_generation = [stage[:thermal_generation] for stage in simulations[1]]</code></pre><pre><code class="language-none">3-element Array{Float64,1}:
 150.0
 100.0
   0.0</code></pre><p>From this, we can see the optimal policy: in the first stage, use 150 MWh of thermal generation and 0 MWh of hydro generation. In the second stage, use 100 MWh of thermal and 50 MWh of hydro. In the third and final stage, use 0 MWh of thermal and 150 MWh of  hydro.</p><h2 id="Extracting-the-water-values"><a class="docs-heading-anchor" href="#Extracting-the-water-values">Extracting the water values</a><a id="Extracting-the-water-values-1"></a><a class="docs-heading-anchor-permalink" href="#Extracting-the-water-values" title="Permalink"></a></h2><p>Finally, we can use <a href="../../apireference/#SDDP.ValueFunction"><code>SDDP.ValueFunction</code></a> and <a href="../../apireference/#SDDP.evaluate"><code>SDDP.evaluate</code></a> to obtain and evaluate the value function at different points in the state-space.</p><p>First, we construct a value function from the first subproblem:</p><pre><code class="language-julia">V = SDDP.ValueFunction(model; node = 1)</code></pre><pre><code class="language-none">A value function for node 1</code></pre><p>Then we can evaluate <code>V</code> at a point:</p><pre><code class="language-julia">cost, price = SDDP.evaluate(V; volume = 10)</code></pre><pre><code class="language-none">(36000.0, Dict(:volume=&gt;-150.0))</code></pre><p>This returns the cost-to-go (<code>cost</code>), and the gradient of the cost-to-go function with resspect to each state variable. Note that since we are minimizing, the price has a negative sign: each additional unit of water leads to a decrease in the the expected long-run cost.</p><p>For our example, the value of water at the end of the first stage is \$150, because each additional unit of water can displace a unit of thermal generation in the final stage when the price is \$150/MWh.</p><p>This concludes our first tutorial for <code>SDDP.jl</code>. In the next tutorial, <a href="../02_adding_uncertainty/#Basic-II:-adding-uncertainty">Basic II: adding uncertainty</a>, we will extend this problem by adding uncertainty.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« Home</a><a class="docs-footer-nextpage" href="../02_adding_uncertainty/">Basic II: adding uncertainty »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 30 March 2021 07:27">Tuesday 30 March 2021</span>. Using Julia version 1.0.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
