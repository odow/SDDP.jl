{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Example: two-stage newsvendor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The purpose of this tutorial is to demonstrate how to model and solve a\n",
    "two-stage stochastic program."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is based on the [Two stage stochastic programs](https://jump.dev/JuMP.jl/dev/tutorials/applications/two_stage_stochastic/)\n",
    "tutorial in JuMP."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This tutorial uses the following packages"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using JuMP\n",
    "using SDDP\n",
    "import Distributions\n",
    "import HiGHS\n",
    "import Plots\n",
    "import StatsPlots\n",
    "import Statistics"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Background"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data for this problem is:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "D = Distributions.TriangularDist(150.0, 250.0, 200.0)\n",
    "N = 100\n",
    "d = sort!(rand(D, N));\n",
    "Ω = 1:N\n",
    "P = fill(1 / N, N);\n",
    "StatsPlots.histogram(d; bins = 20, label = \"\", xlabel = \"Demand\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The L-Shaped method"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The L-Shaped method is a way of solving two-stage stochastic programs by\n",
    "Benders' decomposition. It takes the problem:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\max\\limits_{x,y_\\omega} \\;\\; & -2x + \\mathbb{E}_\\omega[5y_\\omega - 0.1(x - y_\\omega)] \\\\\n",
    "  & y_\\omega \\le x              & \\quad \\forall \\omega \\in \\Omega \\\\\n",
    "  & 0 \\le y_\\omega \\le d_\\omega & \\quad \\forall \\omega \\in \\Omega \\\\\n",
    "  & x \\ge 0.\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "and decomposes it into a second-stage problem:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "V_2(\\bar{x}, d_\\omega) = \\max\\limits_{x,x^\\prime,y_\\omega} \\;\\; & 5y_\\omega - x^\\prime \\\\\n",
    "  & y_\\omega \\le x \\\\\n",
    "  & x^\\prime = x - y_\\omega \\\\\n",
    "  & 0 \\le y_\\omega \\le d_\\omega \\\\\n",
    "  & x = \\bar{x} & [\\lambda]\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "and a first-stage problem:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "V^K = \\max\\limits_{x,\\theta} \\;\\; & -2x + \\theta \\\\\n",
    "  & \\theta \\le \\mathbb{E}_\\omega[V_2(x^k, \\omega) + \\lambda^k(x - x^k)] & \\quad k = 1,\\ldots,K\\\\\n",
    "  & x \\ge 0,\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's a function to compute the second-stage problem;"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function solve_second_stage(x̅, d_ω)\n",
    "    model = Model(HiGHS.Optimizer)\n",
    "    set_silent(model)\n",
    "    @variable(model, x_in)\n",
    "    @variable(model, x_out >= 0)\n",
    "    fix(x_in, x̅)\n",
    "    @variable(model, 0 <= u_sell <= d_ω)\n",
    "    @constraint(model, x_out == x_in - u_sell)\n",
    "    @constraint(model, u_sell <= x_in)\n",
    "    @objective(model, Max, 5 * u_sell - 0.1 * x_out)\n",
    "    optimize!(model)\n",
    "    return (\n",
    "        V = objective_value(model),\n",
    "        λ = reduced_cost(x_in),\n",
    "        x = value(x_out),\n",
    "        u = value(u_sell),\n",
    "    )\n",
    "end\n",
    "\n",
    "solve_second_stage(200, 170)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's the first-stage subproblem:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = Model(HiGHS.Optimizer)\n",
    "set_silent(model)\n",
    "@variable(model, x_in == 0)\n",
    "@variable(model, x_out >= 0)\n",
    "@variable(model, u_make >= 0)\n",
    "@constraint(model, x_out == x_in + u_make)\n",
    "@variable(model, θ <= 10_000)\n",
    "@objective(model, Max, -2 * u_make + θ)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importantly, to ensure we have a bounded solution, we need to add an upper\n",
    "bound to the variable `θ`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "kIterationLimit = 100\n",
    "for k in 1:kIterationLimit\n",
    "    println(\"Solving iteration k = $k\")\n",
    "    optimize!(model)\n",
    "    xᵏ = value(x_out)\n",
    "    println(\"  xᵏ = $xᵏ\")\n",
    "    ub = objective_value(model)\n",
    "    println(\"  V̅ = $ub\")\n",
    "    ret = [solve_second_stage(xᵏ, d[ω]) for ω in Ω]\n",
    "    lb = value(-2 * u_make) + sum(p * r.V for (p, r) in zip(P, ret))\n",
    "    println(\"  V̲ = $lb\")\n",
    "    if ub - lb < 1e-6\n",
    "        println(\"Terminating with near-optimal solution\")\n",
    "        break\n",
    "    end\n",
    "    c = @constraint(\n",
    "        model,\n",
    "        θ <= sum(p * (r.V + r.λ * (x_out - xᵏ)) for (p, r) in zip(P, ret)),\n",
    "    )\n",
    "    println(\"  Added cut: $c\")\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Policy Graph"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can formulate and train a policy for the two-stage newsvendor problem."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.LinearPolicyGraph(;\n",
    "    stages = 2,\n",
    "    sense = :Max,\n",
    "    upper_bound = 5 * maximum(d),  # The `M` in θ <= M\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ") do subproblem::JuMP.Model, stage::Int\n",
    "    @variable(subproblem, x >= 0, SDDP.State, initial_value = 0)\n",
    "    if stage == 1\n",
    "        @variable(subproblem, u_make >= 0)\n",
    "        @constraint(subproblem, x.out == x.in + u_make)\n",
    "        @stageobjective(subproblem, -2 * u_make)\n",
    "    else\n",
    "        @variable(subproblem, u_sell >= 0)\n",
    "        @constraint(subproblem, u_sell <= x.in)\n",
    "        @constraint(subproblem, x.out == x.in - u_sell)\n",
    "        SDDP.parameterize(subproblem, d, P) do ω\n",
    "            set_upper_bound(u_sell, ω)\n",
    "            return\n",
    "        end\n",
    "        @stageobjective(subproblem, 5 * u_sell - 0.1 * x.out)\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "SDDP.train(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "One way to query the optimal policy is with `SDDP.DecisionRule`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "first_stage_rule = SDDP.DecisionRule(model; node = 1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "solution_1 = SDDP.evaluate(first_stage_rule; incoming_state = Dict(:x => 0.0))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's the second stage:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "second_stage_rule = SDDP.DecisionRule(model; node = 2)\n",
    "solution = SDDP.evaluate(\n",
    "    second_stage_rule;\n",
    "    incoming_state = Dict(:x => solution_1.outgoing_state[:x]),\n",
    "    noise = 170.0,  # A value of d[ω], can be out-of-sample.\n",
    "    controls_to_record = [:u_sell],\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Querying the decision rules is tedious. It's often more useful to simulate the\n",
    "policy:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations = SDDP.simulate(\n",
    "    model,\n",
    "    10,  #= number of replications =#\n",
    "    [:x, :u_sell, :u_make];  #= variables to record =#\n",
    "    skip_undefined_variables = true,\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`simulations` is a vector with 10 elements"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "length(simulations)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "and each element is a vector with two elements (one for each stage)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "length(simulations[1])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first stage contains:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations[1][1]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The second stage contains:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations[1][2]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can compute aggregated statistics across the simulations:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "objectives = map(simulations) do simulation\n",
    "    return sum(data[:stage_objective] for data in simulation)\n",
    "end\n",
    "μ, t = SDDP.confidence_interval(objectives)\n",
    "println(\"Simulation ci : $μ ± $t\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Risk aversion revisited"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "SDDP.jl contains a number of risk measures. One example is:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "0.5 * SDDP.Expectation() + 0.5 * SDDP.WorstCase()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can construct a risk-averse policy by passing a risk measure to the\n",
    "`risk_measure` keyword argument of `SDDP.train`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can explore how the optimal decision changes with risk by creating a\n",
    "function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function solve_newsvendor(risk_measure::SDDP.AbstractRiskMeasure)\n",
    "    model = SDDP.LinearPolicyGraph(\n",
    "        stages = 2,\n",
    "        sense = :Max,\n",
    "        upper_bound = 5 * maximum(d),\n",
    "        optimizer = HiGHS.Optimizer,\n",
    "    ) do subproblem, node\n",
    "        @variable(subproblem, x >= 0, SDDP.State, initial_value = 0)\n",
    "        if node == 1\n",
    "            @stageobjective(subproblem, -2 * x.out)\n",
    "        else\n",
    "            @variable(subproblem, u_sell >= 0)\n",
    "            @constraint(subproblem, u_sell <= x.in)\n",
    "            @constraint(subproblem, x.out == x.in - u_sell)\n",
    "            SDDP.parameterize(subproblem, d, P) do ω\n",
    "                set_upper_bound(u_sell, ω)\n",
    "                return\n",
    "            end\n",
    "            @stageobjective(subproblem, 5 * u_sell - 0.1 * x.out)\n",
    "        end\n",
    "        return\n",
    "    end\n",
    "    SDDP.train(model; risk_measure = risk_measure, print_level = 0)\n",
    "    first_stage_rule = SDDP.DecisionRule(model; node = 1)\n",
    "    solution = SDDP.evaluate(first_stage_rule; incoming_state = Dict(:x => 0.0))\n",
    "    return solution.outgoing_state[:x]\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can see how many units a decision maker would order using `CVaR`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "solve_newsvendor(SDDP.CVaR(0.4))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "as well as a decision-maker who cares only about the worst-case outcome:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "solve_newsvendor(SDDP.WorstCase())"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In general, the decision-maker will be somewhere between the two extremes.\n",
    "The `SDDP.Entropic` risk measure is a risk measure that has a single\n",
    "parameter that lets us explore the space of policies between the two extremes.\n",
    "When the parameter is small, the measure acts like `SDDP.Expectation`,\n",
    "and when it is large, it acts like `SDDP.WorstCase`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is what we get if we solve our problem multiple times for different\n",
    "values of the risk aversion parameter $\\gamma$:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Γ = [10^i for i in -4:0.5:1]\n",
    "buy = [solve_newsvendor(SDDP.Entropic(γ)) for γ in Γ]\n",
    "Plots.plot(\n",
    "    Γ,\n",
    "    buy;\n",
    "    xaxis = :log,\n",
    "    xlabel = \"Risk aversion parameter γ\",\n",
    "    ylabel = \"Number of pies to make\",\n",
    "    legend = false,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Things to try"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are a number of things you can try next:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " * Experiment with different buy and sales prices\n",
    " * Experiment with different distributions of demand\n",
    " * Explore how the optimal policy changes if you use a different risk measure\n",
    " * What happens if you can only buy and sell integer numbers of newspapers?\n",
    "   Try this by adding `Int` to the variable definitions:\n",
    "   `@variable(subproblem, buy >= 0, Int)`\n",
    " * What happens if you use a different upper bound? Try an invalid one like\n",
    "   `-100`, and a very large one like `1e12`."
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "kernelspec": {
   "name": "julia-1.9",
   "display_name": "Julia 1.9.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
