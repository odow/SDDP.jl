<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Here-and-now and hazard-decision · SDDP.jl</title><meta name="title" content="Here-and-now and hazard-decision · SDDP.jl"/><meta property="og:title" content="Here-and-now and hazard-decision · SDDP.jl"/><meta property="twitter:title" content="Here-and-now and hazard-decision · SDDP.jl"/><meta name="description" content="Documentation for SDDP.jl."/><meta property="og:description" content="Documentation for SDDP.jl."/><meta property="twitter:description" content="Documentation for SDDP.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="SDDP.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">SDDP.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox" checked/><label class="tocitem" for="menuitem-2"><span class="docs-label">Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../first_steps/">An introduction to SDDP.jl</a></li><li><a class="tocitem" href="../objective_uncertainty/">Uncertainty in the objective function</a></li><li><a class="tocitem" href="../markov_uncertainty/">Markovian policy graphs</a></li><li><a class="tocitem" href="../plotting/">Plotting tools</a></li><li><a class="tocitem" href="../warnings/">Words of warning</a></li><li><a class="tocitem" href="../arma/">Auto-regressive stochastic processes</a></li><li class="is-active"><a class="tocitem" href>Here-and-now and hazard-decision</a><ul class="internal"><li><a class="tocitem" href="#Hazard-decision-formulation"><span>Hazard-decision formulation</span></a></li><li><a class="tocitem" href="#Decision-hazard-formulation"><span>Decision-hazard formulation</span></a></li><li><a class="tocitem" href="#Comparison"><span>Comparison</span></a></li><li><a class="tocitem" href="#Fixing-the-decision-hazard"><span>Fixing the decision-hazard</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li></ul></li><li><a class="tocitem" href="../objective_states/">Objective states</a></li><li><a class="tocitem" href="../pglib_opf/">Alternative forward models</a></li><li><a class="tocitem" href="../mdps/">Example: Markov Decision Processes</a></li><li><a class="tocitem" href="../example_newsvendor/">Example: two-stage newsvendor</a></li><li><a class="tocitem" href="../example_reservoir/">Example: deterministic to stochastic</a></li><li><a class="tocitem" href="../example_milk_producer/">Example: the milk producer</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">How-to guides</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../guides/access_previous_variables/">Access variables from a previous stage</a></li><li><a class="tocitem" href="../../guides/add_a_multidimensional_state_variable/">Add a multi-dimensional state variable</a></li><li><a class="tocitem" href="../../guides/add_a_risk_measure/">Add a risk measure</a></li><li><a class="tocitem" href="../../guides/add_integrality/">Integrality</a></li><li><a class="tocitem" href="../../guides/add_multidimensional_noise/">Add multi-dimensional noise terms</a></li><li><a class="tocitem" href="../../guides/add_noise_in_the_constraint_matrix/">Add noise in the constraint matrix</a></li><li><a class="tocitem" href="../../guides/choose_a_stopping_rule/">Choose a stopping rule</a></li><li><a class="tocitem" href="../../guides/create_a_general_policy_graph/">Create a general policy graph</a></li><li><a class="tocitem" href="../../guides/debug_a_model/">Debug a model</a></li><li><a class="tocitem" href="../../guides/improve_computational_performance/">Improve computational performance</a></li><li><a class="tocitem" href="../../guides/simulate_using_a_different_sampling_scheme/">Simulate using a different sampling scheme</a></li><li><a class="tocitem" href="../../guides/create_a_belief_state/">Create a belief state</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">Explanation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../explanation/theory_intro/">Introductory theory</a></li><li><a class="tocitem" href="../../explanation/risk/">Risk aversion</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/FAST_hydro_thermal/">FAST: the hydro-thermal problem</a></li><li><a class="tocitem" href="../../examples/FAST_production_management/">FAST: the production management problem</a></li><li><a class="tocitem" href="../../examples/FAST_quickstart/">FAST: the quickstart problem</a></li><li><a class="tocitem" href="../../examples/Hydro_thermal/">Hydro-thermal scheduling</a></li><li><a class="tocitem" href="../../examples/StochDynamicProgramming.jl_multistock/">StochDynamicProgramming: the multistock problem</a></li><li><a class="tocitem" href="../../examples/StochDynamicProgramming.jl_stock/">StochDynamicProgramming: the stock problem</a></li><li><a class="tocitem" href="../../examples/StructDualDynProg.jl_prob5.2_2stages/">StructDualDynProg: Problem 5.2, 2 stages</a></li><li><a class="tocitem" href="../../examples/StructDualDynProg.jl_prob5.2_3stages/">StructDualDynProg: Problem 5.2, 3 stages</a></li><li><a class="tocitem" href="../../examples/agriculture_mccardle_farm/">The farm planning problem</a></li><li><a class="tocitem" href="../../examples/air_conditioning/">Air conditioning</a></li><li><a class="tocitem" href="../../examples/air_conditioning_forward/">Training with a different forward model</a></li><li><a class="tocitem" href="../../examples/all_blacks/">Deterministic All Blacks</a></li><li><a class="tocitem" href="../../examples/asset_management_simple/">Asset management</a></li><li><a class="tocitem" href="../../examples/asset_management_stagewise/">Asset management with modifications</a></li><li><a class="tocitem" href="../../examples/belief/">Partially observable inventory management</a></li><li><a class="tocitem" href="../../examples/biobjective_hydro/">Biobjective hydro-thermal</a></li><li><a class="tocitem" href="../../examples/booking_management/">Booking management</a></li><li><a class="tocitem" href="../../examples/generation_expansion/">Generation expansion</a></li><li><a class="tocitem" href="../../examples/hydro_valley/">Hydro valleys</a></li><li><a class="tocitem" href="../../examples/infinite_horizon_hydro_thermal/">Infinite horizon hydro-thermal</a></li><li><a class="tocitem" href="../../examples/infinite_horizon_trivial/">Infinite horizon trivial</a></li><li><a class="tocitem" href="../../examples/no_strong_duality/">No strong duality</a></li><li><a class="tocitem" href="../../examples/objective_state_newsvendor/">Newsvendor</a></li><li><a class="tocitem" href="../../examples/sldp_example_one/">SLDP: example 1</a></li><li><a class="tocitem" href="../../examples/sldp_example_two/">SLDP: example 2</a></li><li><a class="tocitem" href="../../examples/stochastic_all_blacks/">Stochastic All Blacks</a></li><li><a class="tocitem" href="../../examples/the_farmers_problem/">The farmer&#39;s problem</a></li><li><a class="tocitem" href="../../examples/vehicle_location/">Vehicle location</a></li></ul></li><li><a class="tocitem" href="../../apireference/">API Reference</a></li><li><a class="tocitem" href="../../release_notes/">Release notes</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Here-and-now and hazard-decision</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Here-and-now and hazard-decision</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/odow/SDDP.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/odow/SDDP.jl/blob/master/docs/src/tutorial/decision_hazard.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Here-and-now-and-hazard-decision"><a class="docs-heading-anchor" href="#Here-and-now-and-hazard-decision">Here-and-now and hazard-decision</a><a id="Here-and-now-and-hazard-decision-1"></a><a class="docs-heading-anchor-permalink" href="#Here-and-now-and-hazard-decision" title="Permalink"></a></h1><p><em>This tutorial was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em> <a href="../decision_hazard.jl"><em>Download the source as a <code>.jl</code> file</em></a>. <a href="../decision_hazard.ipynb"><em>Download the source as a <code>.ipynb</code> file</em></a>.</p><p>SDDP.jl assumes that the agent gets to make a decision <em>after</em> observing the realization of the random variable. This is called a <em>wait-and-see</em> or <em>hazard-decision</em> model. In contrast, you might want your agent to make decisions <em>before</em> observing the random variable. This is called a <em>here-and-now</em> or <em>decision-hazard</em> model.</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>The terms decision-hazard and hazard-decision from the French <em>hasard</em>, meaning chance. It could also have been translated as uncertainty-decision and decision-uncertainty, but the community seems to have settled on the transliteration <em>hazard</em> instead. We like the hazard-decision and decision-hazard terms because they clearly communicate the order of the decision and the uncertainty.</p></div></div><p>The purpose of this tutorial is to demonstrate how to model here-and-how decisions in SDDP.jl.</p><p>This tutorial uses the following packages:</p><pre><code class="language-julia hljs">using SDDP
import HiGHS</code></pre><h2 id="Hazard-decision-formulation"><a class="docs-heading-anchor" href="#Hazard-decision-formulation">Hazard-decision formulation</a><a id="Hazard-decision-formulation-1"></a><a class="docs-heading-anchor-permalink" href="#Hazard-decision-formulation" title="Permalink"></a></h2><p>As an example, we&#39;re going to build a standard hydro-thermal scheduling model, with a single hydro-reservoir and a single thermal generation plant. In each of the four stages, we need to choose some mix of <code>u_thermal</code> and <code>u_hydro</code> to meet a demand of <code>9</code> units, where unmet demand is penalized at a rate of <span>$</span>500/unit.</p><pre><code class="language-julia hljs">hazard_decision = SDDP.LinearPolicyGraph(
    stages = 4,
    sense = :Min,
    lower_bound = 0.0,
    optimizer = HiGHS.Optimizer,
) do sp, node
    @variables(sp, begin
        0 &lt;= x_storage &lt;= 8, (SDDP.State, initial_value = 6)
        u_thermal &gt;= 0
        u_hydro &gt;= 0
        u_unmet_demand &gt;= 0
    end)
    @constraint(sp, u_thermal + u_hydro == 9 - u_unmet_demand)
    @constraint(sp, c_balance, x_storage.out == x_storage.in - u_hydro + 0)
    SDDP.parameterize(sp, [2, 3]) do ω_inflow
        return set_normalized_rhs(c_balance, ω_inflow)
    end
    @stageobjective(sp, 500 * u_unmet_demand + 20 * u_thermal)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">A policy graph with 4 nodes.
 Node indices: 1, 2, 3, 4
</code></pre><h2 id="Decision-hazard-formulation"><a class="docs-heading-anchor" href="#Decision-hazard-formulation">Decision-hazard formulation</a><a id="Decision-hazard-formulation-1"></a><a class="docs-heading-anchor-permalink" href="#Decision-hazard-formulation" title="Permalink"></a></h2><p>In the wait-and-see formulation, we get to decide the generation variables <em>after</em> observing the realization of <code>ω_inflow</code>. However, a common modeling situation is that we need to decide the level of thermal generation <code>u_thermal</code> <em>before</em> observing the inflow.</p><p>SDDP.jl can model here-and-now decisions with a modeling trick: a wait-and-see decision in stage <em>t-1</em> is equivalent to a here-and-now decision in stage <em>t</em>.</p><p>In other words, we need to convert the <code>u_thermal</code> decision from a control variable that is decided in stage <code>t</code>, to a state variable that is decided in stage <code>t-1</code>. Here&#39;s our new model, with the three lines that have changed:</p><pre><code class="language-julia hljs">decision_hazard = SDDP.LinearPolicyGraph(
    stages = 4,
    sense = :Min,
    lower_bound = 0.0,
    optimizer = HiGHS.Optimizer,
) do sp, node
    @variables(sp, begin
        0 &lt;= x_storage &lt;= 8, (SDDP.State, initial_value = 6)
        u_thermal &gt;= 0, (SDDP.State, initial_value = 0)  # &lt;-- changed
        u_hydro &gt;= 0
        u_unmet_demand &gt;= 0
    end)
    @constraint(sp, u_thermal.in + u_hydro == 9 - u_unmet_demand)  # &lt;-- changed
    @constraint(sp, c_balance, x_storage.out == x_storage.in - u_hydro + 0)
    SDDP.parameterize(sp, [2, 3]) do ω
        return set_normalized_rhs(c_balance, ω)
    end
    @stageobjective(sp, 500 * u_unmet_demand + 20 * u_thermal.in) # &lt;-- changed
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">A policy graph with 4 nodes.
 Node indices: 1, 2, 3, 4
</code></pre><p>Can you understand the reformulation? In each stage, we now use the value of <code>u_thermal.in</code> instead of <code>u_thermal</code>, and the value of the outgoing <code>u_thermal.out</code> is the here-and-how decision for stage <code>t+1</code>.</p><p>(If you can spot a &quot;mistake&quot; with this model, don&#39;t worry, we&#39;ll fix it below. Presenting it like this simplifies the exposition.)</p><h2 id="Comparison"><a class="docs-heading-anchor" href="#Comparison">Comparison</a><a id="Comparison-1"></a><a class="docs-heading-anchor-permalink" href="#Comparison" title="Permalink"></a></h2><p>Let&#39;s compare the cost of operating the two models:</p><pre><code class="language-julia hljs">function train_and_compute_cost(model)
    SDDP.train(model; print_level = 0)
    return println(&quot;Cost = \$&quot;, SDDP.calculate_bound(model))
end

train_and_compute_cost(hazard_decision)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Cost = $400.0</code></pre><pre><code class="language-julia hljs">train_and_compute_cost(decision_hazard)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Cost = $650.0</code></pre><p>This suggests that choosing the thermal generation before observing the inflow adds a cost of <span>$</span>250. But does this make sense?</p><p>If we look carefully at our <code>decision_hazard</code> model, the incoming value of <code>u_thermal.in</code> in the first stage is fixed to the <code>initial_value</code> of <code>0</code>. Therefore, we must always meet the full demand with <code>u_hydro</code>, which we cannot do without incurring unmet demand.</p><p>To allow the model to choose an optimal level of <code>u_thermal</code> in the first-stage, we need to add an extra stage that is deterministic with no stage objective.</p><h2 id="Fixing-the-decision-hazard"><a class="docs-heading-anchor" href="#Fixing-the-decision-hazard">Fixing the decision-hazard</a><a id="Fixing-the-decision-hazard-1"></a><a class="docs-heading-anchor-permalink" href="#Fixing-the-decision-hazard" title="Permalink"></a></h2><p>In the following model, we now have five stages, so that stage <code>t+1</code> in <code>decision_hazard_2</code> corresponds to stage <code>t</code> in <code>decision_hazard</code>. We&#39;ve also added an <code>if</code>-statement, which adds different constraints depending on the node. Note that we need to add an <code>x_storage.out == x_storage.in</code> constraint because the storage can&#39;t change in this new first-stage.</p><pre><code class="language-julia hljs">decision_hazard_2 = SDDP.LinearPolicyGraph(
    stages = 5,  # &lt;-- changed
    sense = :Min,
    lower_bound = 0.0,
    optimizer = HiGHS.Optimizer,
) do sp, node
    @variables(sp, begin
        0 &lt;= x_storage &lt;= 8, (SDDP.State, initial_value = 6)
        u_thermal &gt;= 0, (SDDP.State, initial_value = 0)
        u_hydro &gt;= 0
        u_unmet_demand &gt;= 0
    end)
    if node == 1                                        # &lt;-- new
        @constraint(sp, x_storage.out == x_storage.in)  # &lt;-- new
        @stageobjective(sp, 0)                          # &lt;-- new
    else
        @constraint(sp, u_thermal.in + u_hydro == 9 - u_unmet_demand)
        @constraint(sp, c_balance, x_storage.out == x_storage.in - u_hydro + 0)
        SDDP.parameterize(sp, [2, 3]) do ω
            return set_normalized_rhs(c_balance, ω)
        end
        @stageobjective(sp, 500 * u_unmet_demand + 20 * u_thermal.in)
    end
end

train_and_compute_cost(decision_hazard_2)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Cost = $410.0</code></pre><p>Now we find that the cost of choosing the thermal generation before observing the inflow adds a much more reasonable cost of <span>$</span>10.</p><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>To summarize, the difference between here-and-now and wait-and-see variables is a modeling choice.</p><p>To create a here-and-now decision, add it as a state variable to the previous stage</p><p>In some cases, you&#39;ll need to add an additional &quot;first-stage&quot; problem to enable the model to choose an optimal value for the here-and-now decision variable. You do not need to do this if the first stage is deterministic. You must make sure that the subproblem is feasible for all possible incoming values of the here-and-now decision variable.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../arma/">« Auto-regressive stochastic processes</a><a class="docs-footer-nextpage" href="../objective_states/">Objective states »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.0.1 on <span class="colophon-date" title="Monday 25 September 2023 07:08">Monday 25 September 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
