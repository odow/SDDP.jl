{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Example: Two-stage Newsvendor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The purpose of this tutorial is to demonstrate how to model and solve a\n",
    "two-stage stochastic program."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is based on the [Two stage stochastic programs](https://jump.dev/JuMP.jl/dev/tutorials/applications/two_stage_stochastic/)\n",
    "tutorial in JuMP."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This tutorial uses the following packages"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using JuMP\n",
    "using SDDP\n",
    "import Distributions\n",
    "import HiGHS\n",
    "import Plots\n",
    "import StatsPlots\n",
    "import Statistics"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Background"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data for this problem is:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "D = Distributions.TriangularDist(150.0, 250.0, 200.0)\n",
    "N = 100\n",
    "d = sort!(rand(D, N));\n",
    "Ω = 1:N\n",
    "P = fill(1 / N, N);\n",
    "StatsPlots.histogram(d; bins = 20, label = \"\", xlabel = \"Demand\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Policy Graph"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can formulate and train a policy for the two-stage newsvendor problem."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we need to construct the graph:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "graph = SDDP.LinearGraph(2)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we need to write a function which builds a JuMP model for each node in\n",
    "the graph:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function build_subproblem(subproblem::JuMP.Model, stage::Int)\n",
    "    @variable(subproblem, x >= 0, SDDP.State, initial_value = 0)\n",
    "    if stage == 1\n",
    "        @variable(subproblem, u_make >= 0)\n",
    "        @constraint(subproblem, x.out == x.in + u_make)\n",
    "        @stageobjective(subproblem, -2 * u_make)\n",
    "    else\n",
    "        @variable(subproblem, u_sell >= 0)\n",
    "        @constraint(subproblem, u_sell <= x.in)\n",
    "        @constraint(subproblem, x.out == x.in - u_sell)\n",
    "        SDDP.parameterize(subproblem, d, P) do ω\n",
    "            set_upper_bound(u_sell, ω)\n",
    "            return\n",
    "        end\n",
    "        @stageobjective(subproblem, 5 * u_sell - 0.1 * x.out)\n",
    "    end\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we can combine the graph and the subproblem builder into a policy graph:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.PolicyGraph(\n",
    "    build_subproblem,\n",
    "    graph;\n",
    "    sense = :Max,\n",
    "    upper_bound = 5 * maximum(d), #= some large upper bound =#\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use `SDDP.train` to construct the policy:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "SDDP.train(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To check the first-stage buy decision, we need to obtain a decision rule for\n",
    "the first-stage node `1`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "first_stage_rule = SDDP.DecisionRule(model, node = 1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we can evaluate it, passing in a starting point for the incoming state:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "solution = SDDP.evaluate(first_stage_rule; incoming_state = Dict(:x => 0.0))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The optimal value of the state variable is stored here:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "solution.outgoing_state[:x]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can simplify the model construction by using `SDDP.LinearPolicyGraph`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.LinearPolicyGraph(\n",
    "    build_subproblem;\n",
    "    stages = 2,\n",
    "    sense = :Max,\n",
    "    upper_bound = 5 * maximum(d),\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "and we can use Julia's `do` syntax to avoid writing a separate function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.LinearPolicyGraph(;\n",
    "    stages = 2,\n",
    "    sense = :Max,\n",
    "    upper_bound = 5 * maximum(d),\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ") do subproblem::JuMP.Model, stage::Int\n",
    "    @variable(subproblem, x >= 0, SDDP.State, initial_value = 0)\n",
    "    if stage == 1\n",
    "        @variable(subproblem, u_make >= 0)\n",
    "        @constraint(subproblem, x.out == x.in + u_make)\n",
    "        @stageobjective(subproblem, -2 * u_make)\n",
    "    else\n",
    "        @variable(subproblem, u_sell >= 0)\n",
    "        @constraint(subproblem, u_sell <= x.in)\n",
    "        @constraint(subproblem, x.out == x.in - u_sell)\n",
    "        SDDP.parameterize(subproblem, d, P) do ω\n",
    "            set_upper_bound(u_sell, ω)\n",
    "            return\n",
    "        end\n",
    "        @stageobjective(subproblem, 5 * u_sell - 0.1 * x.out)\n",
    "    end\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Querying the decision rules is tedious. It's often more useful to simulate the\n",
    "policy:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "SDDP.train(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations = SDDP.simulate(\n",
    "    model,\n",
    "    10,  #= number of replications =#\n",
    "    [:x, :u_sell, :u_make];  #= variables to record =#\n",
    "    skip_undefined_variables = true,\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`simulations` is a vector with 10 elements"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "length(simulations)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "and each element is a vector with two elements (one for each stage)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "length(simulations[1])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first stage contains:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations[1][1]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The second stage contains:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations[1][2]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can compute aggregated statistics across the simulations:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "objectives = map(simulations) do simulation\n",
    "    return sum(data[:stage_objective] for data in simulation)\n",
    "end\n",
    "μ, t = SDDP.confidence_interval(objectives)\n",
    "println(\"Simulation ci : $μ ± $t\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Risk aversion revisited"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "SDDP.jl contains a number of risk measures. One example is:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "0.5 * SDDP.Expectation() + 0.5 * SDDP.WorstCase()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can construct a risk-averse policy by passing a risk measure to the\n",
    "`risk_measure` keyword argument of `SDDP.train`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can explore how the optimal decision changes with risk by creating a\n",
    "function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function solve_newsvendor(risk_measure::SDDP.AbstractRiskMeasure)\n",
    "    model = SDDP.LinearPolicyGraph(\n",
    "        stages = 2,\n",
    "        sense = :Max,\n",
    "        upper_bound = 5 * maximum(d),\n",
    "        optimizer = HiGHS.Optimizer,\n",
    "    ) do subproblem, node\n",
    "        @variable(subproblem, x >= 0, SDDP.State, initial_value = 0)\n",
    "        if node == 1\n",
    "            @stageobjective(subproblem, -2 * x.out)\n",
    "        else\n",
    "            @variable(subproblem, u_sell >= 0)\n",
    "            @constraint(subproblem, u_sell <= x.in)\n",
    "            @constraint(subproblem, x.out == x.in - u_sell)\n",
    "            SDDP.parameterize(subproblem, d, P) do ω\n",
    "                set_upper_bound(u_sell, ω)\n",
    "                return\n",
    "            end\n",
    "            @stageobjective(subproblem, 5 * u_sell - 0.1 * x.out)\n",
    "        end\n",
    "        return\n",
    "    end\n",
    "    SDDP.train(model; risk_measure = risk_measure, print_level = 0)\n",
    "    first_stage_rule = SDDP.DecisionRule(model; node = 1)\n",
    "    solution = SDDP.evaluate(first_stage_rule; incoming_state = Dict(:x => 0.0))\n",
    "    return solution.outgoing_state[:x]\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can see how many units a decision maker would order using `CVaR`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "solve_newsvendor(SDDP.CVaR(0.4))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "as well as a decision-maker who cares only about the worst-case outcome:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "solve_newsvendor(SDDP.WorstCase())"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In general, the decision-maker will be somewhere between the two extremes.\n",
    "The `SDDP.Entropic` risk measure is a risk measure that has a single\n",
    "parameter that lets us explore the space of policies between the two extremes.\n",
    "When the parameter is small, the measure acts like `SDDP.Expectation`,\n",
    "and when it is large, it acts like `SDDP.WorstCase`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is what we get if we solve our problem multiple times for different\n",
    "values of the risk aversion parameter $\\gamma$:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Γ = [10^i for i in -4:0.5:1]\n",
    "buy = [solve_newsvendor(SDDP.Entropic(γ)) for γ in Γ]\n",
    "Plots.plot(\n",
    "    Γ,\n",
    "    buy;\n",
    "    xaxis = :log,\n",
    "    xlabel = \"Risk aversion parameter γ\",\n",
    "    ylabel = \"Number of pies to make\",\n",
    "    legend = false,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Things to try"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are a number of things you can try next:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " * Experiment with different buy and sales prices\n",
    " * Experiment with different distributions of demand\n",
    " * Explore how the optimal policy changes if you use a different risk measure\n",
    " * What happens if you can only buy and sell integer numbers of newspapers?\n",
    "   Try this by adding `Int` to the variable definitions:\n",
    "   `@variable(subproblem, buy >= 0, Int)`\n",
    " * What happens if you use a different upper bound? Try an invalid one like\n",
    "   `-100`, and a very large one like `1e12`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Other graphs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The two-stage newsvendor problem can be extended to other graphs. For example,\n",
    "here is a three-stage graph:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "graph = SDDP.Graph(:root)\n",
    "SDDP.add_node(graph, :make_only)\n",
    "SDDP.add_node(graph, :sell_only)\n",
    "SDDP.add_node(graph, :make_and_sell)\n",
    "SDDP.add_edge(graph, :root => :make_only, 1.0)\n",
    "SDDP.add_edge(graph, :make_only => :make_and_sell, 1.0)\n",
    "SDDP.add_edge(graph, :make_and_sell => :sell_only, 1.0)\n",
    "graph"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "with the model formulation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.PolicyGraph(\n",
    "    graph;\n",
    "    sense = :Max,\n",
    "    upper_bound = 2 * 5 * maximum(d),\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ") do subproblem::JuMP.Model, node::Symbol\n",
    "    @variable(subproblem, x >= 0, SDDP.State, initial_value = 0)\n",
    "    if node == :make_only\n",
    "        @variable(subproblem, u_make >= 0)\n",
    "        @constraint(subproblem, x.out == x.in + u_make)\n",
    "        @stageobjective(subproblem, -2 * u_make)\n",
    "    elseif node == :sell_only\n",
    "        @variable(subproblem, u_sell >= 0)\n",
    "        @constraint(subproblem, u_sell <= x.in)\n",
    "        @constraint(subproblem, x.out == x.in - u_sell)\n",
    "        SDDP.parameterize(subproblem, d, P) do ω\n",
    "            set_upper_bound(u_sell, ω)\n",
    "            return\n",
    "        end\n",
    "        @stageobjective(subproblem, 5 * u_sell - 0.1 * x.out)\n",
    "    else\n",
    "        @assert node == :make_and_sell\n",
    "        @variable(subproblem, u_sell >= 0)\n",
    "        @variable(subproblem, u_make >= 0)\n",
    "        @constraint(subproblem, u_sell <= x.in)\n",
    "        @constraint(subproblem, x.out == x.in - u_sell + u_make)\n",
    "        SDDP.parameterize(subproblem, d, P) do ω\n",
    "            set_upper_bound(u_sell, ω)\n",
    "            return\n",
    "        end\n",
    "        @stageobjective(subproblem, 5 * u_sell - 0.1 * x.out - 2 * u_make)\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "SDDP.train(model)\n",
    "simulations = SDDP.simulate(\n",
    "    model,\n",
    "    10,  #= number of replications =#\n",
    "    [:x, :u_sell, :u_make];  #= variables to record =#\n",
    "    skip_undefined_variables = true,\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations[1][1]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations[1][2]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations[1][3]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Try changing the graph. What happens if you add a cycle?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "kernelspec": {
   "name": "julia-1.9",
   "display_name": "Julia 1.9.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
