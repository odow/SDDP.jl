{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Objective states"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are many applications in which we want to model a price process that\n",
    "follows some auto-regressive process. Common examples include stock prices on\n",
    "financial exchanges and spot-prices in energy markets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, it is well known that these cannot be incorporated in to SDDP because\n",
    "they result in cost-to-go functions that are convex with respect to some state\n",
    "variables (e.g., the reservoir levels) and concave with respect to other state\n",
    "variables (e.g., the spot price in the current stage)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To overcome this problem, the approach in the literature has been to\n",
    "discretize the price process in order to model it using a Markovian policy\n",
    "graph like those discussed in Markovian policy graphs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, recent work offers a way to include stagewise-dependent objective\n",
    "uncertainty into the objective function of SDDP subproblems. Readers are\n",
    "directed to the following works for an introduction:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " - Downward, A., Dowson, O., and Baucke, R. (2017). Stochastic dual dynamic\n",
    "   programming with stagewise dependent objective uncertainty. Optimization\n",
    "   Online. [link](http://www.optimization-online.org/DB_HTML/2018/02/6454.html)\n",
    "\n",
    " - Dowson, O. PhD Thesis. University of Auckland, 2018. [link](https://researchspace.auckland.ac.nz/handle/2292/37700)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The method discussed in the above works introduces the concept of an\n",
    "_objective state_ into SDDP. Unlike normal state variables in SDDP (e.g., the\n",
    "volume of water in the reservoir), the cost-to-go function is _concave_ with\n",
    "respect to the objective states. Thus, the method builds an outer\n",
    "approximation of the cost-to-go function in the normal state-space, and an\n",
    "inner approximation of the cost-to-go function in the objective state-space."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "!!! warning\n",
    "    Support for objective states in `SDDP.jl` is experimental. Models are\n",
    "    considerably more computational intensive, the interface is less\n",
    "    user-friendly, and there are subtle gotchas to be aware of.\n",
    "    Only use this if you have read and understood the theory behind the method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## One-dimensional objective states"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's assume that the fuel cost is not fixed, but instead evolves according to\n",
    "a multiplicative auto-regressive process: `fuel_cost[t] = ω * fuel_cost[t-1]`,\n",
    "where `ω` is drawn from the sample space `[0.75, 0.9, 1.1, 1.25]` with equal\n",
    "probability."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "An objective state can be added to a subproblem using the\n",
    "`SDDP.add_objective_state` function. This can only be called once per\n",
    "subproblem. If you want to add a multi-dimensional objective state, read\n",
    "Multi-dimensional objective states. `SDDP.add_objective_state`\n",
    "takes a number of keyword arguments. The two required ones are"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " - `initial_value`: the value of the objective state at the root node of the\n",
    "   policy graph (i.e., identical to the `initial_value` when defining normal\n",
    "   state variables.\n",
    "\n",
    " - `lipschitz`: the Lipschitz constant of the cost-to-go function with respect\n",
    "   to the objective state. In other words, this value is the maximum change in\n",
    "   the cost-to-go function _at any point in the state space_, given a one-unit\n",
    "   change in the objective state."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are also two optional keyword arguments: `lower_bound` and\n",
    "`upper_bound`, which give SDDP.jl hints (importantly, not constraints) about\n",
    "the domain of the objective state. Setting these bounds appropriately can\n",
    "improve the speed of convergence."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, `SDDP.add_objective_state` requires an update function. This\n",
    "function takes two arguments. The first is the incoming value of the objective\n",
    "state, and the second is the realization of the stagewise-independent noise\n",
    "term (set using `SDDP.parameterize`). The function should return the\n",
    "value of the objective state to be used in the current subproblem."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This connection with the stagewise-independent noise term means that\n",
    "`SDDP.parameterize` _must_ be called in a subproblem that defines an\n",
    "objective state. Inside `SDDP.parameterize`, the value of the\n",
    "objective state to be used in the current subproblem (i.e., after the update\n",
    "function), can be queried using `SDDP.objective_state`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the full model with the objective state."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using SDDP, HiGHS\n",
    "\n",
    "model = SDDP.LinearPolicyGraph(\n",
    "    stages = 3,\n",
    "    sense = :Min,\n",
    "    lower_bound = 0.0,\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ") do subproblem, t\n",
    "    @variable(subproblem, 0 <= volume <= 200, SDDP.State, initial_value = 200)\n",
    "    @variables(subproblem, begin\n",
    "        thermal_generation >= 0\n",
    "        hydro_generation >= 0\n",
    "        hydro_spill >= 0\n",
    "        inflow\n",
    "    end)\n",
    "    @constraints(\n",
    "        subproblem,\n",
    "        begin\n",
    "            volume.out == volume.in + inflow - hydro_generation - hydro_spill\n",
    "            demand_constraint, thermal_generation + hydro_generation == 150.0\n",
    "        end\n",
    "    )\n",
    "\n",
    "    # Add an objective state. ω will be the same value that is called in\n",
    "    # `SDDP.parameterize`.\n",
    "\n",
    "    SDDP.add_objective_state(\n",
    "        subproblem,\n",
    "        initial_value = 50.0,\n",
    "        lipschitz = 10_000.0,\n",
    "        lower_bound = 50.0,\n",
    "        upper_bound = 150.0,\n",
    "    ) do fuel_cost, ω\n",
    "        return ω.fuel * fuel_cost\n",
    "    end\n",
    "\n",
    "    # Create the cartesian product of a multi-dimensional random variable.\n",
    "\n",
    "    Ω = [\n",
    "        (fuel = f, inflow = w) for f in [0.75, 0.9, 1.1, 1.25] for\n",
    "        w in [0.0, 50.0, 100.0]\n",
    "    ]\n",
    "\n",
    "    SDDP.parameterize(subproblem, Ω) do ω\n",
    "        # Query the current fuel cost.\n",
    "        fuel_cost = SDDP.objective_state(subproblem)\n",
    "        @stageobjective(subproblem, fuel_cost * thermal_generation)\n",
    "        return JuMP.fix(inflow, ω.inflow)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "After creating our model, we can train and simulate as usual."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "SDDP.train(model; run_numerical_stability_report = false)\n",
    "\n",
    "simulations = SDDP.simulate(model, 1)\n",
    "\n",
    "print(\"Finished training and simulating.\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To demonstrate how the objective states are updated, consider the sequence of\n",
    "noise observations:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "[stage[:noise_term] for stage in simulations[1]]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This, the fuel cost in the first stage should be `0.75 * 50 = 37.5`. The fuel\n",
    "cost in the second stage should be `1.1 * 37.5 = 41.25`. The fuel cost in the\n",
    "third stage should be `0.75 * 41.25 = 30.9375`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To confirm this, the values of the objective state in a simulation can be\n",
    "queried using the `:objective_state` key."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "[stage[:objective_state] for stage in simulations[1]]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-dimensional objective states"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can construct multi-dimensional price processes using `NTuple`s. Just\n",
    "replace every scalar value associated with the objective state by a tuple. For\n",
    "example, `initial_value = 1.0` becomes `initial_value = (1.0, 2.0)`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is an example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.LinearPolicyGraph(\n",
    "    stages = 3,\n",
    "    sense = :Min,\n",
    "    lower_bound = 0.0,\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ") do subproblem, t\n",
    "    @variable(subproblem, 0 <= volume <= 200, SDDP.State, initial_value = 200)\n",
    "    @variables(subproblem, begin\n",
    "        thermal_generation >= 0\n",
    "        hydro_generation >= 0\n",
    "        hydro_spill >= 0\n",
    "        inflow\n",
    "    end)\n",
    "    @constraints(\n",
    "        subproblem,\n",
    "        begin\n",
    "            volume.out == volume.in + inflow - hydro_generation - hydro_spill\n",
    "            demand_constraint, thermal_generation + hydro_generation == 150.0\n",
    "        end\n",
    "    )\n",
    "\n",
    "    SDDP.add_objective_state(\n",
    "        subproblem,\n",
    "        initial_value = (50.0, 50.0),\n",
    "        lipschitz = (10_000.0, 10_000.0),\n",
    "        lower_bound = (50.0, 50.0),\n",
    "        upper_bound = (150.0, 150.0),\n",
    "    ) do fuel_cost, ω\n",
    "        # fuel_cost is a tuple, containing the (fuel_cost[t-1], fuel_cost[t-2])\n",
    "        # This function returns a new tuple containing\n",
    "        # (fuel_cost[t], fuel_cost[t-1]). Thus, we need to compute the new\n",
    "        # cost:\n",
    "        new_cost = fuel_cost[1] + 0.5 * (fuel_cost[1] - fuel_cost[2]) + ω.fuel\n",
    "        # And then return the appropriate tuple:\n",
    "        return (new_cost, fuel_cost[1])\n",
    "    end\n",
    "\n",
    "    Ω = [\n",
    "        (fuel = f, inflow = w) for f in [-10.0, -5.0, 5.0, 10.0] for\n",
    "        w in [0.0, 50.0, 100.0]\n",
    "    ]\n",
    "\n",
    "    SDDP.parameterize(subproblem, Ω) do ω\n",
    "        fuel_cost, _ = SDDP.objective_state(subproblem)\n",
    "        @stageobjective(subproblem, fuel_cost * thermal_generation)\n",
    "        return JuMP.fix(inflow, ω.inflow)\n",
    "    end\n",
    "end\n",
    "\n",
    "SDDP.train(model; run_numerical_stability_report = false)\n",
    "\n",
    "simulations = SDDP.simulate(model, 1)\n",
    "\n",
    "print(\"Finished training and simulating.\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This time, since our objective state is two-dimensional, the objective states\n",
    "are tuples with two elements:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "[stage[:objective_state] for stage in simulations[1]]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Warnings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are number of things to be aware of when using objective states.\n",
    "\n",
    " - The key assumption is that price is independent of the states and actions in\n",
    "   the model.\n",
    "\n",
    "   That means that the price cannot appear in any `@constraint`s. Nor can you\n",
    "   use any `@variable`s in the update function.\n",
    "\n",
    " - Choosing an appropriate Lipschitz constant is difficult.\n",
    "\n",
    "   The points discussed in Choosing an initial bound are relevant.\n",
    "   The Lipschitz constant should not be chosen as large as possible (since\n",
    "   this will help with convergence and the numerical issues discussed above),\n",
    "   but if chosen to small, it may cut of the feasible region and lead to a\n",
    "   sub-optimal solution.\n",
    "\n",
    " - You need to ensure that the cost-to-go function is concave with respect to\n",
    "   the objective state _before_ the update.\n",
    "\n",
    "   If the update function is linear, this is always the case. In some\n",
    "   situations, the update function can be nonlinear (e.g., multiplicative as\n",
    "   we have above). In general, placing constraints on the price (e.g.,\n",
    "   `clamp(price, 0, 1)`) will destroy concavity. [Caveat\n",
    "   emptor](https://en.wikipedia.org/wiki/Caveat_emptor). It's up to you if\n",
    "   this is a problem. If it isn't you'll get a good heuristic with no\n",
    "   guarantee of global optimality."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "kernelspec": {
   "name": "julia-1.9",
   "display_name": "Julia 1.9.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
