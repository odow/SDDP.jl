{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Hydro-thermal scheduling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem Description"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In a hydro-thermal problem, the agent controls a hydro-electric generator and reservoir.\n",
    "Each time period, they need to choose a generation quantity from thermal `g_t`, and hydro\n",
    "`g_h`, in order to meet demand `w_d`, which is a stagewise-independent random variable.\n",
    "The state variable, `x`, is the quantity of water in the reservoir at the start of each\n",
    "time period, and it has a minimum level of 5 units and a maximum level of 15 units. We\n",
    "assume that there are 10 units of water in the reservoir at the start of time, so that\n",
    "`x_0 = 10`. The state-variable is connected through time by the water balance constraint:\n",
    "`x.out = x.in - g_h - s + w_i,` where `x.out` is the quantity of water at the end of the\n",
    "time period, `x.in` is the quantity of water at the start of the time period, `s` is the\n",
    "quantity of water spilled from the reservoir, and `w_i` is a stagewise-independent random\n",
    "variable that represents the inflow into the reservoir during the time period."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We assume that there are three stages, `t=1, 2, 3`, representing summer-fall, winter, and\n",
    "spring, and that we are solving this problem in an infinite-horizon setting with a\n",
    "discount factor of `0.95`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In each stage, the agent incurs the cost of spillage, plus the cost of thermal generation.\n",
    "We assume that the cost of thermal generation is dependent on the stage `t = 1, 2, 3`, and\n",
    "that in each stage, `w` is drawn from the set `(w_i, w_d) = {(0, 7.5), (3, 5), (10, 2.5)}`\n",
    "with equal probability."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this example, in addition to `SDDP`, we need `HiGHS` as a solver and `Statisitics` to\n",
    "compute the mean of our simulations."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using HiGHS\n",
    "using SDDP\n",
    "using Statistics"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constructing the policy graph"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are three stages in our infinite-horizon problem, so we construct a\n",
    "unicyclic policy graph using `SDDP.UnicyclicGraph`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "graph = SDDP.UnicyclicGraph(0.95; num_nodes = 3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constructing the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Much of the macro code (i.e., lines starting with `@`) in the first part of the following\n",
    "should be familiar to users of JuMP."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inside the `do-end` block, `sp` is a standard JuMP model, and `t` is an index\n",
    "for the state variable that will be called with `t = 1, 2, 3`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The state variable `x`, constructed by passing the `SDDP.State` tag to `@variable` is\n",
    "actually a Julia struct with two fields: `x.in` and `x.out` corresponding to the incoming\n",
    "and outgoing state variables respectively. Both `x.in` and `x.out` are standard JuMP\n",
    "variables. The `initial_value` keyword provides the value of the state variable in the\n",
    "root node (i.e., `x_0`)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compared to a JuMP model, one key difference is that we use `@stageobjective`\n",
    "instead of `@objective`. The `SDDP.parameterize` function takes a list of supports\n",
    "for `w` and parameterizes the JuMP model `sp` by setting the right-hand sides of the\n",
    "appropriate constraints (note how the constraints initially have a right-hand side of\n",
    "`0`). By default, it is assumed that the realizations have uniform probability, but a\n",
    "probability mass vector can also be provided."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.PolicyGraph(\n",
    "    graph,\n",
    "    sense = :Min,\n",
    "    lower_bound = 0.0,\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ") do sp, t\n",
    "    @variable(sp, 5 <= x <= 15, SDDP.State, initial_value = 10)\n",
    "    @variable(sp, g_t >= 0)\n",
    "    @variable(sp, g_h >= 0)\n",
    "    @variable(sp, s >= 0)\n",
    "    @constraint(sp, balance, x.out - x.in + g_h + s == 0)\n",
    "    @constraint(sp, demand, g_h + g_t == 0)\n",
    "    @stageobjective(sp, s + t * g_t)\n",
    "    SDDP.parameterize(sp, [[0, 7.5], [3, 5], [10, 2.5]]) do w\n",
    "        set_normalized_rhs(balance, w[1])\n",
    "        return set_normalized_rhs(demand, w[2])\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the policy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once a model has been constructed, the next step is to train the policy. This can be\n",
    "achieved using `SDDP.train`. There are many options that can be passed, but\n",
    "`iteration_limit` terminates the training after the prescribed number of SDDP iterations."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "SDDP.train(model, iteration_limit = 100)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulating the policy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After training, we can simulate the policy using `SDDP.simulate`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "sims = SDDP.simulate(model, 100, [:g_t])\n",
    "mu = round(mean([s[1][:g_t] for s in sims]), digits = 2)\n",
    "println(\"On average, $(mu) units of thermal are used in the first stage.\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extracting the water values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can use `SDDP.ValueFunction` and `SDDP.evaluate` to obtain and\n",
    "evaluate the value function at different points in the state-space. Note that since we\n",
    "are minimizing, the price has a negative sign: each additional unit of water leads to a\n",
    "decrease in the expected long-run cost."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "V = SDDP.ValueFunction(model[1])\n",
    "cost, price = SDDP.evaluate(V, x = 10)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  },
  "kernelspec": {
   "name": "julia-1.9",
   "display_name": "Julia 1.9.2",
   "language": "julia"
  }
 },
 "nbformat": 4
}
