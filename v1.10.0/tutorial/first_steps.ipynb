{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# An introduction to SDDP.jl"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "SDDP.jl is a solver for multistage stochastic optimization problems. By\n",
    "**multistage**, we mean problems in which an agent makes a sequence of\n",
    "decisions over time. By **stochastic**, we mean that the agent is making\n",
    "decisions in the presence of uncertainty that is gradually revealed over the\n",
    "multiple stages."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Tip**\n",
    ">\n",
    "> Multistage stochastic programming has a lot in common with fields like\n",
    "> stochastic optimal control, approximate dynamic programming, Markov\n",
    "> decision processes, and reinforcement learning. If it helps, you can think\n",
    "> of SDDP as Q-learning in which we approximate the value function using\n",
    "> linear programming duality."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This tutorial is in two parts. First, it is an introduction to the background\n",
    "notation and theory we need, and second, it solves a simple multistage\n",
    "stochastic programming problem."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is a node?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A common feature of multistage stochastic optimization problems is that they\n",
    "model an agent controlling a system over time. To simplify things initially,\n",
    "we're going to start by describing what happens at an instant in time at which\n",
    "the agent makes a decision. Only after this will we extend our problem to\n",
    "multiple stages and the notion of time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A **node** is a place at which the agent makes a decision."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Tip**\n",
    ">\n",
    "> For readers with a stochastic programming background, \"node\" is synonymous\n",
    "> with \"stage\" in this section. However, for reasons that will become clear\n",
    "> shortly, there can be more than one \"node\" per instant in time, which is\n",
    "> why we prefer the term \"node\" over \"stage.\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### States, controls, and random variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The system that we are modeling can be described by three types of variables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. **State** variables track a property of the system over time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "   Each node has an associated _incoming_ state variable (the value of the\n",
    "   state at the start of the node), and an _outgoing_ state variable (the\n",
    "   value of the state at the end of the node).\n",
    "\n",
    "   Examples of state variables include the volume of water in a reservoir, the\n",
    "   number of units of inventory in a warehouse, or the spatial position of a\n",
    "   moving vehicle.\n",
    "\n",
    "   Because state variables track the system over time, each node must have the\n",
    "   same set of state variables.\n",
    "\n",
    "   We denote state variables by the letter $x$ for the incoming state variable\n",
    "   and $x^\\prime$ for the outgoing state variable.\n",
    "\n",
    "2. **Control** variables are actions taken (implicitly or explicitly) by the\n",
    "   agent within a node which modify the state variables.\n",
    "\n",
    "   Examples of control variables include releases of water from the reservoir,\n",
    "   sales or purchasing decisions, and acceleration or braking of the vehicle.\n",
    "\n",
    "   Control variables are local to a node $i$, and they can differ between\n",
    "   nodes. For example, some control variables may be available within certain\n",
    "   nodes.\n",
    "\n",
    "   We denote control variables by the letter $u$.\n",
    "\n",
    "3. **Random** variables are finite, discrete, exogenous random variables that\n",
    "   the agent observes at the start of a node, before the control variables are\n",
    "   decided.\n",
    "\n",
    "   Examples of random variables include rainfall inflow into a reservoir,\n",
    "   probabilistic perishing of inventory, and steering errors in a vehicle.\n",
    "\n",
    "   Random variables are local to a node $i$, and they can differ between\n",
    "   nodes. For example, some nodes may have random variables, and some nodes\n",
    "   may not.\n",
    "\n",
    "   We denote random variables by the Greek letter $\\omega$ and the sample\n",
    "   space from which they are drawn by $\\Omega_i$. The probability of sampling\n",
    "   $\\omega$ is denoted $p_{\\omega}$ for simplicity.\n",
    "\n",
    "   Importantly, the random variable associated with node $i$ is independent of\n",
    "   the random variables in all other nodes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dynamics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In a node $i$, the three variables are related by a **transition function**,\n",
    "which maps the incoming state, the controls, and the random variables to the\n",
    "outgoing state as follows: $x^\\prime = T_i(x, u, \\omega)$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a result of entering a node $i$ with the incoming state $x$, observing\n",
    "random variable $\\omega$, and choosing control $u$, the agent incurs a cost\n",
    "$C_i(x, u, \\omega)$. (If the agent is a maximizer, this can be a profit, or a\n",
    "negative cost.) We call $C_i$ the **stage objective**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To choose their control variables in node $i$, the agent uses a **decision**\n",
    "**rule** $u = \\pi_i(x, \\omega)$, which is a function that maps the incoming\n",
    "state variable and observation of the random variable to a control $u$. This\n",
    "control must satisfy some feasibility requirements $u \\in U_i(x, \\omega)$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is a schematic which we can use to visualize a single node:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Hazard-decision node](../assets/hazard_decision.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Policy graphs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have a node, we need to connect multiple nodes together to form a\n",
    "multistage stochastic program. We call the graph created by connecting nodes\n",
    "together a **policy graph**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The simplest type of policy graph is a **linear policy graph**. Here's a\n",
    "linear policy graph with three nodes:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Linear policy graph](../assets/stochastic_linear_policy_graph.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we have dropped the notations inside each node and replaced them by a\n",
    "label (1, 2, and 3) to represent nodes `i=1`, `i=2`, and `i=3`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In addition to nodes 1, 2, and 3, there is also a root node (the circle), and\n",
    "three arcs. Each arc has an origin node and a destination node, like `1 => 2`,\n",
    "and a corresponding probability of transitioning from the origin to the\n",
    "destination. Unless specified, we assume that the arc probabilities are\n",
    "uniform over the number of outgoing arcs. Thus, in this picture the arc\n",
    "probabilities are all 1.0."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "State variables flow long the arcs of the graph. Thus, the outgoing state\n",
    "variable $x^\\prime$ from node 1 becomes the incoming state variable $x$ to\n",
    "node 2, and so on."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We denote the set of nodes by $\\mathcal{N}$, the root node by $R$, and the\n",
    "probability of transitioning from node $i$ to node $j$ by $p_{ij}$. (If no arc\n",
    "exists, then $p_{ij} = 0$.) We define the set of successors of node $i$ as\n",
    "$i^+ = \\{j \\in \\mathcal{N} | p_{ij} > 0\\}$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each node in the graph corresponds to a place at which the agent makes a\n",
    "decision, and we call moments in time at which the agent makes a decision\n",
    "**stages**. By convention, we try to draw policy graphs from left-to-right,\n",
    "with the stages as columns. There can be more than one node in a stage! Here's\n",
    "an example of a structure we call **Markovian policy graphs**:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Markovian policy graph](../assets/enso_markovian.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here each column represents a moment in time, the squiggly lines represent\n",
    "stochastic rainfall, and the rows represent the world in two discrete states:\n",
    "El Niño and La Niña. In the El Niño states, the distribution of the rainfall\n",
    "random variable is different to the distribution of the rainfall random\n",
    "variable in the La Niña states, and there is some switching probability\n",
    "between the two states that can be modelled by a Markov chain."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Moreover, policy graphs can have cycles! This allows them to model infinite\n",
    "horizon problems. Here's another example, taken from the paper\n",
    "[Dowson (2020)](https://doi.org/10.1002/net.21932):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![POWDer policy graph](../assets/powder_policy_graph.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The columns represent time, and the rows represent different states of the\n",
    "world. In this case, the rows represent different prices that milk can be sold\n",
    "for at the end of each year. The squiggly lines denote a multivariate random\n",
    "variable that models the weekly amount of rainfall that occurs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Note**\n",
    ">\n",
    "> The sum of probabilities on the outgoing arcs of node $i$ can be less than\n",
    "> 1, i.e., $\\sum\\limits_{j\\in i^+} p_{ij} \\le 1$. What does this mean?\n",
    "> One interpretation is that the probability is a [discount factor](https://en.wikipedia.org/wiki/Discounting).\n",
    "> Another interpretation is that there is an implicit \"zero\" node that we\n",
    "> have not modeled, with $p_{i0} = 1 - \\sum\\limits_{j\\in i^+} p_{ij}$.\n",
    "> This zero node has $C_0(x, u, \\omega) = 0$, and $0^+ = \\varnothing$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## More notation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recall that each node $i$ has a **decision rule** $u = \\pi_i(x, \\omega)$,\n",
    "which is a function that maps the incoming state variable and observation of\n",
    "the random variable to a control $u$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The set of decision rules, with one element for each node in the policy graph,\n",
    "is called a **policy**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The goal of the agent is to find a policy that minimizes the expected cost of\n",
    "starting at the root node with some initial condition $x_R$, and proceeding\n",
    "from node to node along the probabilistic arcs until they reach a node with no\n",
    "outgoing arcs (or it reaches an implicit \"zero\" node)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\min_{\\pi} \\mathbb{E}_{i \\in R^+, \\omega \\in \\Omega_i}[V_i^\\pi(x_R, \\omega)],\n",
    "$$\n",
    "where\n",
    "$$\n",
    "V_i^\\pi(x, \\omega) = C_i(x, u, \\omega) + \\mathbb{E}_{j \\in i^+, \\varphi \\in \\Omega_j}[V_j(x^\\prime, \\varphi)],\n",
    "$$\n",
    "where $u = \\pi_i(x, \\omega) \\in U_i(x, \\omega)$, and\n",
    "$x^\\prime = T_i(x, u, \\omega)$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The expectations are a bit complicated, but they are equivalent to:\n",
    "$$\n",
    "\\mathbb{E}_{j \\in i^+, \\varphi \\in \\Omega_j}[V_j(x^\\prime, \\varphi)] = \\sum\\limits_{j \\in i^+} p_{ij} \\sum\\limits_{\\varphi \\in \\Omega_j} p_{\\varphi}V_j(x^\\prime, \\varphi).\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "An optimal policy is the set of decision rules that the agent can use to make\n",
    "decisions and achieve the smallest expected cost."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assumptions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Warning**\n",
    ">\n",
    "> This section is important!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The space of problems you can model with this framework is very large. Too\n",
    "large, in fact, for us to form tractable solution algorithms for! Stochastic\n",
    "dual dynamic programming requires the following assumptions in order to work:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Assumption 1: finite nodes**\n",
    "\n",
    "There is a finite number of nodes in $\\mathcal{N}$.\n",
    "\n",
    "**Assumption 2: finite random variables**\n",
    "\n",
    "The sample space $\\Omega_i$ is finite and discrete for each node\n",
    "$i\\in\\mathcal{N}$.\n",
    "\n",
    "**Assumption 3: convex problems**\n",
    "\n",
    "Given fixed $\\omega$, $C_i(x, u, \\omega)$ is a convex function,\n",
    "$T_i(x, u, \\omega)$ is linear, and  $U_i(x, u, \\omega)$ is a non-empty,\n",
    "bounded convex set with respect to $x$ and $u$.\n",
    "\n",
    "**Assumption 4: no infinite loops**\n",
    "\n",
    "For all loops in the policy graph, the product of the arc transition\n",
    "probabilities around the loop is strictly less than 1.\n",
    "\n",
    "**Assumption 5: relatively complete recourse**\n",
    "\n",
    "This is a technical but important assumption. See Relatively complete recourse\n",
    "for more details."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Note**\n",
    ">\n",
    "> SDDP.jl relaxes assumption (3) to allow for integer state and control\n",
    "> variables, but we won't go into the details here. Assumption (4)\n",
    "> essentially means that we obtain a discounted-cost solution for\n",
    "> infinite-horizon problems, instead of an average-cost solution; see\n",
    "> [Dowson (2020)](https://doi.org/10.1002/net.21932) for details."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dynamic programming and subproblems"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have formulated our problem, we need some ways of computing\n",
    "optimal decision rules. One way is to just use a heuristic like \"choose a\n",
    "control randomly from the set of feasible controls.\" However, such a policy\n",
    "is unlikely to be optimal."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A better way of obtaining an optimal policy is to use [Bellman's principle of\n",
    "optimality](https://en.wikipedia.org/wiki/Bellman_equation#Bellman's_principle_of_optimality),\n",
    "a.k.a Dynamic Programming, and define a recursive **subproblem** as follows:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V_i(x, \\omega) = \\min\\limits_{\\bar{x}, x^\\prime, u} \\;\\; & C_i(\\bar{x}, u, \\omega) + \\mathbb{E}_{j \\in i^+, \\varphi \\in \\Omega_j}[V_j(x^\\prime, \\varphi)]\\\\\n",
    "& x^\\prime = T_i(\\bar{x}, u, \\omega) \\\\\n",
    "& u \\in U_i(\\bar{x}, \\omega) \\\\\n",
    "& \\bar{x} = x.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Our decision rule, $\\pi_i(x, \\omega)$, solves this optimization problem and\n",
    "returns a $u^*$ corresponding to an optimal solution.\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> We add $\\bar{x}$ as a decision variable, along with the fishing constraint\n",
    "> $\\bar{x} = x$ for two reasons: it makes it obvious that formulating a\n",
    "> problem with $x \\times u$ results in a bilinear program instead of a\n",
    "> linear program (see Assumption 3), and it simplifies the implementation of\n",
    "> the SDDP algorithm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "These subproblems are very difficult to solve exactly, because they involve\n",
    "recursive optimization problems with lots of nested expectations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Therefore, instead of solving them exactly, SDDP.jl works by iteratively\n",
    "approximating the expectation term of each subproblem, which is also called\n",
    "the cost-to-go term. For now, you don't need to understand the details, other\n",
    "than that there is a nasty cost-to-go term that we deal with\n",
    "behind-the-scenes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The subproblem view of a multistage stochastic program is also important,\n",
    "because it provides a convenient way of communicating the different parts of\n",
    "the broader problem, and it is how we will communicate the problem to SDDP.jl.\n",
    "All we need to do is drop the cost-to-go term and fishing constraint, and\n",
    "define a new subproblem `SP` as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\texttt{SP}_i(x, \\omega) : \\min\\limits_{\\bar{x}, x^\\prime, u} \\;\\; & C_i(\\bar{x}, u, \\omega) \\\\\n",
    "& x^\\prime = T_i(\\bar{x}, u, \\omega) \\\\\n",
    "& u \\in U_i(\\bar{x}, \\omega).\n",
    "\\end{aligned}\n",
    "$$\n",
    "> **Note**\n",
    ">\n",
    "> When we talk about formulating a **subproblem** with SDDP.jl, this is the\n",
    "> formulation we mean."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We've retained the transition function and uncertainty set because they help\n",
    "to motivate the different components of the subproblem. However, in general,\n",
    "the subproblem can be more general. A better (less restrictive) representation\n",
    "might be:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\texttt{SP}_i(x, \\omega) : \\min\\limits_{\\bar{x}, x^\\prime, u} \\;\\; & C_i(\\bar{x}, x^\\prime, u, \\omega) \\\\\n",
    "& (\\bar{x}, x^\\prime, u) \\in \\mathcal{X}_i(\\omega).\n",
    "\\end{aligned}\n",
    "$$\n",
    "Note that the outgoing state variable can appear in the objective, and we can\n",
    "add constraints involving the incoming and outgoing state variables. It\n",
    "should be obvious how to map between the two representations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example: hydro-thermal scheduling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hydrothermal scheduling is the most common application of stochastic dual\n",
    "dynamic programming. To illustrate some of the basic functionality of\n",
    "`SDDP.jl`, we implement a very simple model of the hydrothermal scheduling\n",
    "problem."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Problem statement"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We consider the problem of scheduling electrical generation over three weeks\n",
    "in order to meet a known demand of 150 MWh in each week.\n",
    "\n",
    "There are two generators: a thermal generator, and a hydro generator. In each\n",
    "week, the agent needs to decide how much energy to generate from thermal, and\n",
    "how much energy to generate from hydro.\n",
    "\n",
    "The thermal generator has a short-run marginal cost of \\$50/MWh in the first\n",
    "stage, \\$100/MWh in the second stage, and \\$150/MWh in the third stage.\n",
    "\n",
    "The hydro generator has a short-run marginal cost of \\$0/MWh.\n",
    "\n",
    "The hydro generator draws water from a reservoir which has a maximum capacity\n",
    "of 200 MWh. (Although water is usually measured in m³, we measure it in the\n",
    "energy-equivalent MWh to simplify things. In practice, there is a conversion\n",
    "function between m³ flowing throw the turbine and MWh.) At the start of the\n",
    "first time period, the reservoir is full.\n",
    "\n",
    "In addition to the ability to generate electricity by passing water through\n",
    "the hydroelectric turbine, the hydro generator can also spill water down a\n",
    "spillway (bypassing the turbine) in order to prevent the water from\n",
    "over-topping the dam. We assume that there is no cost of spillage."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In addition to water leaving the reservoir, water that flows into the reservoir\n",
    "through rainfall or rivers are referred to as inflows. These inflows are\n",
    "uncertain, and are the cause of the main trade-off in hydro-thermal\n",
    "scheduling: the desire to use water now to generate cheap electricity, against\n",
    "the risk that future inflows will be low, leading to blackouts or expensive\n",
    "thermal generation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For our simple model, we assume that the inflows can be modelled by a discrete\n",
    "distribution with the three outcomes given in the following table:\n",
    "\n",
    "| ω    |   0 |  50 | 100 |\n",
    "| ---- | --- | --- | --- |\n",
    "| P(ω) | 1/3 | 1/3 | 1/3 |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The value of the noise (the random variable) is observed by the agent at the\n",
    "start of each stage. This makes the problem a _wait-and-see_ or\n",
    "_hazard-decision_ formulation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The goal of the agent is to minimize the expected cost of generation over the\n",
    "three weeks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Formulating the problem"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before going further, we need to load SDDP.jl:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using SDDP"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Graph structure"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we need to identify the structure of the policy graph. From the problem\n",
    "statement, we want to model the problem over three weeks in weekly stages.\n",
    "Therefore, the policy graph is a linear graph with three stages:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "graph = SDDP.LinearGraph(3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Building the subproblem"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we need to construct the associated subproblem for each node in `graph`.\n",
    "To do so, we need to provide SDDP.jl a function which takes two arguments. The\n",
    "first is `subproblem::Model`, which is an empty JuMP model. The second is\n",
    "`node`, which is the name of each node in the policy graph. If the graph is\n",
    "linear, SDDP defaults to naming the nodes using the integers in `1:T`. Here's\n",
    "an example that we are going to flesh out over the next few paragraphs:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function subproblem_builder(subproblem::Model, node::Int)\n",
    "    # ... stuff to go here ...\n",
    "    return subproblem\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Warning**\n",
    ">\n",
    "> If you use a different type of graph, `node` may be a type different to\n",
    "> `Int`. For example, in `SDDP.MarkovianGraph`, `node` is a\n",
    "> `Tuple{Int,Int}`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### State variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first part of the subproblem we need to identify are the state variables.\n",
    "Since we only have one reservoir, there is only one state variable, `volume`,\n",
    "the volume of water in the reservoir [MWh].\n",
    "\n",
    "The volume had bounds of `[0, 200]`, and the reservoir was full at the start\n",
    "of time, so $x_R = 200$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We add state variables to our `subproblem` using JuMP's `@variable` macro.\n",
    "However, in addition to the usual syntax, we also pass `SDDP.State`, and we\n",
    "need to provide the initial value ($x_R$) using the `initial_value` keyword."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function subproblem_builder(subproblem::Model, node::Int)\n",
    "    # State variables\n",
    "    @variable(subproblem, 0 <= volume <= 200, SDDP.State, initial_value = 200)\n",
    "    return subproblem\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The syntax for adding a state variable is a little obtuse, because `volume` is\n",
    "not single JuMP variable. Instead, `volume` is a struct with two fields, `.in`\n",
    "and `.out`, corresponding to the incoming and outgoing state variables\n",
    "respectively."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Note**\n",
    ">\n",
    "> We don't need to add the fishing constraint $\\bar{x} = x$; SDDP.jl does\n",
    "> this automatically."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Control variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next part of the subproblem we need to identify are the control variables.\n",
    "The control variables for our problem are:\n",
    " - `thermal_generation`: the quantity of energy generated from thermal\n",
    "   [MWh/week]\n",
    " - `hydro_generation`: the quantity of energy generated from hydro [MWh/week]\n",
    " - `hydro_spill`: the volume of water spilled from the reservoir in each week\n",
    "   [MWh/week]\n",
    "Each of these variables is non-negative."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We add control variables to our `subproblem` as normal JuMP variables, using\n",
    "`@variable` or `@variables`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function subproblem_builder(subproblem::Model, node::Int)\n",
    "    # State variables\n",
    "    @variable(subproblem, 0 <= volume <= 200, SDDP.State, initial_value = 200)\n",
    "    # Control variables\n",
    "    @variables(subproblem, begin\n",
    "        thermal_generation >= 0\n",
    "        hydro_generation >= 0\n",
    "        hydro_spill >= 0\n",
    "    end)\n",
    "    return subproblem\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Tip**\n",
    ">\n",
    "> Modeling is an art, and a tricky part of that art is figuring out which\n",
    "> variables are state variables, and which are control variables. A good\n",
    "> rule is: if you need a value of a control variable in some future node to\n",
    "> make a decision, it is a state variable instead."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next step is to identify any random variables. In our example, we had\n",
    " - `inflow`: the quantity of water that flows into the reservoir each week\n",
    "   [MWh/week]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To add an uncertain variable to the model, we create a new JuMP variable\n",
    "`inflow`, and then call the function `SDDP.parameterize`. The\n",
    "`SDDP.parameterize` function takes three arguments: the subproblem, a\n",
    "vector of realizations, and a corresponding vector of probabilities."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function subproblem_builder(subproblem::Model, node::Int)\n",
    "    # State variables\n",
    "    @variable(subproblem, 0 <= volume <= 200, SDDP.State, initial_value = 200)\n",
    "    # Control variables\n",
    "    @variables(subproblem, begin\n",
    "        thermal_generation >= 0\n",
    "        hydro_generation >= 0\n",
    "        hydro_spill >= 0\n",
    "    end)\n",
    "    # Random variables\n",
    "    @variable(subproblem, inflow)\n",
    "    Ω = [0.0, 50.0, 100.0]\n",
    "    P = [1 / 3, 1 / 3, 1 / 3]\n",
    "    SDDP.parameterize(subproblem, Ω, P) do ω\n",
    "        return JuMP.fix(inflow, ω)\n",
    "    end\n",
    "    return subproblem\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note how we use the JuMP function\n",
    "[`JuMP.fix`](https://jump.dev/JuMP.jl/stable/reference/variables/#JuMP.fix)\n",
    "to set the value of the `inflow` variable to `ω`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Warning**\n",
    ">\n",
    "> `SDDP.parameterize` can only be called once in each subproblem\n",
    "> definition! If your random variable is multi-variate, read\n",
    "> Add multi-dimensional noise terms."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Transition function and constraints"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we've identified our variables, we can define the transition function\n",
    "and the constraints."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For our problem, the state variable is the volume of water in the reservoir.\n",
    "The volume of water decreases in response to water being used for hydro\n",
    "generation and spillage. So the transition function is:\n",
    "`volume.out = volume.in - hydro_generation - hydro_spill + inflow`. (Note how\n",
    "we use `volume.in` and `volume.out` to refer to the incoming and outgoing\n",
    "state variables.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is also a constraint that the total generation must sum to 150 MWh."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Both the transition function and any additional constraint are added using\n",
    "JuMP's `@constraint` and `@constraints` macro."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function subproblem_builder(subproblem::Model, node::Int)\n",
    "    # State variables\n",
    "    @variable(subproblem, 0 <= volume <= 200, SDDP.State, initial_value = 200)\n",
    "    # Control variables\n",
    "    @variables(subproblem, begin\n",
    "        thermal_generation >= 0\n",
    "        hydro_generation >= 0\n",
    "        hydro_spill >= 0\n",
    "    end)\n",
    "    # Random variables\n",
    "    @variable(subproblem, inflow)\n",
    "    Ω = [0.0, 50.0, 100.0]\n",
    "    P = [1 / 3, 1 / 3, 1 / 3]\n",
    "    SDDP.parameterize(subproblem, Ω, P) do ω\n",
    "        return JuMP.fix(inflow, ω)\n",
    "    end\n",
    "    # Transition function and constraints\n",
    "    @constraints(\n",
    "        subproblem,\n",
    "        begin\n",
    "            volume.out == volume.in - hydro_generation - hydro_spill + inflow\n",
    "            demand_constraint, hydro_generation + thermal_generation == 150\n",
    "        end\n",
    "    )\n",
    "    return subproblem\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Objective function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we need to add an objective function using `@stageobjective`. The\n",
    "objective of the agent is to minimize the cost of thermal generation. This is\n",
    "complicated by a fuel cost that depends on the `node`.\n",
    "\n",
    "One possibility is to use an `if` statement on `node` to define the correct\n",
    "objective:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function subproblem_builder(subproblem::Model, node::Int)\n",
    "    # State variables\n",
    "    @variable(subproblem, 0 <= volume <= 200, SDDP.State, initial_value = 200)\n",
    "    # Control variables\n",
    "    @variables(subproblem, begin\n",
    "        thermal_generation >= 0\n",
    "        hydro_generation >= 0\n",
    "        hydro_spill >= 0\n",
    "    end)\n",
    "    # Random variables\n",
    "    @variable(subproblem, inflow)\n",
    "    Ω = [0.0, 50.0, 100.0]\n",
    "    P = [1 / 3, 1 / 3, 1 / 3]\n",
    "    SDDP.parameterize(subproblem, Ω, P) do ω\n",
    "        return JuMP.fix(inflow, ω)\n",
    "    end\n",
    "    # Transition function and constraints\n",
    "    @constraints(\n",
    "        subproblem,\n",
    "        begin\n",
    "            volume.out == volume.in - hydro_generation - hydro_spill + inflow\n",
    "            demand_constraint, hydro_generation + thermal_generation == 150\n",
    "        end\n",
    "    )\n",
    "    # Stage-objective\n",
    "    if node == 1\n",
    "        @stageobjective(subproblem, 50 * thermal_generation)\n",
    "    elseif node == 2\n",
    "        @stageobjective(subproblem, 100 * thermal_generation)\n",
    "    else\n",
    "        @assert node == 3\n",
    "        @stageobjective(subproblem, 150 * thermal_generation)\n",
    "    end\n",
    "    return subproblem\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "A second possibility is to use an array of fuel costs, and use `node` to index\n",
    "the correct value:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function subproblem_builder(subproblem::Model, node::Int)\n",
    "    # State variables\n",
    "    @variable(subproblem, 0 <= volume <= 200, SDDP.State, initial_value = 200)\n",
    "    # Control variables\n",
    "    @variables(subproblem, begin\n",
    "        thermal_generation >= 0\n",
    "        hydro_generation >= 0\n",
    "        hydro_spill >= 0\n",
    "    end)\n",
    "    # Random variables\n",
    "    @variable(subproblem, inflow)\n",
    "    Ω = [0.0, 50.0, 100.0]\n",
    "    P = [1 / 3, 1 / 3, 1 / 3]\n",
    "    SDDP.parameterize(subproblem, Ω, P) do ω\n",
    "        return JuMP.fix(inflow, ω)\n",
    "    end\n",
    "    # Transition function and constraints\n",
    "    @constraints(\n",
    "        subproblem,\n",
    "        begin\n",
    "            volume.out == volume.in - hydro_generation - hydro_spill + inflow\n",
    "            demand_constraint, hydro_generation + thermal_generation == 150\n",
    "        end\n",
    "    )\n",
    "    # Stage-objective\n",
    "    fuel_cost = [50, 100, 150]\n",
    "    @stageobjective(subproblem, fuel_cost[node] * thermal_generation)\n",
    "    return subproblem\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Constructing the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we've written our subproblem, we need to construct the full model.\n",
    "For that, we're going to need a linear solver. Let's choose HiGHS:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using HiGHS"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Warning**\n",
    ">\n",
    "> In larger problems, you should use a more robust commercial LP solver like\n",
    "> Gurobi. Read Words of warning for more details."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we can create a full model using `SDDP.PolicyGraph`, passing our\n",
    "`subproblem_builder` function as the first argument, and our `graph` as the\n",
    "second:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.PolicyGraph(\n",
    "    subproblem_builder,\n",
    "    graph;\n",
    "    sense = :Min,\n",
    "    lower_bound = 0.0,\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "* `sense`: the optimization sense. Must be `:Min` or `:Max`.\n",
    "* `lower_bound`: you _must_ supply a valid bound on the objective. For our\n",
    "  problem, we know that we cannot incur a negative cost so \\$0 is a valid\n",
    "  lower bound.\n",
    "* `optimizer`: This is borrowed directly from JuMP's `Model` constructor:\n",
    "  `Model(HiGHS.Optimizer)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because linear policy graphs are the most commonly used structure, we can use\n",
    "`SDDP.LinearPolicyGraph` instead of passing `SDDP.LinearGraph(3)` to\n",
    "`SDDP.PolicyGraph`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.LinearPolicyGraph(\n",
    "    subproblem_builder;\n",
    "    stages = 3,\n",
    "    sense = :Min,\n",
    "    lower_bound = 0.0,\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is also the option is to use Julia's `do` syntax to avoid needing to\n",
    "define a `subproblem_builder` function separately:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.LinearPolicyGraph(;\n",
    "    stages = 3,\n",
    "    sense = :Min,\n",
    "    lower_bound = 0.0,\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ") do subproblem, node\n",
    "    # State variables\n",
    "    @variable(subproblem, 0 <= volume <= 200, SDDP.State, initial_value = 200)\n",
    "    # Control variables\n",
    "    @variables(subproblem, begin\n",
    "        thermal_generation >= 0\n",
    "        hydro_generation >= 0\n",
    "        hydro_spill >= 0\n",
    "    end)\n",
    "    # Random variables\n",
    "    @variable(subproblem, inflow)\n",
    "    Ω = [0.0, 50.0, 100.0]\n",
    "    P = [1 / 3, 1 / 3, 1 / 3]\n",
    "    SDDP.parameterize(subproblem, Ω, P) do ω\n",
    "        return JuMP.fix(inflow, ω)\n",
    "    end\n",
    "    # Transition function and constraints\n",
    "    @constraints(\n",
    "        subproblem,\n",
    "        begin\n",
    "            volume.out == volume.in - hydro_generation - hydro_spill + inflow\n",
    "            demand_constraint, hydro_generation + thermal_generation == 150\n",
    "        end\n",
    "    )\n",
    "    # Stage-objective\n",
    "    if node == 1\n",
    "        @stageobjective(subproblem, 50 * thermal_generation)\n",
    "    elseif node == 2\n",
    "        @stageobjective(subproblem, 100 * thermal_generation)\n",
    "    else\n",
    "        @assert node == 3\n",
    "        @stageobjective(subproblem, 150 * thermal_generation)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Info**\n",
    ">\n",
    "> Julia's `do` syntax is just a different way of passing an anonymous\n",
    "> function `inner` to some function `outer` which takes `inner` as the first\n",
    "> argument. For example, given:\n",
    "> ```julia\n",
    "> outer(inner::Function, x, y) = inner(x, y)\n",
    "> ```\n",
    "> then\n",
    "> ```julia\n",
    "> outer(1, 2) do x, y\n",
    ">     return x^2 + y^2\n",
    "> end\n",
    "> ```\n",
    "> is equivalent to:\n",
    "> ```julia\n",
    "> outer((x, y) -> x^2 + y^2, 1, 2)\n",
    "> ```\n",
    "> For our purpose, `inner` is `subproblem_builder`, and `outer` is\n",
    "> `SDDP.PolicyGraph`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training a policy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have a model, which is a description of the policy graph, we need to\n",
    "train a policy. Models can be trained using the `SDDP.train` function.\n",
    "It accepts a number of keyword arguments. `iteration_limit` terminates the\n",
    "training after the provided number of iterations."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "SDDP.train(model; iteration_limit = 10)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "There's a lot going on in this printout! Let's break it down."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first section, \"problem,\" gives some problem statistics. In this example\n",
    "there are 3 nodes, 1 state variable, and 27 scenarios ($3^3$). We haven't\n",
    "solved this problem before so there are no existing cuts."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The \"options\" section lists some options we are using to solve the problem.\n",
    "For more information on the numerical stability report, read the\n",
    "Numerical stability report section."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The \"subproblem structure\" section also needs explaining. This looks at all of\n",
    "the nodes in the policy graph and reports the minimum and maximum number of\n",
    "variables and each constraint type in the corresponding subproblem. In this\n",
    "case each subproblem has 7 variables and various numbers of different\n",
    "constraint types. Note that the exact numbers may not correspond to the\n",
    "formulation as you wrote it, because SDDP.jl adds some extra variables for the\n",
    "cost-to-go function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then comes the iteration log, which is the main part of the printout. It has\n",
    "the following columns:\n",
    " - `iteration`: the SDDP iteration\n",
    " - `simulation`: the cost of the single forward pass simulation for that\n",
    "   iteration. This value is stochastic and is not guaranteed to improve over\n",
    "   time. However, it's useful to check that the units are reasonable, and that\n",
    "   it is not deterministic if you intended for the problem to be stochastic,\n",
    "   etc.\n",
    " - `bound`: this is a lower bound (upper if maximizing) for the value of the\n",
    "   optimal policy. This bound should be monotonically improving (increasing if\n",
    "   minimizing, decreasing if maximizing), but in some cases it can temporarily\n",
    "   worsen due to cut selection, especially in the early iterations of the\n",
    "   algorithm.\n",
    " - `time (s)`: the total number of seconds spent solving so far\n",
    " - `solves`: the total number of subproblem solves to date. This can be very\n",
    "   large!\n",
    " - `pid`: the ID of the processor used to solve that iteration. This\n",
    "   should be 1 unless you are using parallel computation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In addition, if the first character of a line is `†`, then SDDP.jl experienced\n",
    "numerical issues during the solve, but successfully recovered."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The printout finishes with some summary statistics:\n",
    "\n",
    " - `status`: why did the solver stop?\n",
    " - `total time (s)`, `best bound`, and `total solves` are the values from the\n",
    "   last iteration of the solve.\n",
    " - `simulation ci`: a confidence interval that estimates the quality of the\n",
    "   policy from the `Simulation` column.\n",
    " - `numeric issues`: the number of iterations that experienced numerical\n",
    "   issues."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Warning**\n",
    ">\n",
    "> The `simulation ci` result can be misleading if you run a small number of\n",
    "> iterations, or if the initial simulations are very bad. On a more\n",
    "> technical note, it is an _in-sample simulation_, which may not reflect the\n",
    "> true performance of the policy. See Obtaining bounds for more\n",
    "> details."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Obtaining the decision rule"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After training a policy, we can create a decision rule using\n",
    "`SDDP.DecisionRule`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "rule = SDDP.DecisionRule(model; node = 1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, to evaluate the decision rule, we use `SDDP.evaluate`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "solution = SDDP.evaluate(\n",
    "    rule;\n",
    "    incoming_state = Dict(:volume => 150.0),\n",
    "    noise = 50.0,\n",
    "    controls_to_record = [:hydro_generation, :thermal_generation],\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulating the policy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once you have a trained policy, you can also simulate it using\n",
    "`SDDP.simulate`. The return value from `simulate` is a vector with one\n",
    "element for each replication. Each element is itself a vector, with one\n",
    "element for each stage. Each element, corresponding to a particular stage in a\n",
    "particular replication, is a dictionary that records information from the\n",
    "simulation."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations = SDDP.simulate(\n",
    "    # The trained model to simulate.\n",
    "    model,\n",
    "    # The number of replications.\n",
    "    100,\n",
    "    # A list of names to record the values of.\n",
    "    [:volume, :thermal_generation, :hydro_generation, :hydro_spill],\n",
    ")\n",
    "\n",
    "replication = 1\n",
    "stage = 2\n",
    "simulations[replication][stage]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ignore many of the entries for now;  they will be relevant later."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " One element of interest is `:volume`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "outgoing_volume = map(simulations[1]) do node\n",
    "    return node[:volume].out\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    " Another is `:thermal_generation`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "thermal_generation = map(simulations[1]) do node\n",
    "    return node[:thermal_generation]\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Obtaining bounds"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because the optimal policy is stochastic, one common approach to quantify the\n",
    "quality of the policy is to construct a confidence interval for the expected\n",
    "cost by summing the stage objectives along each simulation."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "objectives = map(simulations) do simulation\n",
    "    return sum(stage[:stage_objective] for stage in simulation)\n",
    "end\n",
    "\n",
    "μ, ci = SDDP.confidence_interval(objectives)\n",
    "println(\"Confidence interval: \", μ, \" ± \", ci)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This confidence interval is an estimate for an upper bound of the policy's\n",
    "quality. We can calculate the lower bound using `SDDP.calculate_bound`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "println(\"Lower bound: \", SDDP.calculate_bound(model))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Tip**\n",
    ">\n",
    "> The upper- and lower-bounds are reversed if maximizing, i.e., `SDDP.calculate_bound`.\n",
    "> returns an upper bound."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom recorders"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In addition to simulating the primal values of variables, we can also pass\n",
    "custom recorder functions. Each of these functions takes one argument, the\n",
    "JuMP subproblem corresponding to each node. This function gets called after we\n",
    "have solved each node as we traverse the policy graph in the simulation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For example, the dual of the demand constraint (which we named\n",
    "`demand_constraint`) corresponds to the price we should charge for\n",
    "electricity, since it represents the cost of each additional unit of demand.\n",
    "To calculate this, we can go:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations = SDDP.simulate(\n",
    "    model,\n",
    "    1;  ## Perform a single simulation\n",
    "    custom_recorders = Dict{Symbol,Function}(\n",
    "        :price => (sp::JuMP.Model) -> JuMP.dual(sp[:demand_constraint]),\n",
    "    ),\n",
    ")\n",
    "\n",
    "prices = map(simulations[1]) do node\n",
    "    return node[:price]\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extracting the marginal water values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can use `SDDP.ValueFunction` and `SDDP.evaluate`\n",
    "to obtain and evaluate the value function at different points in the\n",
    "state-space."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Note**\n",
    ">\n",
    "> By \"value function\" we mean $\\mathbb{E}_{j \\in i^+, \\varphi \\in \\Omega_j}[V_j(x^\\prime, \\varphi)]$,\n",
    "> not the function $V_i(x, \\omega)$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we construct a value function from the first subproblem:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "V = SDDP.ValueFunction(model; node = 1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we can evaluate `V` at a point:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cost, price = SDDP.evaluate(V, Dict(\"volume\" => 10))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This returns the cost-to-go (`cost`), and the gradient of the cost-to-go\n",
    "function with respect to each state variable. Note that since we are\n",
    "minimizing, the price has a negative sign: each additional unit of water leads\n",
    "to a decrease in the expected long-run cost."
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  },
  "kernelspec": {
   "name": "julia-1.11",
   "display_name": "Julia 1.11.1",
   "language": "julia"
  }
 },
 "nbformat": 4
}
