<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Example: two-stage newsvendor · SDDP.jl</title><meta name="title" content="Example: two-stage newsvendor · SDDP.jl"/><meta property="og:title" content="Example: two-stage newsvendor · SDDP.jl"/><meta property="twitter:title" content="Example: two-stage newsvendor · SDDP.jl"/><meta name="description" content="Documentation for SDDP.jl."/><meta property="og:description" content="Documentation for SDDP.jl."/><meta property="twitter:description" content="Documentation for SDDP.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="SDDP.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">SDDP.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox" checked/><label class="tocitem" for="menuitem-2"><span class="docs-label">Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../first_steps/">An introduction to SDDP.jl</a></li><li><a class="tocitem" href="../objective_uncertainty/">Uncertainty in the objective function</a></li><li><a class="tocitem" href="../markov_uncertainty/">Markovian policy graphs</a></li><li><a class="tocitem" href="../plotting/">Plotting tools</a></li><li><a class="tocitem" href="../warnings/">Words of warning</a></li><li><a class="tocitem" href="../arma/">Auto-regressive stochastic processes</a></li><li><a class="tocitem" href="../decision_hazard/">Here-and-now and hazard-decision</a></li><li><a class="tocitem" href="../objective_states/">Objective states</a></li><li><a class="tocitem" href="../pglib_opf/">Alternative forward models</a></li><li><a class="tocitem" href="../mdps/">Example: Markov Decision Processes</a></li><li class="is-active"><a class="tocitem" href>Example: two-stage newsvendor</a><ul class="internal"><li><a class="tocitem" href="#Background"><span>Background</span></a></li><li><a class="tocitem" href="#L-Shaped-theory"><span>L-Shaped theory</span></a></li><li><a class="tocitem" href="#L-Shaped-implementation"><span>L-Shaped implementation</span></a></li><li><a class="tocitem" href="#Policy-Graph"><span>Policy Graph</span></a></li><li><a class="tocitem" href="#Simulation"><span>Simulation</span></a></li><li><a class="tocitem" href="#Risk-aversion-revisited"><span>Risk aversion revisited</span></a></li><li><a class="tocitem" href="#Things-to-try"><span>Things to try</span></a></li></ul></li><li><a class="tocitem" href="../example_reservoir/">Example: deterministic to stochastic</a></li><li><a class="tocitem" href="../example_milk_producer/">Example: the milk producer</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">How-to guides</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../guides/access_previous_variables/">Access variables from a previous stage</a></li><li><a class="tocitem" href="../../guides/add_a_multidimensional_state_variable/">Add a multi-dimensional state variable</a></li><li><a class="tocitem" href="../../guides/add_a_risk_measure/">Add a risk measure</a></li><li><a class="tocitem" href="../../guides/add_integrality/">Integrality</a></li><li><a class="tocitem" href="../../guides/add_multidimensional_noise/">Add multi-dimensional noise terms</a></li><li><a class="tocitem" href="../../guides/add_noise_in_the_constraint_matrix/">Add noise in the constraint matrix</a></li><li><a class="tocitem" href="../../guides/choose_a_stopping_rule/">Choose a stopping rule</a></li><li><a class="tocitem" href="../../guides/create_a_general_policy_graph/">Create a general policy graph</a></li><li><a class="tocitem" href="../../guides/debug_a_model/">Debug a model</a></li><li><a class="tocitem" href="../../guides/improve_computational_performance/">Improve computational performance</a></li><li><a class="tocitem" href="../../guides/simulate_using_a_different_sampling_scheme/">Simulate using a different sampling scheme</a></li><li><a class="tocitem" href="../../guides/create_a_belief_state/">Create a belief state</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">Explanation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../explanation/theory_intro/">Introductory theory</a></li><li><a class="tocitem" href="../../explanation/risk/">Risk aversion</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/FAST_hydro_thermal/">FAST: the hydro-thermal problem</a></li><li><a class="tocitem" href="../../examples/FAST_production_management/">FAST: the production management problem</a></li><li><a class="tocitem" href="../../examples/FAST_quickstart/">FAST: the quickstart problem</a></li><li><a class="tocitem" href="../../examples/Hydro_thermal/">Hydro-thermal scheduling</a></li><li><a class="tocitem" href="../../examples/StochDynamicProgramming.jl_multistock/">StochDynamicProgramming: the multistock problem</a></li><li><a class="tocitem" href="../../examples/StochDynamicProgramming.jl_stock/">StochDynamicProgramming: the stock problem</a></li><li><a class="tocitem" href="../../examples/StructDualDynProg.jl_prob5.2_2stages/">StructDualDynProg: Problem 5.2, 2 stages</a></li><li><a class="tocitem" href="../../examples/StructDualDynProg.jl_prob5.2_3stages/">StructDualDynProg: Problem 5.2, 3 stages</a></li><li><a class="tocitem" href="../../examples/agriculture_mccardle_farm/">The farm planning problem</a></li><li><a class="tocitem" href="../../examples/air_conditioning/">Air conditioning</a></li><li><a class="tocitem" href="../../examples/air_conditioning_forward/">Training with a different forward model</a></li><li><a class="tocitem" href="../../examples/all_blacks/">Deterministic All Blacks</a></li><li><a class="tocitem" href="../../examples/asset_management_simple/">Asset management</a></li><li><a class="tocitem" href="../../examples/asset_management_stagewise/">Asset management with modifications</a></li><li><a class="tocitem" href="../../examples/belief/">Partially observable inventory management</a></li><li><a class="tocitem" href="../../examples/biobjective_hydro/">Biobjective hydro-thermal</a></li><li><a class="tocitem" href="../../examples/booking_management/">Booking management</a></li><li><a class="tocitem" href="../../examples/generation_expansion/">Generation expansion</a></li><li><a class="tocitem" href="../../examples/hydro_valley/">Hydro valleys</a></li><li><a class="tocitem" href="../../examples/infinite_horizon_hydro_thermal/">Infinite horizon hydro-thermal</a></li><li><a class="tocitem" href="../../examples/infinite_horizon_trivial/">Infinite horizon trivial</a></li><li><a class="tocitem" href="../../examples/no_strong_duality/">No strong duality</a></li><li><a class="tocitem" href="../../examples/objective_state_newsvendor/">Newsvendor</a></li><li><a class="tocitem" href="../../examples/sldp_example_one/">SLDP: example 1</a></li><li><a class="tocitem" href="../../examples/sldp_example_two/">SLDP: example 2</a></li><li><a class="tocitem" href="../../examples/stochastic_all_blacks/">Stochastic All Blacks</a></li><li><a class="tocitem" href="../../examples/the_farmers_problem/">The farmer&#39;s problem</a></li><li><a class="tocitem" href="../../examples/vehicle_location/">Vehicle location</a></li></ul></li><li><a class="tocitem" href="../../apireference/">API Reference</a></li><li><a class="tocitem" href="../../release_notes/">Release notes</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Example: two-stage newsvendor</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Example: two-stage newsvendor</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/odow/SDDP.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/odow/SDDP.jl/blob/master/docs/src/tutorial/example_newsvendor.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Example:-two-stage-newsvendor"><a class="docs-heading-anchor" href="#Example:-two-stage-newsvendor">Example: two-stage newsvendor</a><a id="Example:-two-stage-newsvendor-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-two-stage-newsvendor" title="Permalink"></a></h1><p><em>This tutorial was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em> <a href="../example_newsvendor.jl"><em>Download the source as a <code>.jl</code> file</em></a>. <a href="../example_newsvendor.ipynb"><em>Download the source as a <code>.ipynb</code> file</em></a>.</p><p>The purpose of this tutorial is to demonstrate how to model and solve a two-stage stochastic program.</p><p>It is based on the <a href="https://jump.dev/JuMP.jl/dev/tutorials/applications/two_stage_stochastic/">Two stage stochastic programs</a> tutorial in JuMP.</p><p>This tutorial uses the following packages</p><pre><code class="language-julia hljs">using JuMP
using SDDP
import Distributions
import HiGHS
import Plots
import StatsPlots
import Statistics</code></pre><h2 id="Background"><a class="docs-heading-anchor" href="#Background">Background</a><a id="Background-1"></a><a class="docs-heading-anchor-permalink" href="#Background" title="Permalink"></a></h2><p>The data for this problem is:</p><pre><code class="language-julia hljs">D = Distributions.TriangularDist(150.0, 250.0, 200.0)
N = 100
d = sort!(rand(D, N));
Ω = 1:N
P = fill(1 / N, N);
StatsPlots.histogram(d; bins = 20, label = &quot;&quot;, xlabel = &quot;Demand&quot;)</code></pre><img src="75b1e408.svg" alt="Example block output"/><h2 id="L-Shaped-theory"><a class="docs-heading-anchor" href="#L-Shaped-theory">L-Shaped theory</a><a id="L-Shaped-theory-1"></a><a class="docs-heading-anchor-permalink" href="#L-Shaped-theory" title="Permalink"></a></h2><p>The L-Shaped method is a way of solving two-stage stochastic programs by Benders&#39; decomposition. It takes the problem:</p><p class="math-container">\[\begin{aligned}
V = \max\limits_{x,y_\omega} \;\; &amp; -2x + \mathbb{E}_\omega[5y_\omega - 0.1(x - y_\omega)] \\
  &amp; y_\omega \le x              &amp; \quad \forall \omega \in \Omega \\
  &amp; 0 \le y_\omega \le d_\omega &amp; \quad \forall \omega \in \Omega \\
  &amp; x \ge 0.
\end{aligned}\]</p><p>and decomposes it into a second-stage problem:</p><p class="math-container">\[\begin{aligned}
V_2(\bar{x}, d_\omega) = \max\limits_{x,x^\prime,y_\omega} \;\; &amp; 5y_\omega - x^\prime \\
  &amp; y_\omega \le x \\
  &amp; x^\prime = x - y_\omega \\
  &amp; 0 \le y_\omega \le d_\omega \\
  &amp; x = \bar{x} &amp; [\lambda]
\end{aligned}\]</p><p>and a first-stage problem:</p><p class="math-container">\[\begin{aligned}
V = \max\limits_{x,\theta} \;\; &amp; -2x + \theta \\
  &amp; \theta \le \mathbb{E}_\omega[V_2(x, \omega)] \\
  &amp; x \ge 0
\end{aligned}\]</p><p>Then, because <span>$V_2$</span> is convex with respect to <span>$\bar{x}$</span> for fixed <span>$\omega$</span>, we can use a set of feasible points <span>$\{x^k\}$</span> construct an outer approximation:</p><p class="math-container">\[\begin{aligned}
V^K = \max\limits_{x,\theta} \;\; &amp; -2x + \theta \\
  &amp; \theta \le \mathbb{E}_\omega[V_2(x^k, \omega) + \nabla V_2(x^k, \omega)^\top(x - x^k)] &amp; \quad k = 1,\ldots,K\\
  &amp; x \ge 0 \\
  &amp; \theta \le M
\end{aligned}\]</p><p>where <span>$M$</span> is an upper bound on possible values of <span>$V_2$</span> so that the problem has a bounded solution.</p><p>It is also useful to see that because <span>$\bar{x}$</span> appears only on the right-hand side of a linear program, <span>$\nabla V_2(x^k, \omega) = \lambda^k$</span>.</p><p>Ignoring how we choose <span>$x^k$</span> for now, we can construct a lower and upper bound on the optimal solution:</p><p class="math-container">\[-2x^K + \mathbb{E}_\omega[V_2(x^K, \omega)] = \underbar{V} \le V \le \overline{V} = V^K\]</p><p>Thus, we need some way of cleverly choosing a sequence of <span>$x^k$</span> so that the lower bound converges to the upper bound.</p><ol><li>Start with <span>$K=1$</span></li><li>Solve <span>$V^{K-1}$</span> to get <span>$x^K$</span></li><li>Set <span>$\overline{V} = V^k$</span></li><li>Solve <span>$V_2(x^K, \omega)$</span> for all <span>$\omega$</span> and store the optimal objective value and dual solution <span>$\lambda^K$</span></li><li>Set <span>$\underbar{V} = -2x^K + \mathbb{E}_\omega[V_2(x^k, \omega)]$</span></li><li>If <span>$\underbar{V} \approx \overline{V}$</span>, STOP</li><li>Add new constraint <span>$\theta \le \mathbb{E}_\omega[V_2(x^K, \omega) +\lambda^K (x - x^K)]$</span></li><li>Increment <span>$K$</span>, GOTO 2</li></ol><p>The next section implements this algorithm in Julia.</p><h2 id="L-Shaped-implementation"><a class="docs-heading-anchor" href="#L-Shaped-implementation">L-Shaped implementation</a><a id="L-Shaped-implementation-1"></a><a class="docs-heading-anchor-permalink" href="#L-Shaped-implementation" title="Permalink"></a></h2><p>Here&#39;s a function to compute the second-stage problem;</p><pre><code class="language-julia hljs">function solve_second_stage(x̅, d_ω)
    model = Model(HiGHS.Optimizer)
    set_silent(model)
    @variable(model, x_in)
    @variable(model, x_out &gt;= 0)
    fix(x_in, x̅)
    @variable(model, 0 &lt;= u_sell &lt;= d_ω)
    @constraint(model, x_out == x_in - u_sell)
    @constraint(model, u_sell &lt;= x_in)
    @objective(model, Max, 5 * u_sell - 0.1 * x_out)
    optimize!(model)
    return (
        V = objective_value(model),
        λ = reduced_cost(x_in),
        x = value(x_out),
        u = value(u_sell),
    )
end

solve_second_stage(200, 170)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(V = 847.0, λ = -0.1, x = 30.0, u = 170.0)</code></pre><p>Here&#39;s the first-stage subproblem:</p><pre><code class="language-julia hljs">model = Model(HiGHS.Optimizer)
set_silent(model)
@variable(model, x_in == 0)
@variable(model, x_out &gt;= 0)
@variable(model, u_make &gt;= 0)
@constraint(model, x_out == x_in + u_make)
M = 5 * maximum(d)
@variable(model, θ &lt;= M)
@objective(model, Max, -2 * u_make + θ)</code></pre><p class="math-container">\[ -2 u\_make + θ \]</p><p>Importantly, to ensure we have a bounded solution, we need to add an upper bound to the variable <code>θ</code>.</p><pre><code class="language-julia hljs">kIterationLimit = 100
for k in 1:kIterationLimit
    println(&quot;Solving iteration k = $k&quot;)
    # Step 2
    optimize!(model)
    xᵏ = value(x_out)
    println(&quot;  xᵏ = $xᵏ&quot;)
    # Step 3
    ub = objective_value(model)
    println(&quot;  V̅ = $ub&quot;)
    # Step 4
    ret = [solve_second_stage(xᵏ, d[ω]) for ω in Ω]
    # Step 5
    lb = value(-2 * u_make) + sum(p * r.V for (p, r) in zip(P, ret))
    println(&quot;  V̲ = $lb&quot;)
    # Step 6
    if ub - lb &lt; 1e-6
        println(&quot;Terminating with near-optimal solution&quot;)
        break
    end
    # Step 7
    c = @constraint(
        model,
        θ &lt;= sum(p * (r.V + r.λ * (x_out - xᵏ)) for (p, r) in zip(P, ret)),
    )
    println(&quot;  Added cut: $c&quot;)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Solving iteration k = 1
  xᵏ = -0.0
  V̅ = 1212.463331814822
  V̲ = 0.0
  Added cut: -4.99999999999999 x_out + θ ≤ 0
Solving iteration k = 2
  xᵏ = 242.49266636296485
  V̅ = 727.4779990888921
  V̲ = 502.28446539268685
  Added cut: 0.10000000000000007 x_out + θ ≤ 1011.5190647549128
Solving iteration k = 3
  xᵏ = 198.33707152057153
  V̅ = 595.0112145617127
  V̲ = 556.3253072620291
  Added cut: -2.4499999999999993 x_out + θ ≤ 467.073625077774
Solving iteration k = 4
  xᵏ = 213.50801555966237
  V̅ = 563.1522320796219
  V̲ = 549.2038523690094
  Added cut: -0.8690000000000002 x_out + θ ≤ 790.6814179669881
Solving iteration k = 5
  xᵏ = 204.68551099887057
  V̅ = 559.1821050272656
  V̲ = 556.4471963073704
  Added cut: -1.5830000000000009 x_out + θ ≤ 641.8010543939006
Solving iteration k = 6
  xᵏ = 201.53106034155329
  V̅ = 557.7626022314728
  V̲ = 557.1525395584326
  Added cut: -1.9910000000000012 x_out + θ ≤ 558.9663191015056
Solving iteration k = 7
  xᵏ = 200.2019477641223
  V̅ = 557.164501571629
  V̲ = 556.9559163110227
  Added cut: -2.2460000000000004 x_out + θ ≤ 507.70623716105047
Solving iteration k = 8
  xᵏ = 201.0199291782563
  V̅ = 557.1571397389015
  V̲ = 557.1209080880819
  Added cut: -2.093000000000001 x_out + θ ≤ 538.4260546745057
Solving iteration k = 9
  xᵏ = 201.37514144117475
  V̅ = 557.1539428285353
  V̲ = 557.152809430718
  Added cut: -2.042000000000001 x_out + θ ≤ 548.69505349019
Solving iteration k = 10
  xᵏ = 201.397364927745
  V̅ = 557.1537428171562
  V̲ = 557.153742817157
Terminating with near-optimal solution</code></pre><p>To get the first-stage solution, we do:</p><pre><code class="language-julia hljs">optimize!(model)
xᵏ = value(x_out)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">201.397364927745</code></pre><p>To compute a second-stage solution, we do:</p><pre><code class="language-julia hljs">solve_second_stage(xᵏ, 170.0)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(V = 846.8602635072255, λ = -0.1, x = 31.397364927745002, u = 170.0)</code></pre><h2 id="Policy-Graph"><a class="docs-heading-anchor" href="#Policy-Graph">Policy Graph</a><a id="Policy-Graph-1"></a><a class="docs-heading-anchor-permalink" href="#Policy-Graph" title="Permalink"></a></h2><p>Now let&#39;s see how we can formulate and train a policy for the two-stage newsvendor problem using <code>SDDP.jl</code>. Under the hood, <code>SDDP.jl</code> implements the exact algorithm that we just wrote by hand.</p><pre><code class="language-julia hljs">model = SDDP.LinearPolicyGraph(;
    stages = 2,
    sense = :Max,
    upper_bound = 5 * maximum(d),  # The `M` in θ &lt;= M
    optimizer = HiGHS.Optimizer,
) do subproblem::JuMP.Model, stage::Int
    @variable(subproblem, x &gt;= 0, SDDP.State, initial_value = 0)
    if stage == 1
        @variable(subproblem, u_make &gt;= 0)
        @constraint(subproblem, x.out == x.in + u_make)
        @stageobjective(subproblem, -2 * u_make)
    else
        @variable(subproblem, u_sell &gt;= 0)
        @constraint(subproblem, u_sell &lt;= x.in)
        @constraint(subproblem, x.out == x.in - u_sell)
        SDDP.parameterize(subproblem, d, P) do ω
            set_upper_bound(u_sell, ω)
            return
        end
        @stageobjective(subproblem, 5 * u_sell - 0.1 * x.out)
    end
    return
end

SDDP.train(model; log_every_iteration = true)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-------------------------------------------------------------------
         SDDP.jl (c) Oscar Dowson and contributors, 2017-23
-------------------------------------------------------------------
problem
  nodes           : 2
  state variables : 1
  scenarios       : 1.00000e+02
  existing cuts   : false
options
  solver          : serial mode
  risk measure    : SDDP.Expectation()
  sampling scheme : SDDP.InSampleMonteCarlo
subproblem structure
  VariableRef                             : [4, 4]
  AffExpr in MOI.EqualTo{Float64}         : [1, 1]
  AffExpr in MOI.LessThan{Float64}        : [1, 1]
  VariableRef in MOI.GreaterThan{Float64} : [2, 3]
  VariableRef in MOI.LessThan{Float64}    : [1, 1]
numerical stability report
  matrix range     [1e+00, 1e+00]
  objective range  [1e-01, 5e+00]
  bounds range     [2e+02, 1e+03]
  rhs range        [0e+00, 0e+00]
-------------------------------------------------------------------
 iteration    simulation      bound        time (s)     solves  pid
-------------------------------------------------------------------
         1   0.000000e+00  7.274780e+02  7.169962e-03       103   1
         2   7.274780e+02  5.950112e+02  2.604890e-02       406   1
         3   5.950112e+02  5.631522e+02  3.157592e-02       509   1
         4   4.826625e+02  5.591821e+02  3.709602e-02       612   1
         5   6.140565e+02  5.577626e+02  4.262209e-02       715   1
         6   6.045932e+02  5.571645e+02  4.820895e-02       818   1
         7   6.006058e+02  5.571571e+02  5.373907e-02       921   1
         8   6.030598e+02  5.571539e+02  5.920100e-02      1024   1
         9   6.041254e+02  5.571537e+02  6.462097e-02      1127   1
        10   5.816030e+02  5.571537e+02  7.029104e-02      1230   1
        11   3.787725e+02  5.571537e+02  1.465421e-01      1333   1
        12   6.041921e+02  5.571537e+02  1.524169e-01      1436   1
        13   5.154734e+02  5.571537e+02  1.581690e-01      1539   1
        14   5.494312e+02  5.571537e+02  1.638269e-01      1642   1
        15   5.304504e+02  5.571537e+02  1.695421e-01      1745   1
        16   5.154734e+02  5.571537e+02  1.753099e-01      1848   1
        17   5.190111e+02  5.571537e+02  1.809609e-01      1951   1
        18   5.875496e+02  5.571537e+02  1.866851e-01      2054   1
        19   6.041921e+02  5.571537e+02  1.924191e-01      2157   1
        20   6.041921e+02  5.571537e+02  1.981821e-01      2260   1
        21   5.977671e+02  5.571537e+02  2.203331e-01      2563   1
        22   6.041921e+02  5.571537e+02  2.260721e-01      2666   1
        23   6.041921e+02  5.571537e+02  2.318251e-01      2769   1
        24   6.041921e+02  5.571537e+02  2.376239e-01      2872   1
        25   5.527482e+02  5.571537e+02  2.435110e-01      2975   1
        26   6.039654e+02  5.571537e+02  2.492249e-01      3078   1
        27   3.865338e+02  5.571537e+02  2.549829e-01      3181   1
        28   6.041921e+02  5.571537e+02  2.607470e-01      3284   1
        29   6.041921e+02  5.571537e+02  2.665379e-01      3387   1
        30   6.041921e+02  5.571537e+02  2.722909e-01      3490   1
        31   5.744560e+02  5.571537e+02  2.780700e-01      3593   1
        32   5.154734e+02  5.571537e+02  2.838049e-01      3696   1
        33   6.041921e+02  5.571537e+02  2.896399e-01      3799   1
        34   5.440962e+02  5.571537e+02  2.955050e-01      3902   1
        35   6.041921e+02  5.571537e+02  3.013420e-01      4005   1
        36   6.041921e+02  5.571537e+02  3.071761e-01      4108   1
        37   4.256853e+02  5.571537e+02  3.130951e-01      4211   1
        38   6.041921e+02  5.571537e+02  3.190370e-01      4314   1
        39   5.527482e+02  5.571537e+02  3.248670e-01      4417   1
        40   5.675978e+02  5.571537e+02  3.307950e-01      4520   1
-------------------------------------------------------------------
status         : simulation_stopping
total time (s) : 3.307950e-01
total solves   : 4520
best bound     :  5.571537e+02
simulation ci  :  5.546231e+02 ± 3.431777e+01
numeric issues : 0
-------------------------------------------------------------------</code></pre><p>One way to query the optimal policy is with <a href="../../apireference/#SDDP.DecisionRule"><code>SDDP.DecisionRule</code></a>:</p><pre><code class="language-julia hljs">first_stage_rule = SDDP.DecisionRule(model; node = 1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">A decision rule for node 1</code></pre><pre><code class="language-julia hljs">solution_1 = SDDP.evaluate(first_stage_rule; incoming_state = Dict(:x =&gt; 0.0))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(stage_objective = -402.7947298555181, outgoing_state = Dict(:x =&gt; 201.39736492775904), controls = Dict{Any, Any}())</code></pre><p>Here&#39;s the second stage:</p><pre><code class="language-julia hljs">second_stage_rule = SDDP.DecisionRule(model; node = 2)
solution = SDDP.evaluate(
    second_stage_rule;
    incoming_state = Dict(:x =&gt; solution_1.outgoing_state[:x]),
    noise = 170.0,  # A value of d[ω], can be out-of-sample.
    controls_to_record = [:u_sell],
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(stage_objective = 846.8602635072241, outgoing_state = Dict(:x =&gt; 31.397364927759043), controls = Dict(:u_sell =&gt; 170.0))</code></pre><h2 id="Simulation"><a class="docs-heading-anchor" href="#Simulation">Simulation</a><a id="Simulation-1"></a><a class="docs-heading-anchor-permalink" href="#Simulation" title="Permalink"></a></h2><p>Querying the decision rules is tedious. It&#39;s often more useful to simulate the policy:</p><pre><code class="language-julia hljs">simulations = SDDP.simulate(
    model,
    10,  #= number of replications =#
    [:x, :u_sell, :u_make];  #= variables to record =#
    skip_undefined_variables = true,
);</code></pre><p><code>simulations</code> is a vector with 10 elements</p><pre><code class="language-julia hljs">length(simulations)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10</code></pre><p>and each element is a vector with two elements (one for each stage)</p><pre><code class="language-julia hljs">length(simulations[1])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2</code></pre><p>The first stage contains:</p><pre><code class="language-julia hljs">simulations[1][1]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Dict{Symbol, Any} with 9 entries:
  :u_make          =&gt; 201.397
  :bellman_term    =&gt; 959.948
  :noise_term      =&gt; nothing
  :node_index      =&gt; 1
  :stage_objective =&gt; -402.795
  :objective_state =&gt; nothing
  :u_sell          =&gt; NaN
  :belief          =&gt; Dict(1=&gt;1.0)
  :x               =&gt; State{Float64}(0.0, 201.397)</code></pre><p>The second stage contains:</p><pre><code class="language-julia hljs">simulations[1][2]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Dict{Symbol, Any} with 9 entries:
  :u_make          =&gt; NaN
  :bellman_term    =&gt; 0.0
  :noise_term      =&gt; 187.454
  :node_index      =&gt; 2
  :stage_objective =&gt; 935.875
  :objective_state =&gt; nothing
  :u_sell          =&gt; 187.454
  :belief          =&gt; Dict(2=&gt;1.0)
  :x               =&gt; State{Float64}(201.397, 13.9435)</code></pre><p>We can compute aggregated statistics across the simulations:</p><pre><code class="language-julia hljs">objectives = map(simulations) do simulation
    return sum(data[:stage_objective] for data in simulation)
end
μ, t = SDDP.confidence_interval(objectives)
println(&quot;Simulation ci : $μ ± $t&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Simulation ci : 537.904660373077 ± 43.17034421193409</code></pre><h2 id="Risk-aversion-revisited"><a class="docs-heading-anchor" href="#Risk-aversion-revisited">Risk aversion revisited</a><a id="Risk-aversion-revisited-1"></a><a class="docs-heading-anchor-permalink" href="#Risk-aversion-revisited" title="Permalink"></a></h2><p>SDDP.jl contains a number of risk measures. One example is:</p><pre><code class="language-julia hljs">0.5 * SDDP.Expectation() + 0.5 * SDDP.WorstCase()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">A convex combination of 0.5 * SDDP.Expectation() + 0.5 * SDDP.WorstCase()</code></pre><p>You can construct a risk-averse policy by passing a risk measure to the <code>risk_measure</code> keyword argument of <a href="../../apireference/#SDDP.train"><code>SDDP.train</code></a>.</p><p>We can explore how the optimal decision changes with risk by creating a function:</p><pre><code class="language-julia hljs">function solve_newsvendor(risk_measure::SDDP.AbstractRiskMeasure)
    model = SDDP.LinearPolicyGraph(
        stages = 2,
        sense = :Max,
        upper_bound = 5 * maximum(d),
        optimizer = HiGHS.Optimizer,
    ) do subproblem, node
        @variable(subproblem, x &gt;= 0, SDDP.State, initial_value = 0)
        if node == 1
            @stageobjective(subproblem, -2 * x.out)
        else
            @variable(subproblem, u_sell &gt;= 0)
            @constraint(subproblem, u_sell &lt;= x.in)
            @constraint(subproblem, x.out == x.in - u_sell)
            SDDP.parameterize(subproblem, d, P) do ω
                set_upper_bound(u_sell, ω)
                return
            end
            @stageobjective(subproblem, 5 * u_sell - 0.1 * x.out)
        end
        return
    end
    SDDP.train(model; risk_measure = risk_measure, print_level = 0)
    first_stage_rule = SDDP.DecisionRule(model; node = 1)
    solution = SDDP.evaluate(first_stage_rule; incoming_state = Dict(:x =&gt; 0.0))
    return solution.outgoing_state[:x]
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">solve_newsvendor (generic function with 1 method)</code></pre><p>Now we can see how many units a decision maker would order using <code>CVaR</code>:</p><pre><code class="language-julia hljs">solve_newsvendor(SDDP.CVaR(0.4))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">186.39560244432928</code></pre><p>as well as a decision-maker who cares only about the worst-case outcome:</p><pre><code class="language-julia hljs">solve_newsvendor(SDDP.WorstCase())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">153.7714528298821</code></pre><p>In general, the decision-maker will be somewhere between the two extremes. The <a href="../../guides/add_a_risk_measure/#SDDP.Entropic"><code>SDDP.Entropic</code></a> risk measure is a risk measure that has a single parameter that lets us explore the space of policies between the two extremes. When the parameter is small, the measure acts like <a href="../../guides/add_a_risk_measure/#SDDP.Expectation"><code>SDDP.Expectation</code></a>, and when it is large, it acts like <a href="../../guides/add_a_risk_measure/#SDDP.WorstCase"><code>SDDP.WorstCase</code></a>.</p><p>Here is what we get if we solve our problem multiple times for different values of the risk aversion parameter <span>$\gamma$</span>:</p><pre><code class="language-julia hljs">Γ = [10^i for i in -4:0.5:1]
buy = [solve_newsvendor(SDDP.Entropic(γ)) for γ in Γ]
Plots.plot(
    Γ,
    buy;
    xaxis = :log,
    xlabel = &quot;Risk aversion parameter γ&quot;,
    ylabel = &quot;Number of pies to make&quot;,
    legend = false,
)</code></pre><img src="7db81df1.svg" alt="Example block output"/><h2 id="Things-to-try"><a class="docs-heading-anchor" href="#Things-to-try">Things to try</a><a id="Things-to-try-1"></a><a class="docs-heading-anchor-permalink" href="#Things-to-try" title="Permalink"></a></h2><p>There are a number of things you can try next:</p><ul><li>Experiment with different buy and sales prices</li><li>Experiment with different distributions of demand</li><li>Explore how the optimal policy changes if you use a different risk measure</li><li>What happens if you can only buy and sell integer numbers of newspapers? Try this by adding <code>Int</code> to the variable definitions: <code>@variable(subproblem, buy &gt;= 0, Int)</code></li><li>What happens if you use a different upper bound? Try an invalid one like <code>-100</code>, and a very large one like <code>1e12</code>.</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../mdps/">« Example: Markov Decision Processes</a><a class="docs-footer-nextpage" href="../example_reservoir/">Example: deterministic to stochastic »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.1.0 on <span class="colophon-date" title="Friday 29 September 2023 00:25">Friday 29 September 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
