{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Example: Markov Decision Processes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`SDDP.jl` can be used to solve a variety of Markov Decision processes. If the\n",
    "problem has continuous state and control spaces, and the objective and\n",
    "transition function are convex, then SDDP.jl can find a globally optimal\n",
    "policy. In other cases, SDDP.jl will find a locally optimal policy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A simple example"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A simple demonstration of this is the example taken from page 98 of the book\n",
    "\"Markov Decision Processes: Discrete stochastic Dynamic Programming\", by\n",
    "Martin L. Putterman."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The example, as described in Section 4.6.3 of the book, is to minimize a sum\n",
    "of squares of `N` non-negative variables, subject to a budget constraint that\n",
    "the variable values add up to `M`. Put mathematically, that is:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min \\;\\; & \\sum\\limits_{i=1}^N x_i^2       \\\\\n",
    "s.t. \\;\\; & \\sum\\limits_{i=1}^N x_i = M     \\\\\n",
    "          & x_i \\ge 0, \\quad i \\in 1,\\ldots,N\n",
    "\\end{aligned}\n",
    "$$\n",
    "The optimal objective value is $M^2/N$, and the optimal solution is\n",
    "$x_i = M / N$, which can be shown by induction."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This can be reformulated as a Markov Decision Process by introducing a state\n",
    "variable, $s$, which tracks the un-spent budget over $N$ stages."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "V_t(s) = \\min \\;\\; & x^2 + V_{t+1}(s^\\prime) \\\\\n",
    "s.t. \\;\\; & s^\\prime = s - x \\\\\n",
    "          & x \\le s \\\\\n",
    "          & x \\ge 0 \\\\\n",
    "          & s \\ge 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "and in the last stage $V_N$, there is an additional constraint that\n",
    "$s^\\prime = 0$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The budget of $M$ is computed by solving for $V_1(M)$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Info**\n",
    ">\n",
    "> Since everything here is continuous and convex, SDDP.jl will find the\n",
    "> globally optimal policy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the reformulation from the single problem into the recursive form of the\n",
    "Markov Decision Process is not obvious, consult Putterman's book."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can model and solve this problem using SDDP.jl as follows:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using SDDP\n",
    "import Ipopt\n",
    "\n",
    "M, N = 5, 3\n",
    "\n",
    "model = SDDP.LinearPolicyGraph(;\n",
    "    stages = N,\n",
    "    lower_bound = 0.0,\n",
    "    optimizer = Ipopt.Optimizer,\n",
    ") do subproblem, node\n",
    "    @variable(subproblem, s >= 0, SDDP.State, initial_value = M)\n",
    "    @variable(subproblem, x >= 0)\n",
    "    @stageobjective(subproblem, x^2)\n",
    "    @constraint(subproblem, x <= s.in)\n",
    "    @constraint(subproblem, s.out == s.in - x)\n",
    "    if node == N\n",
    "        fix(s.out, 0.0; force = true)\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "SDDP.train(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check that we got the theoretical optimum:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "SDDP.calculate_bound(model), M^2 / N"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And check that we found the theoretical value for each $x_i$:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations = SDDP.simulate(model, 1, [:x])\n",
    "for data in simulations[1]\n",
    "    println(\"x_$(data[:node_index]) = $(data[:x])\")\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Close enough! We don't get exactly 5/3 because of numerical tolerances within\n",
    "our choice of optimization solver (in this case, Ipopt)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A more complicated policy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "SDDP.jl is also capable of finding policies for other types of Markov Decision\n",
    "Processes. A classic example of a Markov Decision Process is the problem of\n",
    "finding a path through a maze."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's one example of a maze. Try changing the parameters to explore different\n",
    "mazes:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "M, N = 3, 4\n",
    "initial_square = (1, 1)\n",
    "reward, illegal_squares, penalties = (3, 4), [(2, 2)], [(3, 1), (2, 4)]\n",
    "path = fill(\"⋅\", M, N)\n",
    "path[initial_square...] = \"1\"\n",
    "for (k, v) in (illegal_squares => \"▩\", penalties => \"†\", [reward] => \"*\")\n",
    "    for (i, j) in k\n",
    "        path[i, j] = v\n",
    "    end\n",
    "end\n",
    "print(join([join(path[i, :], ' ') for i in 1:size(path, 1)], '\\n'))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our goal is to get from square `1` to square `*`. If we step on a `†`, we\n",
    "incur a penalty of `1`. Squares with `▩` are blocked; we cannot move there."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are a variety of ways that we can solve this problem. We're going to\n",
    "solve it using a stationary binary stochastic programming formulation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our state variable will be a matrix of binary variables $x_{i,j}$, where\n",
    "each element is $1$ if the agent is in the square and $0$ otherwise. In\n",
    "each period, we incur a reward of $1$ if we are in the `reward` square and a\n",
    "penalty of $-1$ if we are in a `penalties` square. We cannot move to the\n",
    "`illegal_squares`, so those $x_{i,j} = 0$. Feasibility between moves is\n",
    "modelled by constraints of the form:\n",
    "$$\n",
    "x^\\prime_{i,j} \\le \\sum\\limits_{(a,b)\\in P} x_{a,b}\n",
    "$$\n",
    "where $P$ is the set of squares from which it is valid to move from `(a, b)`\n",
    "to `(i, j)`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because we are looking for a stationary policy, we need a unicyclic graph with\n",
    "a discount factor:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "discount_factor = 0.9\n",
    "graph = SDDP.UnicyclicGraph(discount_factor)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we can formulate our full model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import HiGHS\n",
    "\n",
    "model = SDDP.PolicyGraph(\n",
    "    graph;\n",
    "    sense = :Max,\n",
    "    upper_bound = 1 / (1 - discount_factor),\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ") do sp, _\n",
    "    # Our state is a binary variable for each square\n",
    "    @variable(\n",
    "        sp,\n",
    "        x[i = 1:M, j = 1:N],\n",
    "        Bin,\n",
    "        SDDP.State,\n",
    "        initial_value = (i, j) == initial_square,\n",
    "    )\n",
    "    # Can only be in one square at a time\n",
    "    @constraint(sp, sum(x[i, j].out for i in 1:M, j in 1:N) == 1)\n",
    "    # Incur rewards and penalties\n",
    "    @stageobjective(\n",
    "        sp,\n",
    "        x[reward...].out - sum(x[i, j].out for (i, j) in penalties)\n",
    "    )\n",
    "    # Some squares are illegal\n",
    "    @constraint(sp, [(i, j) in illegal_squares], x[i, j].out <= 0)\n",
    "    # Constraints on valid moves\n",
    "    for i in 1:M, j in 1:N\n",
    "        moves = [(i - 1, j), (i + 1, j), (i, j), (i, j + 1), (i, j - 1)]\n",
    "        filter!(v -> 1 <= v[1] <= M && 1 <= v[2] <= N, moves)\n",
    "        @constraint(sp, x[i, j].out <= sum(x[a, b].in for (a, b) in moves))\n",
    "    end\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The upper bound is obtained by assuming that we reach the reward square in one\n",
    "move and stay there."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Warning**\n",
    ">\n",
    "> Since there are discrete decisions here, SDDP.jl is not guaranteed to find\n",
    "> the globally optimal policy."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "SDDP.train(model)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simulating a cyclic policy graph requires an explicit `sampling_scheme` that\n",
    "does not terminate early based on the cycle probability:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations = SDDP.simulate(\n",
    "    model,\n",
    "    1,\n",
    "    [:x];\n",
    "    sampling_scheme = SDDP.InSampleMonteCarlo(;\n",
    "        max_depth = 5,\n",
    "        terminate_on_dummy_leaf = false,\n",
    "    ),\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fill in the `path` with the time-step in which we visit the square:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "for (t, data) in enumerate(simulations[1]), i in 1:M, j in 1:N\n",
    "    if data[:x][i, j].in > 0.5\n",
    "        path[i, j] = \"$t\"\n",
    "    end\n",
    "end\n",
    "\n",
    "print(join([join(path[i, :], ' ') for i in 1:size(path, 1)], '\\n'))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **Tip**\n",
    ">\n",
    "> This formulation will likely struggle as the number of cells in the maze\n",
    "> increases. Can you think of an equivalent formulation that uses fewer\n",
    "> state variables?"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  },
  "kernelspec": {
   "name": "julia-1.11",
   "display_name": "Julia 1.11.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
