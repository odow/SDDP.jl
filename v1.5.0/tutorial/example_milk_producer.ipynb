{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Example: the milk producer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The purpose of this tutorial is to demonstrate how to fit a Markovian policy\n",
    "graph to a univariate stochastic process."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This tutorial uses the following packages:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using SDDP\n",
    "import HiGHS\n",
    "import Plots"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Background"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A company produces milk for sale on a spot market each month. The quantity of\n",
    "milk they produce is uncertain, and so too is the price on the spot market."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The company can store the milk, but, over time, some milk spoils and must be\n",
    "discarded. Eventually the milk expires and is worthless."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The spot price is determined by an auction system, and so varies from month to\n",
    "month, but demonstrates serial correlation. In each auction, there is\n",
    "sufficient demand that the milk producer finds a buyer for all their\n",
    "widgets, regardless of the quantity they supply. Furthermore, the spot price\n",
    "is independent of the milk producer (they are a small player in the market)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The spot price is highly volatile, and is the result of a process that is out\n",
    "of the control of the company. To counteract their price risk, the company\n",
    "engages in a forward contracting programme."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The forward contracting programme is a deal for physical milk at a future\n",
    "date in time, up to four months in the future."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The futures price is the current spot price, plus some forward contango (the\n",
    "buyers gain certainty that they will receive the milk in the future)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In general, the milk company should forward contract (since they reduce\n",
    "their price risk), however they also have production risk. Therefore, it may\n",
    "be the case that they forward contract a fixed amount, but find that they do\n",
    "not produce enough milk to meet the fixed demand. They are then forced to\n",
    "buy additional milk on the spot market."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The goal of the milk company is to choose the extent to which they forward\n",
    "contract in order to maximise (risk-adjusted) revenues, whilst managing their\n",
    "production risk."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A stochastic process for price"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is outside the scope of this tutorial, but assume that we have gone away\n",
    "and analysed historical data to fit a stochastic process to the sequence of\n",
    "monthly auction spot prices."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One plausible model is a multiplicative auto-regressive model of order one,\n",
    "where the white noise term is modeled by a finite distribution of empirical\n",
    "residuals. We can simulate this stochastic process as follows:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function simulator()\n",
    "    residuals = [0.0987, 0.199, 0.303, 0.412, 0.530, 0.661, 0.814, 1.010, 1.290]\n",
    "    residuals = 0.1 * vcat(-residuals, 0.0, residuals)\n",
    "    scenario = zeros(12)\n",
    "    y, μ, α = 4.5, 6.0, 0.05\n",
    "    for t in 1:12\n",
    "        y = exp((1 - α) * log(y) + α * log(μ) + rand(residuals))\n",
    "        scenario[t] = clamp(y, 3.0, 9.0)\n",
    "    end\n",
    "    return scenario\n",
    "end\n",
    "\n",
    "simulator()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "It may be helpful to visualize a number of simulations of the price process:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot = Plots.plot(\n",
    "    [simulator() for _ in 1:500];\n",
    "    color = \"gray\",\n",
    "    opacity = 0.2,\n",
    "    legend = false,\n",
    "    xlabel = \"Month\",\n",
    "    ylabel = \"Price [\\$/kg]\",\n",
    "    xlims = (1, 12),\n",
    "    ylims = (3, 9),\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The prices gradually revert to the mean of \\$6/kg, and there is high\n",
    "volatility."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can't incorporate this price process directly into SDDP.jl, but we can fit\n",
    "a `SDDP.MarkovianGraph` directly from the simulator:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "graph = SDDP.MarkovianGraph(simulator; budget = 60, scenarios = 10_000);\n",
    "nothing  # hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here `budget` is the number of nodes in the policy graph, and `scenarios` is\n",
    "the number of simulations to use when estimating the transition probabilities."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The graph contains too many nodes to be show, but we can plot it:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "for ((t, price), edges) in graph.nodes\n",
    "    for ((t′, price′), probability) in edges\n",
    "        Plots.plot!(\n",
    "            plot,\n",
    "            [t, t′],\n",
    "            [price, price′];\n",
    "            color = \"red\",\n",
    "            width = 3 * probability,\n",
    "        )\n",
    "    end\n",
    "end\n",
    "\n",
    "plot"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "That looks okay. Try changing `budget` and `scenarios` to see how different\n",
    "Markovian policy graphs can be created."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have a Markovian graph, we can build the model. See if you can\n",
    "work out how we arrived at this formulation by reading the background\n",
    "description. Do all the variables and constraints make sense?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = SDDP.PolicyGraph(\n",
    "    graph;\n",
    "    sense = :Max,\n",
    "    upper_bound = 1e4,\n",
    "    optimizer = HiGHS.Optimizer,\n",
    ") do sp, node\n",
    "    t, price = node\n",
    "    c_contango = [1.0, 1.025, 1.05, 1.075]\n",
    "    c_transaction, c_perish_factor, c_buy_premium = 0.01, 0.95, 1.5\n",
    "    F, P = length(c_contango), 5\n",
    "    @variable(sp, 0 <= x_forward[1:F], SDDP.State, initial_value = 0)\n",
    "    @variable(sp, 0 <= x_stock[p = 1:P], SDDP.State, initial_value = 0)\n",
    "    @variable(sp, 0 <= u_spot_sell[1:P])\n",
    "    @variable(sp, 0 <= u_spot_buy)\n",
    "    @variable(sp, 0 <= u_forward_deliver[1:P])\n",
    "    @variable(sp, 0 <= u_forward_sell[1:F] <= 10)\n",
    "    @variable(sp, 0 <= u_production)\n",
    "    for f in 1:F\n",
    "        if t + f >= 12\n",
    "            fix(u_forward_sell[f], 0.0; force = true)\n",
    "        end\n",
    "    end\n",
    "    @constraint(\n",
    "        sp,\n",
    "        [i = 1:F-1],\n",
    "        x_forward[i].out == x_forward[i+1].in + u_forward_sell[i],\n",
    "    )\n",
    "    @constraint(sp, x_forward[F].out == u_forward_sell[F])\n",
    "    @constraint(sp, x_forward[1].in == sum(u_forward_deliver))\n",
    "    @constraint(\n",
    "        sp,\n",
    "        x_stock[1].out ==\n",
    "        u_production - u_forward_deliver[1] - u_spot_sell[1] + u_spot_buy\n",
    "    )\n",
    "    @constraint(\n",
    "        sp,\n",
    "        [i = 2:P],\n",
    "        x_stock[i].out ==\n",
    "        c_perish_factor * x_stock[i-1].in - u_forward_deliver[i] -\n",
    "        u_spot_sell[i]\n",
    "    )\n",
    "    Ω = [(price, (production = p,)) for p in range(0.1, 0.2; length = 5)]\n",
    "    SDDP.parameterize(sp, Ω) do (price, ω)\n",
    "        set_upper_bound(u_production, ω.production)\n",
    "        @stageobjective(\n",
    "            sp,\n",
    "            price * sum(u_spot_sell) +\n",
    "            price * sum(c_contango[f] * u_forward_sell[f] for f in 1:F) -\n",
    "            price * c_buy_premium * u_spot_buy -\n",
    "            c_transaction * sum(u_forward_sell)\n",
    "        )\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training a policy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have a model, we train a policy. The `SDDP.SimulatorSamplingScheme`\n",
    "is used in the forward pass. It generates an out-of-sample sequence of prices\n",
    "using `simulator` and traverses the closest sequence of nodes in the policy\n",
    "graph. When calling `SDDP.parameterize` for each subproblem, it uses\n",
    "the new out-of-sample price instead of the price associated with the Markov\n",
    "node."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "SDDP.train(\n",
    "    model;\n",
    "    time_limit = 20,\n",
    "    stopping_rules = [SDDP.SimulationStoppingRule()],\n",
    "    risk_measure = SDDP.EAVaR(; lambda = 0.5, beta = 0.25),\n",
    "    sampling_scheme = SDDP.SimulatorSamplingScheme(simulator),\n",
    "    log_every_seconds = 2.0,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "!!! warning\n",
    "    We're intentionally terminating the training early so that the\n",
    "    documentation doesn't take too long to build. If you run this example\n",
    "    locally, increase the time limit."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulating the policy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When simulating the policy, we can also use the\n",
    "`SDDP.SimulatorSamplingScheme`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations = SDDP.simulate(\n",
    "    model,\n",
    "    200,\n",
    "    Symbol[:x_forward, :x_stock, :u_spot_sell, :u_spot_buy];\n",
    "    sampling_scheme = SDDP.SimulatorSamplingScheme(simulator),\n",
    ");\n",
    "nothing  # hide"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To show how the sampling scheme uses the new out-of-sample price instead of\n",
    "the price associated with the Markov node, compare the index of the Markov\n",
    "state visited in stage 12 of the first simulation:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations[1][12][:node_index]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "to the realization of the noise `(price, ω)` passed to `SDDP.parameterize`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "simulations[1][12][:noise_term]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing the policy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can plot the policy to gain insight (although note that we\n",
    "terminated the training early, so we should run the re-train the policy for\n",
    "more iterations before making too many judgements)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Plots.plot(\n",
    "    SDDP.publication_plot(simulations; title = \"sum(x_stock.out)\") do data\n",
    "        return sum(y.out for y in data[:x_stock])\n",
    "    end,\n",
    "    SDDP.publication_plot(simulations; title = \"sum(x_forward.out)\") do data\n",
    "        return sum(y.out for y in data[:x_forward])\n",
    "    end,\n",
    "    SDDP.publication_plot(simulations; title = \"u_spot_buy\") do data\n",
    "        return data[:u_spot_buy]\n",
    "    end,\n",
    "    SDDP.publication_plot(simulations; title = \"sum(u_spot_sell)\") do data\n",
    "        return sum(data[:u_spot_sell])\n",
    "    end;\n",
    "    layout = (2, 2),\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  },
  "kernelspec": {
   "name": "julia-1.9",
   "display_name": "Julia 1.9.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
