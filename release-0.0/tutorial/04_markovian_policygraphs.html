<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tutorial Four: Markovian policy graphs · SDDP.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>SDDP.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../index.html">Home</a></li><li><span class="toctext">Tutorials</span><ul><li><a class="toctext" href="01_first_steps.html">Tutorial One: first steps</a></li><li><a class="toctext" href="02_rhs_noise.html">Tutorial Two: RHS noise</a></li><li><a class="toctext" href="03_objective_noise.html">Tutorial Three: objective noise</a></li><li class="current"><a class="toctext" href="04_markovian_policygraphs.html">Tutorial Four: Markovian policy graphs</a><ul class="internal"><li><a class="toctext" href="#Formulating-the-problem-1">Formulating the problem</a></li><li><a class="toctext" href="#Solving-the-problem-1">Solving the problem</a></li><li><a class="toctext" href="#Understanding-the-solution-1">Understanding the solution</a></li></ul></li><li><a class="toctext" href="05_risk.html">Tutorial Five: risk</a></li><li><a class="toctext" href="06_cut_selection.html">Tutorial Six: cut selection</a></li><li><a class="toctext" href="07_plotting.html">Tutorial Seven: plotting</a></li><li><a class="toctext" href="08_odds_and_ends.html">Tutorial Eight: odds and ends</a></li><li><a class="toctext" href="09_nonlinear.html">Tutorial Nine: nonlinear models</a></li><li><a class="toctext" href="10_parallel.html">Tutorial Ten: parallelism</a></li><li><a class="toctext" href="11_DRO.html">Tutorial Eleven: distributionally robust SDDP</a></li><li><a class="toctext" href="12_price_interpolation.html">Tutorial Twelve: price interpolation</a></li><li><a class="toctext" href="13_constraint_noise.html">Tutorial Thirteen: constraint noise</a></li></ul></li><li><a class="toctext" href="../readings.html">Readings</a></li><li><a class="toctext" href="../apireference.html">Reference</a></li></ul></nav><article id="docs"><header><nav><ul><li>Tutorials</li><li><a href="04_markovian_policygraphs.html">Tutorial Four: Markovian policy graphs</a></li></ul><a class="edit-page" href="https://github.com/odow/SDDP.jl/blob/master/docs/src/tutorial/04_markovian_policygraphs.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Tutorial Four: Markovian policy graphs</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Tutorial-Four:-Markovian-policy-graphs-1" href="#Tutorial-Four:-Markovian-policy-graphs-1">Tutorial Four: Markovian policy graphs</a></h1><p>In our three tutorials (<a href="01_first_steps.html#Tutorial-One:-first-steps-1">Tutorial One: first steps</a>, <a href="02_rhs_noise.html#Tutorial-Two:-RHS-noise-1">Tutorial Two: RHS noise</a>, and <a href="03_objective_noise.html#Tutorial-Three:-objective-noise-1">Tutorial Three: objective noise</a>), we formulated a simple hydrothermal scheduling problem with stagewise-independent noise in the right-hand side of the constraints and in the objective function. Now, in this tutorial, we introduce some <em>stagewise-dependent</em> uncertainty using a Markov chain.</p><p>Recall that our model for the hydrothermal scheduling problem  from <a href="03_objective_noise.html#Tutorial-Three:-objective-noise-1">Tutorial Three: objective noise</a> is:</p><pre><code class="language-julia">m = SDDPModel(
                  sense = :Min,
                 stages = 3,
                 solver = ClpSolver(),
        objective_bound = 0.0
                                        ) do sp, t
    @state(sp, 0 &lt;= outgoing_volume &lt;= 200, incoming_volume == 200)
    @variables(sp, begin
        thermal_generation &gt;= 0
        hydro_generation   &gt;= 0
        hydro_spill        &gt;= 0
    end)
    @rhsnoise(sp, inflow = [0.0, 50.0, 100.0],
        outgoing_volume - (incoming_volume - hydro_generation - hydro_spill) == inflow
    )
    setnoiseprobability!(sp, [1/3, 1/3, 1/3])
    @constraints(sp, begin
        thermal_generation + hydro_generation == 150
    end)
    fuel_cost = [50.0, 100.0, 150.0]
    @stageobjective(sp, mupliplier = [1.2, 1.0, 0.8],
        mupliplier * fuel_cost[t] * thermal_generation
    )
end</code></pre><h2><a class="nav-anchor" id="Formulating-the-problem-1" href="#Formulating-the-problem-1">Formulating the problem</a></h2><p>In this tutorial we consider a Markov chain with two <em>climate</em> states: wet and dry. Each Markov state is associated with an integer, in this case the wet climate state  is Markov state <code>1</code> and the dry climate state is Markov state <code>2</code>. In the wet climate state, the probability of the high inflow increases to 50%, and the probability of the low inflow decreases to 1/6. In the dry climate state, the converse happens. There is also persistence in the climate state: the probability of remaining in the current state is 75%, and the probability of transitioning to the other climate state is 25%. We assume that the first stage starts in the wet climate state.</p><p>For each stage, we need to provide a Markov transition matrix. This is an <code>M</code>x<code>N</code> matrix, where the element <code>A[i,j]</code> gives the probability of transitioning from Markov state <code>i</code> in the previous stage to Markov state <code>j</code> in the current stage. The first stage is special because we assume there is a &quot;zero&#39;th&quot; stage which has one Markov state. Furthermore, the number of columns in the transition matrix of a stage (i.e. the number of Markov states) must equal the number of rows in the next stage&#39;s transition matrix. For our example, the vector of Markov transition matrices is given by:</p><pre><code class="language-julia">T = Array{Float64, 2}[
    [ 1.0 0.0 ],
    [ 0.75 0.25 ; 0.25 0.75 ],
    [ 0.75 0.25 ; 0.25 0.75 ]
]</code></pre><p>However, note that we never sample the dry Markov state in stage one. Therefore, we can drop that Markov state so that there is only one Markov state in stage 1. We also need to modify the transition matrix in stage 2 to account for this:</p><pre><code class="language-julia">T = Array{Float64, 2}[
    [ 1.0 ]&#39;,
    [ 0.75 0.25 ],
    [ 0.75 0.25 ; 0.25 0.75 ]
]</code></pre><p>To add the Markov chain to the model, we modifications are required. First, we give the vector of transition matrices to the <a href="../apireference.html#SDDP.SDDPModel"><code>SDDPModel</code></a> constructor using the <code>markov_transition</code> keyword. Second, the <code>do sp, t ... end</code> syntax is extended to <code>do sp, t, i ... end</code>, where <code>i</code> is the index of the Markov state and runs from <code>i=1</code> to the number of Markov states in stage <code>t</code>. Now, both <code>t</code> and <code>i</code> can be used anywhere inside the subproblem definition.</p><p>Our model is now:</p><pre><code class="language-julia">m = SDDPModel(
                  sense = :Min,
                 stages = 3,
                 solver = ClpSolver(),
        objective_bound = 0.0,
      markov_transition = Array{Float64, 2}[
          [ 1.0 ]&#39;,
          [ 0.75 0.25 ],
          [ 0.75 0.25 ; 0.25 0.75 ]
      ]
                                        ) do sp, t, i
    @state(sp, 0 &lt;= outgoing_volume &lt;= 200, incoming_volume == 200)
    @variables(sp, begin
        thermal_generation &gt;= 0
        hydro_generation   &gt;= 0
        hydro_spill        &gt;= 0
    end)
    @rhsnoise(sp, inflow = [0.0, 50.0, 100.0],
        outgoing_volume - (incoming_volume - hydro_generation - hydro_spill) == inflow
    )
    @constraints(sp, begin
        thermal_generation + hydro_generation == 150
    end)
    fuel_cost = [50.0, 100.0, 150.0]
    @stageobjective(sp, mupliplier = [1.2, 1.0, 0.8],
        mupliplier * fuel_cost[t] * thermal_generation
    )
    if i == 1  # wet climate state
        setnoiseprobability!(sp, [1/6, 1/3, 0.5])
    else       # dry climate state
        setnoiseprobability!(sp, [0.5, 1/3, 1/6])
    end
end</code></pre><h2><a class="nav-anchor" id="Solving-the-problem-1" href="#Solving-the-problem-1">Solving the problem</a></h2><p>Now we need to solve the problem. As in the previous two tutorials, we use the <a href="../apireference.html#JuMP.solve"><code>solve</code></a> function. However, this time we terminate the SDDP algorithm by setting a time limit (in seconds) using the <code>time_limit</code> keyword:</p><pre><code class="language-julia">status = solve(m,
    time_limit = 0.05
)</code></pre><p>The termination status is <code>:time_limit</code>, and the output from the log is now:</p><pre><code class="language-none">-------------------------------------------------------------------------------
                          SDDP.jl © Oscar Dowson, 2017-2018
-------------------------------------------------------------------------------
    Solver:
        Serial solver
    Model:
        Stages:         3
        States:         1
        Subproblems:    3
        Value Function: Default
-------------------------------------------------------------------------------
              Objective              |  Cut  Passes    Simulations   Total
     Simulation       Bound   % Gap  |   #     Time     #    Time    Time
-------------------------------------------------------------------------------
        0.000          6.198K        |     1    0.0      0    0.0    0.0
        2.000K         7.050K        |     2    0.0      0    0.0    0.0
        2.000K         7.050K        |     3    0.0      0    0.0    0.0
        2.000K         7.135K        |     4    0.0      0    0.0    0.0
        5.000K         7.135K        |     5    0.0      0    0.0    0.0
        2.000K         7.135K        |     6    0.0      0    0.0    0.0
        2.000K         7.135K        |     7    0.0      0    0.0    0.0
        2.000K         7.135K        |     8    0.0      0    0.0    0.0
        2.000K         7.135K        |     9    0.0      0    0.0    0.0
        9.000K         7.135K        |    10    0.0      0    0.0    0.0
        2.000K         7.135K        |    11    0.0      0    0.0    0.0
        5.000K         7.135K        |    12    0.1      0    0.0    0.1
-------------------------------------------------------------------------------
    Other Statistics:
        Iterations:         12
        Termination Status: time_limit
===============================================================================</code></pre><h2><a class="nav-anchor" id="Understanding-the-solution-1" href="#Understanding-the-solution-1">Understanding the solution</a></h2><p>Instead of performing a Monte Carlo simulation, you may want to simulate one particular sequence of noise realizations. This <em>historical</em> simulation can also be conducted using the <a href="../apireference.html#SDDP.simulate"><code>simulate</code></a> function.</p><pre><code class="language-julia">simulation_result = simulate(m,
    [:outgoing_volume, :thermal_generation, :hydro_generation, :hydro_spill],
    noises       = [1, 1, 3],
    markovstates = [1, 2, 2]
)</code></pre><p>Again, <code>simulation_result</code> is a single dictionary. In addition to the variable values and the special keys <code>:noise</code>, <code>:objective</code>, and <code>:stageobjective</code>, SDDP.jl also records the index of the Markov state in each stage via the <code>:markov</code> key. We can confirm that the historical sequence of Markov states was visited as follows:</p><pre><code class="language-julia">julia&gt; simulation_result[:markov]
3-element Array{Int, 1}:
 1
 2
 2</code></pre><p>This concludes our fourth tutorial for SDDP.jl. In the next tutorial, <a href="05_risk.html#Tutorial-Five:-risk-1">Tutorial Five: risk</a>, we introduce risk into the problem.</p><footer><hr/><a class="previous" href="03_objective_noise.html"><span class="direction">Previous</span><span class="title">Tutorial Three: objective noise</span></a><a class="next" href="05_risk.html"><span class="direction">Next</span><span class="title">Tutorial Five: risk</span></a></footer></article></body></html>
